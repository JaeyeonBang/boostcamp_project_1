{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a68501",
   "metadata": {},
   "source": [
    "# 앙상블 메타모델 (Stacking) - WandB 포함\n",
    "\n",
    "여러 베이스 모델의 예측 결과(probabilities)를 입력으로 받아 메타모델을 학습하고, 최종 예측을 생성합니다.\n",
    "\n",
    "## 포함 기능\n",
    "- 베이스 모델 예측 CSV 로드 및 스택 특성 생성\n",
    "- Stratified K-Fold OOF 학습/평가 (메타모델: 로지스틱 회귀)\n",
    "- Test 예측 생성 및 저장\n",
    "- WandB 로깅 및 아티팩트 업로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7754cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/py310/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/py310/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 라이브러리 임포트 완료\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import wandb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"✅ 라이브러리 임포트 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d82a0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디바이스: cuda\n",
      "✅ 설정 완료\n",
      "출력 디렉토리: ./predictions/ensemble_ori\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "RANDOM_STATE = 42\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# 예측 파일 디렉토리\n",
    "PRED_DIR = \"./predictions\"\n",
    "\n",
    "# 자동 탐색: 디렉토리 내 *_val_predictions.csv, *_test_predictions.csv\n",
    "AUTO_DISCOVER = True\n",
    "\n",
    "# 열 이름 설정\n",
    "ID_COL_CANDIDATES = [\"ID\", \"id\"]\n",
    "LABEL_COL = \"label\"\n",
    "PROB_COL_PREFIX = \"prob_class_\"  # prob_class_0..3\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "# 출력\n",
    "OUTPUT_DIR = os.path.join(PRED_DIR, \"ensemble_ori\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# PyTorch 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"디바이스: {device}\")\n",
    "\n",
    "# WandB\n",
    "PROJECT_NAME = \"[domain_project]_Ensemble_Models\"\n",
    "RUN_NAME = f\"stacking_torch_{TIMESTAMP}\"\n",
    "USE_WANDB = True\n",
    "\n",
    "print(\"✅ 설정 완료\")\n",
    "print(f\"출력 디렉토리: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7dcb6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/code/wandb/run-20251030_093813-tvb54n1k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Ensemble_Models/runs/tvb54n1k' target=\"_blank\">stacking_torch_2025-10-30_09-38-01</a></strong> to <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Ensemble_Models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Ensemble_Models' target=\"_blank\">https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Ensemble_Models</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Ensemble_Models/runs/tvb54n1k' target=\"_blank\">https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Ensemble_Models/runs/tvb54n1k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ WandB 초기화 완료\n"
     ]
    }
   ],
   "source": [
    "# WandB init\n",
    "if USE_WANDB:\n",
    "    run = wandb.init(\n",
    "        project=PROJECT_NAME,\n",
    "        name=RUN_NAME,\n",
    "        config={\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "            \"num_classes\": NUM_CLASSES,\n",
    "            \"meta_model\": \"torch.nn.Linear\",\n",
    "            \"cv_folds\": 5,\n",
    "            \"device\": str(device),\n",
    "        }\n",
    "    )\n",
    "    print(\"✅ WandB 초기화 완료\")\n",
    "else:\n",
    "    print(\"⚠️ WandB 비활성화\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf18372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 발견된 파일 수:\n",
      "  Val: 3  Test: 3\n"
     ]
    }
   ],
   "source": [
    "# 유틸: 예측 CSV 탐색 및 로드\n",
    "from glob import glob\n",
    "\n",
    "def discover_prediction_files(pred_dir: str):\n",
    "    val_files = sorted(glob(os.path.join(pred_dir, \"*_val_predictions.csv\")))\n",
    "    test_files = sorted(glob(os.path.join(pred_dir, \"*_test_predictions.csv\")))\n",
    "    return val_files, test_files\n",
    "\n",
    "val_files, test_files = ([], [])\n",
    "if AUTO_DISCOVER:\n",
    "    val_files, test_files = discover_prediction_files(PRED_DIR)\n",
    "\n",
    "print(\"🔎 발견된 파일 수:\")\n",
    "print(f\"  Val: {len(val_files)}  Test: {len(test_files)}\")\n",
    "\n",
    "#assert len(val_files) > 0 and len(test_files) > 0, \"예측 파일을 찾을 수 없습니다. predictions 디렉토리를 확인하세요.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1573d6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./predictions/TAPT_klue_Roberta-kor-base_final_training_2025-10-30_00-40-22_RANDOM_42_val_predictions.csv\n",
      "./predictions/TAPT_kykim_bert-kor-base_final_training_RAN_42_val_predictions.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./predictions/TAPT_monologg_koelectra-base-v3-discriminator_augX3_best_discriminator_1028_final_training_2025-10-30_05-34-33_RANDOM_42_val_predictions.csv\n",
      "[ 24475  24420 206143 ... 198563 293967  41866]\n",
      "./predictions/TAPT_klue_Roberta-kor-base_final_training_2025-10-30_00-40-22_RANDOM_42_final_training_2025-10-30_07-36-17_RANDOM_42_epoch_2_test_predictions.csv\n",
      "./predictions/TAPT_kykim_bert-kor-base_final_training_RAN_42_final_training_2025-10-30_08-54-54_RANDOM_42_epoch_3_test_predictions.csv\n",
      "./predictions/TAPT_monologg_koelectra-base-v3-discriminator_augX3_best_discriminator_1028_final_training_2025-10-30_07-21-18_RANDOM_42_epoch_2_test_predictions.csv\n",
      "✅ 스택 특성 생성 완료: X_val=(6823, 12), X_test=(59928, 12)\n"
     ]
    }
   ],
   "source": [
    "# 스택 특성 생성 함수\n",
    "\n",
    "def extract_id_column(df: pd.DataFrame):\n",
    "    for c in ID_COL_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            return df[c]\n",
    "    return pd.Series(np.arange(len(df)), name=\"row_idx\")\n",
    "\n",
    "\n",
    "def build_stack_features(file_list: list, is_val: bool):\n",
    "    \"\"\"여러 CSV에서 prob_class_* 열을 수집하여 [N, M*NUM_CLASSES] 특성 행렬 생성\"\"\"\n",
    "    probs = []\n",
    "    ids = None\n",
    "    labels = None\n",
    "\n",
    "    for path in file_list:\n",
    "        print(path)\n",
    "        df = pd.read_csv(path)\n",
    "        # ID\n",
    "        cur_ids = extract_id_column(df)\n",
    "        if ids is None:\n",
    "            ids = cur_ids\n",
    "        # 라벨 (val 기준)\n",
    "        if is_val and LABEL_COL in df.columns and labels is None:\n",
    "            labels = df[LABEL_COL].values\n",
    "        # 확률\n",
    "        model_probs = df[[f\"{PROB_COL_PREFIX}{i}\" for i in range(NUM_CLASSES)]].values\n",
    "        probs.append(model_probs)\n",
    "\n",
    "    X = np.hstack(probs)  # [N, num_models*NUM_CLASSES]\n",
    "    if is_val and labels is None:\n",
    "        raise ValueError(\"검증 데이터에서는 라벨 컬럼이 필요합니다.\")\n",
    "\n",
    "    return ids.values, X, labels\n",
    "\n",
    "# 생성\n",
    "val_ids, X_val, y_val = build_stack_features(val_files, is_val=True)\n",
    "print(val_ids)\n",
    "_, X_test, _ = build_stack_features(test_files, is_val=False)\n",
    "print(f\"✅ 스택 특성 생성 완료: X_val={X_val.shape}, X_test={X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "590c84c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== 간단 앙상블(가중 평균) - 정확도 최대화로 가중치 최적화 =====\n",
    "# from typing import List, Tuple\n",
    "\n",
    "\n",
    "# def load_probs_list(file_list: List[str], is_val: bool) -> Tuple[np.ndarray, List[np.ndarray], np.ndarray]:\n",
    "#     ids = None\n",
    "#     labels = None\n",
    "#     probs_list = []\n",
    "#     for path in file_list:\n",
    "#         df = pd.read_csv(path)\n",
    "#         cur_ids = extract_id_column(df)\n",
    "#         if ids is None:\n",
    "#             ids = cur_ids.values\n",
    "#         model_probs = df[[f\"{PROB_COL_PREFIX}{i}\" for i in range(NUM_CLASSES)]].values\n",
    "#         probs_list.append(model_probs)\n",
    "#         if is_val and labels is None and LABEL_COL in df.columns:\n",
    "#             labels = df[LABEL_COL].values\n",
    "#     if is_val and labels is None:\n",
    "#         raise ValueError(\"검증 데이터에서는 라벨 컬럼이 필요합니다.\")\n",
    "#     return ids, probs_list, labels\n",
    "\n",
    "\n",
    "# def optimize_weights_accuracy(probs_list: List[np.ndarray], y_true: np.ndarray, n_starts: int = 8):\n",
    "#     \"\"\"가중치 합=1, 각 가중치>=0 제약에서 정확도 최대화\"\"\"\n",
    "#     M = len(probs_list)\n",
    "#     probs = np.stack(probs_list, axis=0)  # [M, N, C]\n",
    "\n",
    "#     def acc_from_w(w):\n",
    "#         w = np.clip(w, 0, 1)\n",
    "#         s = w.sum()\n",
    "#         if s == 0:\n",
    "#             w = np.full_like(w, 1.0 / len(w))\n",
    "#         else:\n",
    "#             w = w / s\n",
    "#         ens = np.tensordot(w, probs, axes=(0, 0))  # [N, C]\n",
    "#         pred = ens.argmax(axis=1)\n",
    "#         return accuracy_score(y_true, pred)\n",
    "\n",
    "#     def obj(w):\n",
    "#         return -acc_from_w(w)\n",
    "\n",
    "#     bounds = [(0.0, 1.0)] * M\n",
    "#     cons = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}]\n",
    "\n",
    "#     inits = [np.full(M, 1.0 / M)]\n",
    "#     for _ in range(max(0, n_starts - 1)):\n",
    "#         r = np.random.rand(M)\n",
    "#         inits.append(r / r.sum())\n",
    "\n",
    "#     best_w, best_acc = None, -1.0\n",
    "#     for w0 in inits:\n",
    "#         res = minimize(obj, w0, method='SLSQP', bounds=bounds, constraints={ 'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0 }, options={'maxiter': 500, 'ftol': 1e-9, 'disp': False})\n",
    "#         if res.success:\n",
    "#             w = np.clip(res.x, 0, 1)\n",
    "#             s = w.sum()\n",
    "#             if s == 0:\n",
    "#                 w = np.full_like(w, 1.0 / len(w))\n",
    "#             else:\n",
    "#                 w = w / s\n",
    "#             acc = acc_from_w(w)\n",
    "#             if acc > best_acc:\n",
    "#                 best_w, best_acc = w, acc\n",
    "\n",
    "#     return best_w, best_acc\n",
    "\n",
    "\n",
    "# # 프로브 로드\n",
    "# val_ids_simple, val_probs_list, y_val_simple = load_probs_list(val_files, is_val=True)\n",
    "# _, test_probs_list, _ = load_probs_list(test_files, is_val=False)\n",
    "\n",
    "# # 최적 가중치 탐색 (정확도 최대화)\n",
    "# best_w, best_acc = optimize_weights_accuracy(val_probs_list, y_val_simple, n_starts=10)\n",
    "# print(\"🔧 Simple Ensemble - Best weights:\", best_w)\n",
    "# print(f\"🔧 Simple Ensemble - Val Accuracy: {best_acc:.6f}\")\n",
    "\n",
    "# # 검증/테스트 앙상블 확률 계산\n",
    "# val_probs_stack = np.stack(val_probs_list, axis=0)  # [M, N, C]\n",
    "# val_ens = np.tensordot(best_w, val_probs_stack, axes=(0, 0))\n",
    "# val_pred = val_ens.argmax(axis=1)\n",
    "# val_acc_simple = accuracy_score(y_val_simple, val_pred)\n",
    "# print(f\"✅ Simple Ensemble (Val) Accuracy: {val_acc_simple:.6f}\")\n",
    "\n",
    "# # 테스트\n",
    "# test_probs_stack = np.stack(test_probs_list, axis=0)\n",
    "# test_ens = np.tensordot(best_w, test_probs_stack, axes=(0, 0))\n",
    "# test_pred_simple = test_ens.argmax(axis=1)\n",
    "\n",
    "# # 저장\n",
    "# simple_val_out = pd.DataFrame({\n",
    "#     \"ID\": val_ids_simple,\n",
    "#     \"pred\": val_pred,\n",
    "#     \"label\": y_val_simple,\n",
    "# })\n",
    "# for i in range(NUM_CLASSES):\n",
    "#     simple_val_out[f\"prob_{i}\"] = val_ens[:, i]\n",
    "\n",
    "# simple_val_path = os.path.join(OUTPUT_DIR, f\"simple_ensemble_val_{TIMESTAMP}.csv\")\n",
    "# simple_val_out.to_csv(simple_val_path, index=False)\n",
    "# print(f\"💾 Simple Ensemble Val 저장: {simple_val_path}\")\n",
    "\n",
    "# # Test 저장\n",
    "# test_ids_simple = extract_id_column(pd.read_csv(test_files[0])).values\n",
    "# simple_test_out = pd.DataFrame({\n",
    "#     \"ID\": test_ids_simple,\n",
    "#     \"pred\": test_pred_simple,\n",
    "# })\n",
    "# for i in range(NUM_CLASSES):\n",
    "#     simple_test_out[f\"prob_{i}\"] = test_ens[:, i]\n",
    "\n",
    "# simple_test_path = os.path.join(OUTPUT_DIR, f\"simple_ensemble_test_{TIMESTAMP}.csv\")\n",
    "# simple_test_out.to_csv(simple_test_path, index=False)\n",
    "# print(f\"💾 Simple Ensemble Test 저장: {simple_test_path}\")\n",
    "\n",
    "# if USE_WANDB:\n",
    "#     wandb.log({\n",
    "#         \"simple_ensemble_val_acc\": float(val_acc_simple),\n",
    "#     })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 메타모델 정의\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "class MetaModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, hidden_dim=16):\n",
    "        super(MetaModel, self).__init__()\n",
    "        self.linear_1 = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        return x\n",
    "\n",
    "# 훈련 함수\n",
    "def train_model(model, train_loader, val_loader, device, epochs=60, lr=0.0001):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 훈련\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # 검증\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += batch_y.size(0)\n",
    "                val_correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}: Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # 최고 모델 복원\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ade6dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 메타모델 학습: Stratified K-Fold OOF\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# oof_preds = np.zeros((len(y_val), NUM_CLASSES), dtype=float)\n",
    "# fold_metrics = []\n",
    "# models = []\n",
    "\n",
    "# for fold, (tr_idx, va_idx) in enumerate(skf.split(X_val, y_val), start=1):\n",
    "#     X_tr, X_va = X_val[tr_idx], X_val[va_idx]\n",
    "#     y_tr, y_va = y_val[tr_idx], y_val[va_idx]\n",
    "    \n",
    "#     # PyTorch 데이터로더 생성\n",
    "#     train_dataset = TensorDataset(\n",
    "#         torch.FloatTensor(X_tr), \n",
    "#         torch.LongTensor(y_tr)\n",
    "#     )\n",
    "#     val_dataset = TensorDataset(\n",
    "#         torch.FloatTensor(X_va), \n",
    "#         torch.LongTensor(y_va)\n",
    "#     )\n",
    "    \n",
    "#     train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "#     # 모델 생성 및 훈련\n",
    "#     model = MetaModel(X_val.shape[1], NUM_CLASSES)\n",
    "#     model = train_model(model, train_loader, val_loader, device, epochs=100, lr=0.01)\n",
    "    \n",
    "#     # 검증 예측 및 메트릭\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         X_va_tensor = torch.FloatTensor(X_va).to(device)\n",
    "#         va_logits = model(X_va_tensor)\n",
    "#         va_proba = torch.softmax(va_logits, dim=1).cpu().numpy()\n",
    "#         va_pred = va_proba.argmax(axis=1)\n",
    "    \n",
    "#     acc = accuracy_score(y_va, va_pred)\n",
    "#     f1 = f1_score(y_va, va_pred, average=\"weighted\")\n",
    "\n",
    "#     oof_preds[va_idx] = va_proba\n",
    "#     fold_metrics.append({\"fold\": fold, \"acc\": acc, \"f1\": f1})\n",
    "#     models.append(model.state_dict().copy())\n",
    "\n",
    "#     print(f\"Fold {fold} -> Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
    "#     if USE_WANDB:\n",
    "#         wandb.log({\"fold\": fold, \"fold_acc\": acc, \"fold_f1\": f1})\n",
    "\n",
    "# # OOF 성능\n",
    "# oof_pred_labels = oof_preds.argmax(axis=1)\n",
    "# oof_acc = accuracy_score(y_val, oof_pred_labels)\n",
    "# oof_f1 = f1_score(y_val, oof_pred_labels, average=\"weighted\")\n",
    "# print(f\"\\nOOF Acc: {oof_acc:.4f}, OOF F1: {oof_f1:.4f}\")\n",
    "# if USE_WANDB:\n",
    "#     wandb.log({\"oof_acc\": oof_acc, \"oof_f1\": oof_f1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfdee7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Val Acc: 0.2900\n",
      "Epoch 20: Val Acc: 0.8800\n",
      "Epoch 40: Val Acc: 0.9000\n",
      "✅ 최종 메타모델 학습 완료\n"
     ]
    }
   ],
   "source": [
    "# 최종 메타모델 학습 (전체 val 사용) 및 Test 예측\n",
    "final_model = MetaModel(X_val.shape[1], NUM_CLASSES, hidden_dim=12)\n",
    "\n",
    "# 전체 val 데이터로 훈련\n",
    "train_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_val), \n",
    "    torch.LongTensor(y_val)\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 더미 검증 데이터로더 (전체 데이터로 훈련)\n",
    "val_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_val[:100]), \n",
    "    torch.LongTensor(y_val[:100])\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "final_model = train_model(final_model, train_loader, val_loader, device, epochs=60, lr=0.0001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"✅ 최종 메타모델 학습 완료\")\n",
    "if USE_WANDB:\n",
    "    wandb.log({\"meta_model_trained\": 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbeedea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Test 저장: ./predictions/ensemble_ori/ensemble_test_2025-10-30_09-38-01.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "    test_logits = final_model(X_test_tensor)\n",
    "    test_proba = torch.softmax(test_logits, dim=1).cpu().numpy()\n",
    "    test_pred = test_proba.argmax(axis=1)\n",
    "\n",
    "test_ids = extract_id_column(pd.read_csv(test_files[0])).values\n",
    "\n",
    "test_out = pd.DataFrame({\n",
    "    \"ID\": test_ids,\n",
    "    \"pred\": test_pred\n",
    "})\n",
    "for i in range(NUM_CLASSES):\n",
    "    test_out[f\"prob_{i}\"] = test_proba[:, i]\n",
    "\n",
    "test_path = os.path.join(OUTPUT_DIR, f\"ensemble_test_{TIMESTAMP}.csv\")\n",
    "test_out.to_csv(test_path, index=False)\n",
    "print(f\"💾 Test 저장: {test_path}\")\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.log({\"oof_saved\": 1, \"test_saved\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "119ed4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 메타모델 저장: ./predictions/ensemble_ori/ensemble_torch_2025-10-30_09-38-01.pth\n",
      "💾 설정 저장: ./predictions/ensemble_ori/ensemble_run_2025-10-30_09-38-28.json\n"
     ]
    }
   ],
   "source": [
    "# 모델 및 아티팩트 저장\n",
    "model_path = os.path.join(OUTPUT_DIR, f\"ensemble_torch_{TIMESTAMP}.pth\")\n",
    "torch.save(final_model.state_dict(), model_path)\n",
    "print(f\"💾 메타모델 저장: {model_path}\")\n",
    "\n",
    "\n",
    "TIMESTAMP_2 = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# 모델 구조도 저장 (로드 시 필요)\n",
    "model_config = {\n",
    "    \"input_dim\": X_val.shape[1],\n",
    "    \"num_classes\": NUM_CLASSES,\n",
    "    \"model_class\": \"MetaModel\"\n",
    "}\n",
    "config_path = os.path.join(OUTPUT_DIR, f\"ensemble_run_{TIMESTAMP_2}.json\")\n",
    "with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"timestamp\": TIMESTAMP,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"num_classes\": NUM_CLASSES,\n",
    "        \"val_files\": val_files,\n",
    "        \"test_files\": test_files,\n",
    "        \"model_config\": model_config\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "print(f\"💾 설정 저장: {config_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b5e8d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== WandB Sweep for hidden_dim (MetaModel) =====\n",
    "\n",
    "# # 주의: 아래 셀은 USE_WANDB=True 상태에서 실행하세요.\n",
    "# # sweep는 MetaModel의 hidden_dim만 탐색하고, 나머지 하이퍼파라미터는 고정합니다.\n",
    "\n",
    "# import math\n",
    "\n",
    "# def run_meta_cv_with_params(hidden_dim: int = 24, epochs: int = 80, lr: float = 0.01, folds: int = 5, batch_size: int = 64):\n",
    "#     skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=RANDOM_STATE)\n",
    "#     oof_preds_local = np.zeros((len(y_val), NUM_CLASSES), dtype=float)\n",
    "\n",
    "#     for tr_idx, va_idx in skf.split(X_val, y_val):\n",
    "#         X_tr, X_va = X_val[tr_idx], X_val[va_idx]\n",
    "#         y_tr, y_va = y_val[tr_idx], y_val[va_idx]\n",
    "\n",
    "#         train_dataset = TensorDataset(torch.FloatTensor(X_tr), torch.LongTensor(y_tr))\n",
    "#         val_dataset = TensorDataset(torch.FloatTensor(X_va), torch.LongTensor(y_va))\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#         val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#         model = MetaModel(X_val.shape[1], NUM_CLASSES, hidden_dim=hidden_dim)\n",
    "#         model = train_model(model, train_loader, val_loader, device, epochs=epochs, lr=lr)\n",
    "\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             X_va_tensor = torch.FloatTensor(X_va).to(device)\n",
    "#             va_logits = model(X_va_tensor)\n",
    "#             va_proba = torch.softmax(va_logits, dim=1).cpu().numpy()\n",
    "#         oof_preds_local[va_idx] = va_proba\n",
    "\n",
    "#     oof_pred_labels_local = oof_preds_local.argmax(axis=1)\n",
    "#     oof_acc_local = accuracy_score(y_val, oof_pred_labels_local)\n",
    "#     return float(oof_acc_local)\n",
    "\n",
    "# sweep_config = {\n",
    "#     \"name\": f\"meta_hidden_dim_sweep_{TIMESTAMP}\",\n",
    "#     \"method\": \"grid\",\n",
    "#     \"metric\": {\"name\": \"oof_acc\", \"goal\": \"maximize\"},\n",
    "#     \"parameters\": {\n",
    "#         \"hidden_dim\": {\"values\": [16, 24, 32]},\n",
    "#         \"epochs\": {\"values\": [40, 60]},\n",
    "#         \"lr\": {\"values\": [1e-4, 5e-5, 1e-5]},\n",
    "#     },\n",
    "#     \"early_terminate\": {\"type\": \"hyperband\", \"min_iter\": 10}\n",
    "# }\n",
    "\n",
    "# def sweep_train():\n",
    "#     with wandb.init(project=PROJECT_NAME, config={\"cv_folds\": 4, \"batch_size\": 64}):\n",
    "#         cfg = wandb.config\n",
    "#         hidden_dim = int(cfg.get(\"hidden_dim\", 12))\n",
    "#         epochs = int(cfg.get(\"epochs\", 80))\n",
    "#         lr = float(cfg.get(\"lr\", 0.01))\n",
    "#         folds = 4\n",
    "#         batch_size = 64\n",
    "\n",
    "#         oof_acc = run_meta_cv_with_params(\n",
    "#             hidden_dim=hidden_dim, epochs=epochs, lr=lr,\n",
    "#             folds=folds, batch_size=batch_size\n",
    "#         )\n",
    "#         wandb.log({\"oof_acc\": oof_acc, \"hidden_dim\": hidden_dim, \"epochs\": epochs, \"lr\": lr})\n",
    "#         print(f\"[SWEEP] hidden_dim={hidden_dim}, epochs={epochs}, lr={lr} -> OOF Acc: {oof_acc:.6f}\")\n",
    "\n",
    "# if USE_WANDB:\n",
    "#     sweep_id = wandb.sweep(sweep_config, project=PROJECT_NAME)\n",
    "#     wandb.agent(sweep_id, function=sweep_train)  # grid 전체 조합 자동 실행\n",
    "# else:\n",
    "#     print(\"⚠️ USE_WANDB=False: Sweep를 실행하려면 USE_WANDB를 True로 설정하세요.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c80fe1b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'oof_pred_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 결과 저장\u001b[39;00m\n\u001b[1;32m      2\u001b[0m val_out \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_ids,\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moof_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43moof_pred_labels\u001b[49m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_val\n\u001b[1;32m      6\u001b[0m })\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_CLASSES):\n\u001b[1;32m      8\u001b[0m     val_out[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moof_prob_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m oof_preds[:, i]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'oof_pred_labels' is not defined"
     ]
    }
   ],
   "source": [
    "# 결과 저장\n",
    "val_out = pd.DataFrame({\n",
    "    \"ID\": val_ids,\n",
    "    \"oof_pred\": oof_pred_labels,\n",
    "    \"label\": y_val\n",
    "})\n",
    "for i in range(NUM_CLASSES):\n",
    "    val_out[f\"oof_prob_{i}\"] = oof_preds[:, i]\n",
    "\n",
    "val_path = os.path.join(OUTPUT_DIR, f\"ensemble_oof_{TIMESTAMP}.csv\")\n",
    "val_out.to_csv(val_path, index=False)\n",
    "print(f\"💾 OOF 저장: {val_path}\")\n",
    "\n",
    "# Test 저장\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1985c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
