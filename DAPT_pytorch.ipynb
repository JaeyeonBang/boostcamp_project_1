{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 한국어 텍스트 감정 분석 - PyTorch 모델 학습\n",
        "\n",
        "이 노트북은 전처리된 데이터를 사용하여 PyTorch 기반의 BERT/RoBERTa 모델을 학습합니다.\n",
        "\n",
        "## 주요 기능\n",
        "- PyTorch 네이티브 학습 루프 (Transformers Trainer 대신)\n",
        "- LoRA 파인튜닝\n",
        "- WandB 실험 추적\n",
        "- 혼합 정밀도 학습 (Mixed Precision)\n",
        "- 조기 종료 및 모델 체크포인팅\n",
        "- 테스트 데이터 추론 및 제출 파일 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/ephemeral/home/py310/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "/data/ephemeral/home/py310/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 라이브러리 임포트 완료\n"
          ]
        }
      ],
      "source": [
        "# Library Import\n",
        "import os\n",
        "import math\n",
        "import warnings\n",
        "from collections import Counter\n",
        "import platform\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import LinearLR\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm.auto import tqdm\n",
        "import wandb\n",
        "\n",
        "# Transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModelForSequenceClassification,\n",
        "    set_seed,\n",
        "    AutoModelForMaskedLM\n",
        ")\n",
        "\n",
        "# PEFT for LoRA\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# 경고 메시지 필터링\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"✅ 라이브러리 임포트 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "EXPERIMENT_NAME = \"DAPT\"\n",
        "PROJECT_NAME = f\"[domain_project]_Experiment_{EXPERIMENT_NAME}\"\n",
        "MODEL_NAME = \"kykim_bert-kor-base\"\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "디바이스: cuda\n",
            "GPU 개수: 1\n",
            "   GPU 0: Tesla V100-SXM2-32GB\n"
          ]
        }
      ],
      "source": [
        "# 환경 설정\n",
        "RANDOM_STATE = 42\n",
        "set_seed(RANDOM_STATE)\n",
        "\n",
        "# GPU 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"디바이스: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU 개수: {torch.cuda.device_count()}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    print(\"⚠️  CUDA 사용 불가 - CPU로 훈련 진행\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# 이 함수가 .env 파일을 읽어서 환경 변수로 로드합니다.\n",
        "load_dotenv()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "molels = [\"beomi_kcbert-base\", \"klue_roberta-base\", \"klue_bert-base\",\"kykim_bert-base\", \"monologg_koelectra-base-v3-discriminator\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 하이퍼파라미터 설정 완료\n"
          ]
        }
      ],
      "source": [
        "# 하이퍼파라미터 설정\n",
        "\n",
        "class Config:\n",
        "    # 모델 설정\n",
        "    model_name = MODEL_NAME\n",
        "    base_models_path = \"/data/ephemeral/home/code/basemodels/\"\n",
        "    local_model_path = base_models_path + MODEL_NAME\n",
        "    num_classes = 4\n",
        "    max_length = 128\n",
        "    \n",
        "    # 훈련 설정\n",
        "    unsupervised_batch_size = 128\n",
        "    batch_size = 256\n",
        "    eval_batch_size = 256\n",
        "    num_mlm_epochs = 20\n",
        "    unsupervised_num_epochs = 20\n",
        "    num_epochs = 20\n",
        "    learning_rate = 2e-5\n",
        "    weight_decay = 0.01\n",
        "    warmup_steps = 500\n",
        "    \n",
        "    # 기타 설정\n",
        "    gradient_accumulation_steps = 1\n",
        "    max_grad_norm = 1.0\n",
        "    early_stopping_patience = 5\n",
        "    save_best_model = True\n",
        "    \n",
        "    # WandB 설정\n",
        "    use_wandb = True\n",
        "    project_name = PROJECT_NAME\n",
        "    run_name = f\"{EXPERIMENT_NAME}-training\"\n",
        "\n",
        "config = Config()\n",
        "print(\"✅ 하이퍼파라미터 설정 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "전처리된 데이터 로드 중...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "훈련 데이터: 221,016개\n",
            "검증 데이터: 55,255개\n",
            "테스트 데이터: 59,928개\n",
            "✅ 데이터 로드 완료\n"
          ]
        }
      ],
      "source": [
        "# 데이터 로드\n",
        "print(\"전처리된 데이터 로드 중...\")\n",
        "\n",
        "# 훈련 데이터 로드\n",
        "train_df = pd.read_csv(\"data/train_processed.csv\")\n",
        "print(f\"훈련 데이터: {len(train_df):,}개\")\n",
        "\n",
        "# 검증 데이터 로드\n",
        "val_df = pd.read_csv(\"data/val_processed.csv\")\n",
        "print(f\"검증 데이터: {len(val_df):,}개\")\n",
        "\n",
        "test_df = pd.read_csv(\"data/test_processed.csv\")\n",
        "print(f\"테스트 데이터: {len(test_df):,}개\")\n",
        "\n",
        "# 라벨 매핑\n",
        "LABEL_MAPPING = {0: \"강한 부정\", 1: \"약한 부정\", 2: \"약한 긍정\", 3: \"강한 긍정\"}\n",
        "\n",
        "print(\"✅ 데이터 로드 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 데이터셋 클래스 정의 완료\n"
          ]
        }
      ],
      "source": [
        "# 데이터셋 클래스 정의\n",
        "class ReviewDataset(Dataset):\n",
        "    \"\"\"\n",
        "    리뷰 텍스트 데이셋 클래스\n",
        "    - BERT 모델 훈련/추론을 위한 PyTorch Dataset 구현\n",
        "    - 텍스트 토크나이징 및 텐서 변환 처리\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        \"\"\"\n",
        "        데이터셋 초기화\n",
        "        \"\"\"\n",
        "        self.texts, self.labels, self.tokenizer, self.max_length = (\n",
        "            texts,\n",
        "            labels,\n",
        "            tokenizer,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"데이터셋 크기 반환\"\"\"\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        특정 인덱스의 데이터 아이템 반환\n",
        "        \"\"\"\n",
        "        # 텍스트 토크나이징 및 패딩\n",
        "        encoding = self.tokenizer(\n",
        "            str(self.texts.iloc[idx]),\n",
        "            truncation=True,  # 최대 길이 초과시 자르기\n",
        "            padding=\"max_length\",  # 최대 길이까지 패딩\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",  # PyTorch 텐서로 반환\n",
        "        )\n",
        "\n",
        "        # 기본 아이템 구성 (input_ids, attention_mask)\n",
        "        item = {\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "        }\n",
        "\n",
        "        # labels가 None이 아닌 경우에만 추가 (train/valid용)\n",
        "        if self.labels is not None:\n",
        "            item[\"labels\"] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
        "\n",
        "        return item\n",
        "\n",
        "print(\"✅ 데이터셋 클래스 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UnlabeledDataset(Dataset):\n",
        "    \"\"\"\n",
        "    라벨이 없는 데이터셋 클래스 (MLM 마스킹 지원)\n",
        "    \"\"\"\n",
        "    def __init__(self, texts, tokenizer, max_length, mask_probability=0.15):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.mask_probability = mask_probability\n",
        "        self.mask_token_id = tokenizer.mask_token_id\n",
        "        self.vocab_size = tokenizer.vocab_size\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def _apply_mlm_masking(self, input_ids, attention_mask):\n",
        "        \"\"\"MLM 스타일 마스킹 적용\"\"\"\n",
        "        labels = input_ids.clone()\n",
        "        \n",
        "        # 패딩 토큰이 아닌 위치만 선택\n",
        "        non_padding_mask = attention_mask.bool()\n",
        "        \n",
        "        # 마스킹할 토큰 선택 (패딩 토큰 제외)\n",
        "        candidate_indices = torch.where(non_padding_mask)[0]\n",
        "        \n",
        "        if len(candidate_indices) == 0:\n",
        "            return input_ids, labels\n",
        "        \n",
        "        # 마스킹할 토큰 수 계산\n",
        "        num_tokens_to_mask = max(1, int(len(candidate_indices) * self.mask_probability))\n",
        "        \n",
        "        # 랜덤하게 토큰 선택\n",
        "        selected_indices = torch.randperm(len(candidate_indices))[:num_tokens_to_mask]\n",
        "        selected_positions = candidate_indices[selected_indices]\n",
        "        \n",
        "        # 선택된 토큰에 대해 마스킹 적용\n",
        "        for pos in selected_positions:\n",
        "            # 80% 확률로 [MASK] 토큰으로 교체\n",
        "            if torch.rand(1) < 0.8:\n",
        "                input_ids[pos] = self.mask_token_id\n",
        "            # 10% 확률로 랜덤 토큰으로 교체\n",
        "            elif torch.rand(1) < 0.1:\n",
        "                input_ids[pos] = torch.randint(0, self.vocab_size, (1,))\n",
        "            # 10% 확률로 원본 토큰 유지\n",
        "        \n",
        "        return input_ids, labels\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"인덱스에 해당하는 샘플 반환 (MLM 마스킹 적용)\"\"\"\n",
        "        # 텍스트 추출 - 문자열로 변환 보장\n",
        "        if hasattr(self.texts, 'iloc'):\n",
        "            text = str(self.texts.iloc[idx])  # pandas Series인 경우\n",
        "        else:\n",
        "            text = str(self.texts[idx])  # list나 다른 타입인 경우\n",
        "        \n",
        "        # None이나 NaN 값 처리\n",
        "        if text == 'None' or text == 'nan' or text == '':\n",
        "            text = \"빈 텍스트입니다.\"  # 기본값 설정\n",
        "        \n",
        "        # 텍스트 토크나이징\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        input_ids = encoding['input_ids'].flatten()\n",
        "        attention_mask = encoding['attention_mask'].flatten()\n",
        "        \n",
        "        # MLM 마스킹 적용\n",
        "        masked_input_ids, labels = self._apply_mlm_masking(input_ids, attention_mask)\n",
        "        \n",
        "        return {\n",
        "            'input_ids': masked_input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels  # MLM 학습을 위한 라벨\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 평가 메트릭 함수 정의 완료\n"
          ]
        }
      ],
      "source": [
        "# 평가 메트릭 함수\n",
        "def compute_metrics(predictions, labels):\n",
        "    \"\"\"\n",
        "    모델 평가 메트릭 계산 함수\n",
        "    \"\"\"\n",
        "    # 예측값에서 가장 높은 확률의 클래스 선택\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    \n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
        "    \n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "    }\n",
        "\n",
        "print(\"✅ 평가 메트릭 함수 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_load_from_local(local_model_path : str = None, model_name : str = None):\n",
        "    # 1. 토크나이저 로드\n",
        "    # 절대 경로를 사용하여 로컬 모델 로드\n",
        "    print(\"로컬 경로에서 토크나이저 로딩 중...\")\n",
        "\n",
        "    # 경로가 존재하는지 확인\n",
        "    if not os.path.exists(local_model_path):\n",
        "        print(f\"❌ 경로가 존재하지 않습니다: {local_model_path}\")\n",
        "        return None, None\n",
        "    else:\n",
        "        print(f\"✅ 경로 확인됨: {local_model_path}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
        "\n",
        "    # 2. 모델 로드\n",
        "    # 마찬가지로 로컬 경로(local_model_path)를 사용\n",
        "    print(\"로컬 경로에서 모델 로딩 중...\")\n",
        "    model = AutoModelForMaskedLM.from_pretrained(\n",
        "        local_model_path,\n",
        "        num_labels=7  # 예: KLUE-TC 감성 분석 클래스 개수\n",
        "    )\n",
        "\n",
        "    print(\"✅ 로컬 스냅샷에서 모델과 토크나이저 로딩 성공!\")\n",
        "\n",
        "    # --- 이제 평소처럼 모델을 사용할 수 있습니다 ---\n",
        "    inputs = tokenizer(\"이 영화 정말 재미있네요!\", return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    print(outputs.logits)\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "로컬 경로에서 토크나이저 로딩 중...\n",
            "✅ 경로 확인됨: /data/ephemeral/home/code/basemodels/kykim_bert-kor-base\n",
            "로컬 경로에서 모델 로딩 중...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /data/ephemeral/home/code/basemodels/kykim_bert-kor-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 로컬 스냅샷에서 모델과 토크나이저 로딩 성공!\n",
            "tensor([[[ -8.7732,  -5.3598,  -4.4707,  ...,  -5.5349,  -4.1685,  -6.2984],\n",
            "         [-13.5281,  -7.3256,  -5.6978,  ...,  -4.6070,  -7.2017, -13.3684],\n",
            "         [-13.5850, -10.6456,  -6.2533,  ...,  -5.0207,  -6.9485, -10.5358],\n",
            "         ...,\n",
            "         [-13.1823,  -8.8666,  -4.9712,  ..., -10.3408,  -7.1187, -11.3603],\n",
            "         [-11.3309,  -6.7285,  -2.5937,  ...,  -6.8013,  -4.9377, -10.5218],\n",
            "         [ -9.3873,  -4.1437,  -6.7594,  ...,  -3.4999,  -1.9037,  -9.5742]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = model_load_from_local(config.local_model_path, config.model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 모델 및 토크나이저 로드 완료\n"
          ]
        }
      ],
      "source": [
        "# LoRA 설정 적용\n",
        "# print(\"LoRA 설정 적용 중...\")\n",
        "# peft_config = LoraConfig(\n",
        "#     task_type=TaskType.SEQ_CLS,\n",
        "#     r=config.lora_r,\n",
        "#     lora_alpha=config.lora_alpha,\n",
        "#     target_modules=[\"query\", \"key\", \"value\"],\n",
        "#     lora_dropout=config.lora_dropout,\n",
        "#     bias=\"none\",\n",
        "# )\n",
        "\n",
        "# model = get_peft_model(model, peft_config)\n",
        "# model.print_trainable_parameters()\n",
        "\n",
        "# 모델을 디바이스로 이동\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"✅ 모델 및 토크나이저 로드 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(42000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=42000, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "데이터셋 및 데이터로더 생성 중...\n",
            "훈련 데이터: 221,016개\n",
            "검증 데이터: 55,255개\n",
            "Unlabeled 데이터: 336,199개\n",
            "✅ 데이터셋 및 데이터로더 생성 완료\n"
          ]
        }
      ],
      "source": [
        "# 데이터셋 및 데이터로더 생성\n",
        "print(\"데이터셋 및 데이터로더 생성 중...\")\n",
        "\n",
        "unlabeled_df = pd.DataFrame(pd.concat([\n",
        "    train_df[\"review\"], \n",
        "    val_df[\"review\"], \n",
        "    test_df[\"review\"]\n",
        "], ignore_index=True))\n",
        "\n",
        "\n",
        "# 훈련 데이터셋 생성\n",
        "train_dataset = ReviewDataset(\n",
        "    train_df[\"review\"],\n",
        "    train_df[\"label\"],\n",
        "    tokenizer,\n",
        "    config.max_length,\n",
        ")\n",
        "\n",
        "# 검증 데이터셋 생성\n",
        "val_dataset = ReviewDataset(\n",
        "    val_df[\"review\"],\n",
        "    val_df[\"label\"],\n",
        "    tokenizer,\n",
        "    config.max_length,\n",
        ")\n",
        "\n",
        "unlabeled_dataset = UnlabeledDataset(\n",
        "    unlabeled_df[\"review\"],\n",
        "    tokenizer,\n",
        "    config.max_length,\n",
        ")\n",
        "\n",
        "# 데이터로더 생성\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config.eval_batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "unlabeled_dataloader = DataLoader(\n",
        "    unlabeled_dataset,\n",
        "    batch_size=config.unsupervised_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"훈련 데이터: {len(train_dataset):,}개\")\n",
        "print(f\"검증 데이터: {len(val_dataset):,}개\")\n",
        "print(f\"Unlabeled 데이터: {len(unlabeled_dataset):,}개\")\n",
        "print(\"✅ 데이터셋 및 데이터로더 생성 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "옵티마이저 및 스케줄러 설정 중...\n",
            "총 훈련 스텝: 864\n",
            "워밍업 스텝: 500\n",
            "✅ 옵티마이저 및 스케줄러 설정 완료\n"
          ]
        }
      ],
      "source": [
        "# 옵티마이저 및 스케줄러 설정\n",
        "print(\"옵티마이저 및 스케줄러 설정 중...\")\n",
        "\n",
        "# 옵티마이저 설정\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config.learning_rate,\n",
        "    weight_decay=config.weight_decay\n",
        ")\n",
        "\n",
        "# 전체 훈련 스텝 계산\n",
        "total_steps = len(train_dataloader) * config.num_epochs // config.gradient_accumulation_steps\n",
        "\n",
        "# 스케줄러 설정 (warmup + linear decay)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=config.warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# 혼합 정밀도 스케일러\n",
        "scaler = GradScaler() if torch.cuda.is_available() else None\n",
        "\n",
        "print(f\"총 훈련 스텝: {total_steps:,}\")\n",
        "print(f\"워밍업 스텝: {config.warmup_steps:,}\")\n",
        "print(\"✅ 옵티마이저 및 스케줄러 설정 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/data/ephemeral/home/code/wandb/run-20251023_093825-d82pdoh7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Experiment_DAPT/runs/d82pdoh7' target=\"_blank\">DAPT-training</a></strong> to <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Experiment_DAPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Experiment_DAPT' target=\"_blank\">https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Experiment_DAPT</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Experiment_DAPT/runs/d82pdoh7' target=\"_blank\">https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Experiment_DAPT/runs/d82pdoh7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ WandB 초기화 완료\n"
          ]
        }
      ],
      "source": [
        "# WandB 초기화\n",
        "if config.use_wandb:\n",
        "    wandb.init(\n",
        "        project=config.project_name,\n",
        "        name=config.run_name,\n",
        "        config={\n",
        "            \"model_name\": config.model_name,\n",
        "            \"num_epochs\": config.num_epochs,\n",
        "            \"batch_size\": config.batch_size,\n",
        "            \"learning_rate\": config.learning_rate,\n",
        "            \"max_length\": config.max_length,\n",
        "            \"num_classes\": config.num_classes,\n",
        "            \"warmup_steps\": config.warmup_steps,\n",
        "            \"weight_decay\": config.weight_decay,\n",
        "            # \"lora_r\": config.lora_r,\n",
        "            # \"lora_alpha\": config.lora_alpha,\n",
        "            # \"lora_dropout\": config.lora_dropout,\n",
        "            \"random_seed\": RANDOM_STATE\n",
        "        }\n",
        "    )\n",
        "    print(\"✅ WandB 초기화 완료\")\n",
        "else:\n",
        "    print(\"⚠️  WandB 사용 안함\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def unsupervie_train_epoch(model, dataloader, optimizer, scheduler, scaler, device, config):\n",
        "    \"\"\"한 에포크 훈련 (MLM 비지도 학습)\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    progress_bar = tqdm(dataloader, desc=\"Unsupervised Training\")\n",
        "    \n",
        "    for step, batch in enumerate(progress_bar):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)  # MLM을 위한 원본 토큰 라벨\n",
        "        \n",
        "        # 그래디언트 초기화\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # 혼합 정밀도 학습\n",
        "        if scaler is not None:\n",
        "            with autocast():\n",
        "                # MLM 손실 계산 - labels를 올바르게 처리\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels  # labels는 input_ids와 동일한 shape이어야 함\n",
        "                )\n",
        "                loss = outputs.loss\n",
        "                \n",
        "            # 스케일된 그래디언트 계산\n",
        "            scaler.scale(loss).backward()\n",
        "            \n",
        "            # 그래디언트 클리핑\n",
        "            if config.max_grad_norm > 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
        "            \n",
        "            # 옵티마이저 스텝\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            \n",
        "        else:\n",
        "            # 일반 정밀도 학습\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            \n",
        "            # 역전파\n",
        "            loss.backward()\n",
        "            \n",
        "            # 그래디언트 클리핑\n",
        "            if config.max_grad_norm > 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
        "            \n",
        "            # 옵티마이저 스텝\n",
        "            optimizer.step()\n",
        "        \n",
        "        # 스케줄러 스텝\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "        # 손실 누적\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "        # 진행률 표시 업데이트\n",
        "        avg_loss = total_loss / num_batches\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f'{avg_loss:.4f}',\n",
        "            'lr': f'{optimizer.param_groups[0][\"lr\"]:.2e}'\n",
        "        })\n",
        "        \n",
        "        # 그래디언트 누적 (선택사항)\n",
        "        if config.gradient_accumulation_steps > 1:\n",
        "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                if scheduler is not None:\n",
        "                    scheduler.step()\n",
        "    \n",
        "    # 평균 손실 계산\n",
        "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
        "    \n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, scheduler, scaler, device, config):\n",
        "    \"\"\"한 에포크 훈련\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    \n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "    \n",
        "    for step, batch in enumerate(progress_bar):\n",
        "        # 배치를 디바이스로 이동\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        batch_labels = batch[\"labels\"].to(device)\n",
        "        \n",
        "        # 그래디언트 초기화\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # 혼합 정밀도 훈련\n",
        "        if scaler is not None:\n",
        "            with autocast():\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=batch_labels\n",
        "                )\n",
        "                loss = outputs.loss\n",
        "                logits = outputs.logits\n",
        "            \n",
        "            # 스케일된 그래디언트 계산\n",
        "            scaler.scale(loss).backward()\n",
        "            \n",
        "            # 그래디언트 클리핑 (조건부)\n",
        "            if config.max_grad_norm > 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
        "            \n",
        "            # 옵티마이저 스텝\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            \n",
        "        else:\n",
        "            # 일반 정밀도 학습\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=batch_labels\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "            \n",
        "            # 역전파\n",
        "            loss.backward()\n",
        "            \n",
        "            # 그래디언트 클리핑\n",
        "            if config.max_grad_norm > 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
        "            \n",
        "            # 옵티마이저 스텝\n",
        "            optimizer.step()\n",
        "        \n",
        "        # 스케줄러 스텝\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "        # 손실 누적\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        # 예측값 저장\n",
        "        predictions.extend(logits.detach().cpu().numpy())\n",
        "        labels.extend(batch_labels.detach().cpu().numpy())\n",
        "        \n",
        "        # 진행률 업데이트\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
        "        })\n",
        "    \n",
        "    # 메트릭 계산\n",
        "    metrics = compute_metrics(predictions, labels)\n",
        "    metrics['loss'] = total_loss / len(dataloader)\n",
        "    \n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 검증 함수 정의 완료\n"
          ]
        }
      ],
      "source": [
        "# 검증 함수\n",
        "def validate_epoch(model, dataloader, device):\n",
        "    \"\"\"한 에포크 검증\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    \n",
        "    progress_bar = tqdm(dataloader, desc=\"Validation\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            # 배치를 디바이스로 이동\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            batch_labels = batch[\"labels\"].to(device)\n",
        "            \n",
        "            # 순전파\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=batch_labels\n",
        "            )\n",
        "            \n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # 예측값 저장\n",
        "            predictions.extend(logits.detach().cpu().numpy())\n",
        "            labels.extend(batch_labels.detach().cpu().numpy())\n",
        "            \n",
        "            # 진행률 업데이트\n",
        "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "    \n",
        "    # 메트릭 계산\n",
        "    metrics = compute_metrics(predictions, labels)\n",
        "    metrics['loss'] = total_loss / len(dataloader)\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "print(\"✅ 검증 함수 정의 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def unsupervised_training_loop(model, unlabeled_dataloader, optimizer, scheduler, scaler, device, config):\n",
        "    \"\"\"비지도 학습 메인 루프\"\"\"\n",
        "    \n",
        "    # 초기화\n",
        "    best_unsup_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    \n",
        "    print(\"🚀 비지도 학습 시작...\")\n",
        "    print(f\"총 에포크: {config.num_epochs}\")\n",
        "    print(f\"조기 종료 인내심: {config.early_stopping_patience}\")\n",
        "    \n",
        "    for epoch in range(config.unsupervised_num_epochs):\n",
        "        print(f\"\\n=== Epoch {epoch+1}/{config.unsupervised_num_epochs} ===\")\n",
        "        \n",
        "        # 비지도 학습 에포크\n",
        "        unsup_loss = unsupervie_train_epoch(\n",
        "            model=model,\n",
        "            dataloader=unlabeled_dataloader,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            scaler=scaler,\n",
        "            device=device,\n",
        "            config=config\n",
        "        )\n",
        "        \n",
        "        print(f\"MLM Loss: {unsup_loss:.4f}\")\n",
        "        \n",
        "        # 모델 저장 조건 (손실이 개선되었을 때)\n",
        "        if config.save_best_model and unsup_loss < best_unsup_loss:\n",
        "            best_unsup_loss = unsup_loss\n",
        "            patience_counter = 0\n",
        "            \n",
        "            # 모델 저장\n",
        "            os.makedirs(\"./best_unsupervised_model\", exist_ok=True)\n",
        "            model.save_pretrained(\"./best_unsupervised_model\")\n",
        "            tokenizer.save_pretrained(\"./best_unsupervised_model\")\n",
        "            print(f\"✅ 최고 성능 비지도 모델 저장 (MLM Loss: {best_unsup_loss:.4f})\")\n",
        "            \n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"⏳ 조기 종료 카운터: {patience_counter}/{config.early_stopping_patience}\")\n",
        "        \n",
        "        # WandB 로깅 (선택사항)\n",
        "        if config.use_wandb:\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"unsupervised_loss\": unsup_loss,\n",
        "                \"best_unsupervised_loss\": best_unsup_loss,\n",
        "                \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "            })\n",
        "        \n",
        "        # 조기 종료 체크\n",
        "        if patience_counter >= config.early_stopping_patience:\n",
        "            print(f\"🛑 조기 종료: {config.early_stopping_patience} 에포크 동안 개선 없음\")\n",
        "            break\n",
        "    \n",
        "    print(f\"\\n🎯 비지도 학습 완료!\")\n",
        "    print(f\"최고 MLM Loss: {best_unsup_loss:.4f}\")\n",
        "    \n",
        "    return best_unsup_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 비지도 학습 시작...\n",
            "총 에포크: 1\n",
            "조기 종료 인내심: 5\n",
            "\n",
            "=== Epoch 1/1 ===\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf2170d3e5064c21ad3387240d3e9d31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsupervised Training:   0%|          | 0/2627 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLM Loss: 0.2915\n",
            "✅ 최고 성능 비지도 모델 저장 (MLM Loss: 0.2915)\n",
            "\n",
            "🎯 비지도 학습 완료!\n",
            "최고 MLM Loss: 0.2915\n"
          ]
        }
      ],
      "source": [
        "best_loss = unsupervised_training_loop(\n",
        "    model=model, \n",
        "    unlabeled_dataloader=unlabeled_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    scaler=scaler,\n",
        "    device=device,\n",
        "    config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./best_unsupervised_model and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "비지도 사전 훈련 모델 기반 지도 학습 시작\n",
            "==================================================\n",
            "🔄 비지도 사전 훈련 모델 로드 중...\n",
            "✅ 비지도 사전 훈련 모델 발견, 로드 중...\n",
            "✅ 비지도 사전 훈련 모델 로드 완료\n",
            "훈련 샘플: 221,016개\n",
            "검증 샘플: 55,255개\n",
            "훈련 에포크: 1회\n",
            "배치 크기: 256 (훈련) / 256 (검증)\n",
            "학습률: 2e-05\n",
            "시드값: 42\n",
            "디바이스: cuda\n",
            "WandB 사용: True\n",
            "\n",
            "==================== Epoch 1/1 ====================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aec69610ee02494e8f0468fdcf8ad3ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/864 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "❌ 훈련 중 오류 발생: unscale_() has already been called on this optimizer since the last update().\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "unscale_() has already been called on this optimizer since the last update().",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[39], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# 훈련\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# 검증\u001b[39;00m\n\u001b[1;32m     54\u001b[0m val_metrics \u001b[38;5;241m=\u001b[39m validate_epoch(model, val_dataloader, device)\n",
            "Cell \u001b[0;32mIn[38], line 35\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, scheduler, scaler, device, config)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# 그래디언트 클리핑 (조건부)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mmax_grad_norm \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 옵티마이저 스텝\u001b[39;00m\n",
            "File \u001b[0;32m~/py310/lib/python3.10/site-packages/torch/amp/grad_scaler.py:327\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    324\u001b[0m optimizer_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_per_optimizer_states[\u001b[38;5;28mid\u001b[39m(optimizer)]\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m OptState\u001b[38;5;241m.\u001b[39mUNSCALED:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munscale_() has already been called on this optimizer since the last update().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    329\u001b[0m     )\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m OptState\u001b[38;5;241m.\u001b[39mSTEPPED:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munscale_() is being called after step().\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: unscale_() has already been called on this optimizer since the last update()."
          ]
        }
      ],
      "source": [
        "# 메인 훈련 루프 (비지도 사전 훈련 모델 기반)\n",
        "print(\"=\" * 50)\n",
        "print(\"비지도 사전 훈련 모델 기반 지도 학습 시작\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 비지도 사전 훈련 모델 로드\n",
        "print(\"🔄 비지도 사전 훈련 모델 로드 중...\")\n",
        "if os.path.exists(\"./best_unsupervised_model\"):\n",
        "    print(\"✅ 비지도 사전 훈련 모델 발견, 로드 중...\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"./best_unsupervised_model\",\n",
        "        num_labels=config.num_classes\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"./best_unsupervised_model\")\n",
        "    print(\"✅ 비지도 사전 훈련 모델 로드 완료\")\n",
        "else:\n",
        "    print(\"⚠️  비지도 사전 훈련 모델이 없습니다. 기본 모델을 사용합니다.\")\n",
        "    # 기본 모델 로드 (기존 코드)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        config.local_model_path,\n",
        "        num_labels=config.num_classes\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.local_model_path)\n",
        "\n",
        "# 모델을 디바이스로 이동\n",
        "model = model.to(device)\n",
        "\n",
        "# 훈련 정보 출력\n",
        "print(f\"훈련 샘플: {len(train_dataset):,}개\")\n",
        "print(f\"검증 샘플: {len(val_dataset):,}개\")\n",
        "print(f\"훈련 에포크: {config.num_epochs}회\")\n",
        "print(f\"배치 크기: {config.batch_size} (훈련) / {config.eval_batch_size} (검증)\")\n",
        "print(f\"학습률: {config.learning_rate}\")\n",
        "print(f\"시드값: {RANDOM_STATE}\")\n",
        "print(f\"디바이스: {device}\")\n",
        "print(f\"WandB 사용: {config.use_wandb}\")\n",
        "\n",
        "# 조기 종료 및 체크포인팅 설정\n",
        "best_val_accuracy = 0\n",
        "patience_counter = 0\n",
        "training_history = []\n",
        "\n",
        "# 훈련 시작\n",
        "try:\n",
        "    for epoch in range(config.num_epochs):\n",
        "        print(f\"\\n{'='*20} Epoch {epoch+1}/{config.num_epochs} {'='*20}\")\n",
        "        \n",
        "        # 훈련\n",
        "        train_metrics = train_epoch(\n",
        "            model, train_dataloader, optimizer, scheduler, scaler, device, config\n",
        "        )\n",
        "        \n",
        "        # 검증\n",
        "        val_metrics = validate_epoch(model, val_dataloader, device)\n",
        "        \n",
        "        # 결과 출력\n",
        "        print(f\"\\n훈련 결과:\")\n",
        "        print(f\"  Loss: {train_metrics['loss']:.4f}\")\n",
        "        print(f\"  Accuracy: {train_metrics['accuracy']:.4f}\")\n",
        "        print(f\"  F1: {train_metrics['f1']:.4f}\")\n",
        "        \n",
        "        print(f\"\\n검증 결과:\")\n",
        "        print(f\"  Loss: {val_metrics['loss']:.4f}\")\n",
        "        print(f\"  Accuracy: {val_metrics['accuracy']:.4f}\")\n",
        "        print(f\"  F1: {val_metrics['f1']:.4f}\")\n",
        "        \n",
        "        # WandB 로깅\n",
        "        if config.use_wandb:\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train_loss\": train_metrics['loss'],\n",
        "                \"train_accuracy\": train_metrics['accuracy'],\n",
        "                \"train_f1\": train_metrics['f1'],\n",
        "                \"val_loss\": val_metrics['loss'],\n",
        "                \"val_accuracy\": val_metrics['accuracy'],\n",
        "                \"val_f1\": val_metrics['f1'],\n",
        "                \"learning_rate\": scheduler.get_last_lr()[0]\n",
        "            })\n",
        "        \n",
        "        # 히스토리 저장\n",
        "        training_history.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_metrics['loss'],\n",
        "            'train_accuracy': train_metrics['accuracy'],\n",
        "            'train_f1': train_metrics['f1'],\n",
        "            'val_loss': val_metrics['loss'],\n",
        "            'val_accuracy': val_metrics['accuracy'],\n",
        "            'val_f1': val_metrics['f1']\n",
        "        })\n",
        "        \n",
        "        # 최고 성능 모델 저장\n",
        "        if config.save_best_model and val_metrics['accuracy'] > best_val_accuracy:\n",
        "            best_val_accuracy = val_metrics['accuracy']\n",
        "            patience_counter = 0\n",
        "            \n",
        "            # 모델 저장 (지도 학습 완료 모델)\n",
        "            os.makedirs(\"./best_supervised_model\", exist_ok=True)\n",
        "            model.save_pretrained(\"./best_supervised_model\")\n",
        "            tokenizer.save_pretrained(\"./best_supervised_model\")\n",
        "            print(f\"✅ 최고 성능 지도 학습 모델 저장 (Accuracy: {best_val_accuracy:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        # 조기 종료 체크\n",
        "        if patience_counter >= config.early_stopping_patience:\n",
        "            print(f\"\\n⚠️  조기 종료: {config.early_stopping_patience} 에포크 동안 개선 없음\")\n",
        "            break\n",
        "    \n",
        "    print(f\"\\n✅ 지도 학습 완료!\")\n",
        "    print(f\"최고 검증 정확도: {best_val_accuracy:.4f}\")\n",
        "    \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n⚠️  사용자에 의해 훈련이 중단되었습니다.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ 훈련 중 오류 발생: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 테스트 데이터 추론\n",
        "print(\"테스트 데이터 추론 시작...\")\n",
        "\n",
        "# 테스트 데이터 로드\n",
        "test_df = pd.read_csv(\"data/test.csv\")\n",
        "print(f\"테스트 데이터: {len(test_df):,}개\")\n",
        "\n",
        "# 최고 성능 모델 로드\n",
        "if os.path.exists(\"./best_model\"):\n",
        "    print(\"최고 성능 모델 로드 중...\")\n",
        "    inference_model = AutoModelForSequenceClassification.from_pretrained(\"./best_model\")\n",
        "    inference_tokenizer = AutoTokenizer.from_pretrained(\"./best_model\")\n",
        "    inference_model = inference_model.to(device)\n",
        "    inference_model.eval()\n",
        "    print(\"✅ 최고 성능 모델 로드 완료\")\n",
        "else:\n",
        "    print(\"⚠️  저장된 모델이 없습니다. 현재 모델 사용\")\n",
        "    inference_model = model\n",
        "    inference_tokenizer = tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 테스트 데이터셋 및 데이터로더 생성\n",
        "test_dataset = ReviewDataset(\n",
        "    test_df[\"review\"],\n",
        "    None,  # 테스트 데이터는 라벨 없음\n",
        "    inference_tokenizer,\n",
        "    config.max_length,\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config.eval_batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(\"✅ 테스트 데이터셋 준비 완료\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 추론 실행\n",
        "print(\"추론 실행 중...\")\n",
        "inference_model.eval()\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dataloader, desc=\"Inference\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        \n",
        "        outputs = inference_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        \n",
        "        logits = outputs.logits\n",
        "        batch_predictions = torch.argmax(logits, dim=-1)\n",
        "        predictions.extend(batch_predictions.cpu().numpy())\n",
        "\n",
        "print(f\"✅ 추론 완료: {len(predictions):,}개 예측\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 제출 파일 생성\n",
        "print(\"제출 파일 생성 중...\")\n",
        "\n",
        "# 예측 결과를 데이터프레임에 추가\n",
        "test_df[\"pred\"] = predictions\n",
        "\n",
        "# 클래스별 예측 분포 확인\n",
        "unique_predictions, counts = np.unique(predictions, return_counts=True)\n",
        "print(\"\\n클래스별 예측 분포:\")\n",
        "for pred, count in zip(unique_predictions, counts):\n",
        "    percentage = (count / len(predictions)) * 100\n",
        "    class_name = LABEL_MAPPING.get(pred, f\"클래스 {pred}\")\n",
        "    print(f\"   {class_name} ({pred}): {count:,}개 ({percentage:.1f}%)\")\n",
        "\n",
        "# 샘플 제출 파일 로드\n",
        "sample_submission = pd.read_csv(\"data/sample_submission.csv\")\n",
        "\n",
        "# ID를 기준으로 병합하여 제출 파일 생성\n",
        "submission_df = sample_submission[[\"ID\"]].merge(\n",
        "    test_df[[\"ID\", \"pred\"]], \n",
        "    left_on=\"ID\", \n",
        "    right_on=\"ID\", \n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# 제출 파일 검증\n",
        "assert len(submission_df) == len(sample_submission), f\"길이 불일치: {len(submission_df)} vs {len(sample_submission)}\"\n",
        "assert submission_df[\"pred\"].isin([0, 1, 2, 3]).all(), \"모든 예측값은 [0, 1, 2, 3] 범위에 있어야 합니다\"\n",
        "assert not submission_df[\"pred\"].isnull().any(), \"예측값에 null 값이 있으면 안됩니다\"\n",
        "assert not submission_df[\"ID\"].isnull().any(), \"ID 컬럼에 null 값이 있으면 안됩니다\"\n",
        "\n",
        "print(\"✅ 제출 파일 검증 통과\")\n",
        "\n",
        "# 제출 파일 저장\n",
        "submission_path = \"./output.csv\"\n",
        "submission_df.to_csv(submission_path, index=False)\n",
        "print(f\"✅ 제출 파일 저장 완료: {submission_path}\")\n",
        "\n",
        "# WandB 종료\n",
        "if config.use_wandb:\n",
        "    wandb.finish()\n",
        "    print(\"✅ WandB 세션 종료\")\n",
        "\n",
        "print(\"\\n🎉 모든 작업 완료!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 테스트 데이터 추론\n",
        "print(\"=\" * 50)\n",
        "print(\"테스트 데이터 추론\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 테스트 데이터 로드\n",
        "test_df = pd.read_csv(\"data/test.csv\")\n",
        "print(f\"테스트 데이터: {len(test_df):,}개\")\n",
        "\n",
        "# 최고 성능 모델 로드\n",
        "if os.path.exists(\"./best_model\"):\n",
        "    print(\"최고 성능 모델 로드 중...\")\n",
        "    inference_model = AutoModelForSequenceClassification.from_pretrained(\"./best_model\")\n",
        "    inference_tokenizer = AutoTokenizer.from_pretrained(\"./best_model\")\n",
        "    inference_model = inference_model.to(device)\n",
        "    inference_model.eval()\n",
        "    print(\"✅ 최고 성능 모델 로드 완료\")\n",
        "else:\n",
        "    print(\"⚠️  저장된 모델이 없습니다. 현재 모델 사용\")\n",
        "    inference_model = model\n",
        "    inference_tokenizer = tokenizer\n",
        "\n",
        "# 테스트 데이터셋 생성\n",
        "test_dataset = ReviewDataset(\n",
        "    test_df[\"review\"],\n",
        "    None,  # 테스트 데이터는 라벨 없음\n",
        "    inference_tokenizer,\n",
        "    config.max_length,\n",
        ")\n",
        "\n",
        "# 테스트 데이터로더 생성\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config.eval_batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(\"✅ 테스트 데이터셋 준비 완료\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
