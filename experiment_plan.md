## ğŸ¤– ë¨¸ì‹ _ëŸ¬ë‹_ì„±ëŠ¥_ìµœì í™”ë¥¼_ìœ„í•œ_ì‹¤í—˜_ê³„íšì„œ.md

ì œì‹œí•´ì£¼ì‹  7ê°€ì§€ í•µì‹¬ ìš”ì†Œë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìµœì¢… ëª¨ë¸ì„ ë„ì¶œí•˜ê¸° ìœ„í•œ ì²´ê³„ì ì¸ ì‹¤í—˜ ê³„íšì„œë¥¼ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.

ì‹¤í—˜ì€ **'í­í¬ìˆ˜(Waterfall)' ë°©ì‹ê³¼ 'ê·¸ë¦¬ë“œ íƒìƒ‰(Grid Search)'ì˜ ì¡°í•©**ìœ¼ë¡œ ì ‘ê·¼í•©ë‹ˆë‹¤. ëª¨ë“  ì¡°í•©ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” ê²ƒì€ ë¹„íš¨ìœ¨ì ì´ë¯€ë¡œ(Combinatorial Explosion), ê° ë‹¨ê³„(Phase)ì—ì„œ ìµœì ì˜ ìš”ì†Œë¥¼ ì„ ë³„í•˜ê³ , ì´ ì„ ë³„ëœ ìš”ì†Œë¥¼ ë‹¤ìŒ ë‹¨ê³„ì˜ ê¸°ë³¸ê°’(Baseline)ìœ¼ë¡œ í™œìš©í•˜ì—¬ ì‹¤í—˜ì„ ëˆ„ì í•´ë‚˜ê°€ëŠ” ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤.

---

### 1. ì‹¤í—˜ ëª©í‘œ

- 5ì¢…ì˜ í•œêµ­ì–´ PLM(Pre-trained Language Model) ì„±ëŠ¥ ë¹„êµ
- ë°ì´í„° ì „ì²˜ë¦¬, ì¦ê°•, ì†ì‹¤ í•¨ìˆ˜, íŠœë‹ ê¸°ë²•(DAPT, LoRA) ë“± ë‹¤ì–‘í•œ ìš”ì†Œê°€ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„
- ì•™ìƒë¸” ê¸°ë²•ì„ í¬í•¨í•œ ìµœì ì˜ ì„±ëŠ¥ì„ ë‚´ëŠ” **ìµœì¢… ëª¨ë¸ íŒŒì´í”„ë¼ì¸(Final Model Pipeline)** í™•ë¦½

---

### 2. ë² ì´ìŠ¤ë¼ì¸(Baseline) ì„¤ì •

ëª¨ë“  ë¹„êµ ì‹¤í—˜ì˜ ì¶œë°œì (Control Group)ì´ ë  ê¸°ë³¸ ì„¤ì •ì„ ì •ì˜í•©ë‹ˆë‹¤.

- **Model:** `klue/bert-base` (ê°€ì¥ í‘œì¤€ì ì¸ ëª¨ë¸ ì¤‘ í•˜ë‚˜)
- **Preprocessing:** ëª¨ë¸ì˜ ê¸°ë³¸ Tokenizer ì‚¬ìš© (íŠ¹ë³„í•œ ì •ì œ ì‘ì—… X)
- **Data Augmentation:** ì ìš© ì•ˆ í•¨ (Ratio: 0%)
- **Loss Function:** Cross-Entropy Loss
- **Training:** Full Fine-tuning (LoRA, DAPT ë¯¸ì ìš©)
- **Ensemble:** ì ìš© ì•ˆ í•¨

---

### 3. ì‹¤í—˜ ì„¤ê³„ ë° ì ˆì°¨

#### Phase 1: í•µì‹¬ ë°±ë³¸ ëª¨ë¸(Backbone Model) ì„ ì •

ê°€ì¥ í° ì„±ëŠ¥ ì°¨ì´ë¥¼ ìœ ë°œí•˜ëŠ” ê¸°ë³¸ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ë¨¼ì € ë¹„êµí•©ë‹ˆë‹¤.

- **ì‹¤í—˜ 1.1: 5ì¢… PLM ì„±ëŠ¥ ë¹„êµ**
  - **ëª©í‘œ:** ë™ì¼í•œ ì¡°ê±´ í•˜ì—ì„œ ê°€ì¥ ë†’ì€ ì ì¬ë ¥ì„ ë³´ì´ëŠ” ëª¨ë¸ ì„ ì •.
  - **ë³€ìˆ˜ (Models):**

    1. `klue/roberta-base`(í›„ë³´1)
    2. `klue/bert-base`
    3. `kykim/bert-kor-base`(Baseline)
    4. `beomi/kcbert-base`
    5. `monologg/koelectra-base-v3-discriminator`
  - **í†µì œ ë³€ì¸:** **Baseline ì„¤ì •**ì˜ ëª¨ë“  ìš”ì†Œ (ê¸°ë³¸ ì „ì²˜ë¦¬, CE Loss, Full-tuning ë“±)

    ì‹¤í—˜ ê²°ê³¼
  - ![](assets/20251023_141218_image.png)

---

#### Phase 2: ë°ì´í„° ìµœì í™” (Data Optimization)

ì„ ì •ëœ `Best_Model`ì„ ê¸°ì¤€ìœ¼ë¡œ, ëª¨ë¸ì— ì…ë ¥ë˜ëŠ” ë°ì´í„°ì˜ í’ˆì§ˆì„ ë†’ì´ëŠ” ì‹¤í—˜ì„ ì§„í–‰í•©ë‹ˆë‹¤.

- **ì‹¤í—˜ 2.1: ì „ì²˜ë¦¬(Preprocessing) ê¸°ë²• ë¹„êµ**

  - **ëª©í‘œ:** ë…¸ì´ì¦ˆ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ì œê°€ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ í™•ì¸.
  - **ê¸°ì¤€ ëª¨ë¸:** `Best_Model`
  - **ë³€ìˆ˜ (Methods):**

    1. Baseline (ê¸°ë³¸ Tokenizerë§Œ)
    2. íŠ¹ìˆ˜ë¬¸ì, ì´ëª¨í‹°ì½˜ ë“± ë…¸ì´ì¦ˆ ì œê±°
    3. ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±° ë° ì˜¤íƒˆì êµì • (ì„ íƒì )
    4. 2 + 3 (ë³‘í•©)
  - **ê²°ê³¼:** ê°€ì¥ ì„±ëŠ¥ì´ ìš°ìˆ˜í•œ ì „ì²˜ë¦¬ ë°©ì‹(ì´í•˜ **`Best_Preprocess`**)ì„ ì„ ì •í•©ë‹ˆë‹¤.

    > **1023 í˜„ì¬ ë² ì´ìŠ¤ ì½”ë“œ**
    >

    ```
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤ êµ¬ì„±
    class TextPreprocessingPipeline:
        """
        í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤
        - ê¸°ë³¸ ì „ì²˜ë¦¬ì™€ í•™ìŠµ ë°ì´í„° ê¸°ë°˜ ê³ ê¸‰ ì „ì²˜ë¦¬ë¥¼ í†µí•© ê´€ë¦¬
        - ì¬ì‚¬ìš© ê°€ëŠ¥í•˜ê³  í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡°
        """

        def __init__(self):
            self.is_fitted = False
            self.vocab_info = {}
            self.label_patterns = {}

        def basic_preprocess(self, texts):
            """ê¸°ë³¸ ì „ì²˜ë¦¬ (clean_text + normalize ê¸°ëŠ¥)"""
            processed_texts = []
            for text in texts:
                # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ë¦¬
                cleaned = self._clean_text(text)
                processed_texts.append(cleaned)
            return processed_texts

        def _clean_text(self, text):
            """ê¸°ì¡´ clean_text í•¨ìˆ˜ ë‚´ìš©"""
            if pd.isna(text):
                return ""

            text = str(text).strip()
            text = text.lower() # ì†Œë¬¸ì ë³€í™˜
            text = self._remove_urls_emails_mentions(text) # URL, ì´ë©”ì¼, ë©˜ì…˜ ì œê±°
            text = self._normalize_punctuation(text)  # êµ¬ë‘ì  ì •ê·œí™”
            #text = self._remove_incomplete_korean(text)
            text = self._normalize_emotion_expressions(text) # ê°ì • í‘œí˜„ ì •ê·œí™” (ã…‹ã…‹ã…‹ , ã…ã…ã…)
            text = self._reduce_excessive_repetition(text) # ê³¼ë„í•œ ë¬¸ì ë°˜ë³µ ì¶•ì†Œ (ì•„ì•„ì•„ì•„ì•„ì•„ì•™ -> ì•„ì•„ì•„ì•„)
            text = self._clean_special_characters(text) # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì´ëª¨í‹°ì½˜, íŠ¹ìˆ˜ê¸°í˜¸)
            text = self._normalize_whitespace(text) # ê³µë°± ì •ê·œí™” (ì—¬ëŸ¬ ê°œì˜ ê³µë°± -> í•˜ë‚˜ì˜ ê³µë°±)

            return text.strip()

        def fit(self, texts, labels=None):
            """í•™ìŠµ ë°ì´í„°ë¡œë¶€í„° ì „ì²˜ë¦¬ ì •ë³´ í•™ìŠµ (í’ˆì§ˆ ê²€ì‚¬ ê¸°ì¤€ í•™ìŠµ)"""

            self.is_fitted = True
            print("âœ“ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í•™ìŠµ ì™„ë£Œ")


        def transform(self, texts):
            """ì „ì²˜ë¦¬ ì ìš© (í’ˆì§ˆ ë¬¸ì œ ë°ì´í„° ì œê±° + í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬)"""
            if not self.is_fitted:
                print(
                    "Warning: íŒŒì´í”„ë¼ì¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì „ì²˜ë¦¬ë§Œ ì ìš©í•©ë‹ˆë‹¤."
                )
                return self.basic_preprocess(texts)

            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì ìš©
            return self.basic_preprocess(texts)

        def fit_transform(self, texts, labels=None):
            """í•™ìŠµê³¼ ë³€í™˜ì„ ë™ì‹œì— ìˆ˜í–‰"""
            # 1. í•™ìŠµ ë‹¨ê³„ (í’ˆì§ˆ ê²€ì‚¬ ê¸°ì¤€ í•™ìŠµ)
            self.fit(texts, labels)

            # 2. ë³€í™˜ ë‹¨ê³„ (í’ˆì§ˆ ë¬¸ì œ ë°ì´í„° ì œê±° + í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬)
            processed_texts = self.transform(texts)

            # 3. ë¼ë²¨ë„ ë™ì¼í•˜ê²Œ í•„í„°ë§
            return processed_texts


        @staticmethod
        def _remove_incomplete_korean(text):
            """ë¶ˆì™„ì „í•œ í•œê¸€ ì œê±° (ììŒ/ëª¨ìŒë§Œ ìˆëŠ” ê²½ìš°)"""
            return re.sub(r"[ã„±-ã…ã…-ã…£]+", "", text)

        @staticmethod
        def _normalize_emotion_expressions(text):
            """ê°ì • í‘œí˜„ ì •ê·œí™”"""
            def replace_emotion(match):
                char = match.group(1)
                count = len(match.group(0))
                # log2x + 1 ê³µì‹ì„ ì •ìˆ˜ë¡œ ë³€í™˜
                new_count = int(math.log2(count)) + 1 if count > 0 else 1
                return char * new_count

            # ì›ƒìŒê³¼ ìŠ¬í”” í‘œí˜„ ì •ê·œí™” (2ë²ˆ ì´ìƒ ë°˜ë³µ)
            text = re.sub(r"([ã…‹ã…])\1+", replace_emotion, text)
            text = re.sub(r"([ã… ã…œã…¡])\1+", replace_emotion, text)
            return text

        @staticmethod
        def _reduce_excessive_repetition(text):
            """ê³¼ë„í•œ ë¬¸ì ë°˜ë³µ ì¶•ì†Œ (4ë²ˆ ì´ìƒ â†’ 3ë²ˆìœ¼ë¡œ)"""

            def replace_repetition(match):
                char = match.group(1)
                count = len(match.group(0))
                # log2x + 1 ê³µì‹ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ê³  ìµœì†Œ 1ê°œ ë³´ì¥
                new_count = max(1, int(math.log2(count)) + 1) if count > 0 else 1
                return char * new_count

            return re.sub(r"(.)\1{3,}", replace_repetition, text)

        @staticmethod
        def _clean_special_characters(text):
            """íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì´ëª¨í‹°ì½˜ ë³´ì¡´)"""

            # 1. í—ˆìš©í•  ì´ëª¨í‹°ì½˜ ë²”ìœ„ ì •ì˜
            # emoji_ranges = r"\U0001F600-\U0001F64F"  # Emoticons
            # emoji_ranges += r"\U0001F300-\U0001F5FF"  # Misc Symbols/Pictographs
            # emoji_ranges += r"\U0001F680-\U0001F6FF"  # Transport/Map
            # emoji_ranges += r"\U00002600-\U000026FF"  # Misc Symbols (â˜… í¬í•¨)
            # emoji_ranges += r"\U00002700-\U000027BF"  # Dingbats

            # 2. í—ˆìš©í•  ê¸°íƒ€ íŠ¹ìˆ˜ê¸°í˜¸ ì •ì˜
            other_symbols = r"@â˜…#$" # ì˜ˆì‹œë¡œ @ ì¶”ê°€

            # 3. í—ˆìš©í•  ë¬¸ìë“¤ì„ ì¡°í•©í•˜ì—¬ ì •ê·œì‹ ìƒì„±
            #allowed_chars = rf"\w\sê°€-í£.,!?ã…‹ã…ã… ã…œã…¡~\-{emoji_ranges}{other_symbols}"
            allowed_chars = rf"\w\sê°€-í£.,!?ã…‹ã…ã… ã…œã…¡~\-"

            return re.sub(rf"[^{allowed_chars}]", " ", text)


        @staticmethod
        def _normalize_whitespace(text):
            """ê³µë°± ì •ê·œí™”"""
            return re.sub(r"\s+", " ", text)

        @staticmethod
        def _normalize_punctuation(text):
            """êµ¬ë‘ì  ì •ê·œí™” (log ê³µì‹ ì ìš©)"""
            def replace_punctuation(match):
                char = match.group(1)
                count = len(match.group(0))
                # log2x + 1 ê³µì‹ì„ ì •ìˆ˜ë¡œ ë³€í™˜
                new_count = int(math.log2(count)) + 1 if count > 0 else 1
                return char * new_count

            # ê° êµ¬ë‘ì ë³„ë¡œ log ê³µì‹ ì ìš©
            text = re.sub(r"([.])\1+", replace_punctuation, text)
            text = re.sub(r"([!])\1+", replace_punctuation, text)
            text = re.sub(r"([?])\1+", replace_punctuation, text)
            text = re.sub(r"([,])\1+", replace_punctuation, text)

            # êµ¬ë‘ì  ì•ë’¤ ê³µë°± ì •ë¦¬
            text = re.sub(r"\s+([.,!?])", r"\1", text)
            text = re.sub(r"([.,!?])\s+", r"\1 ", text)

            return text

        @staticmethod
        def _remove_urls_emails_mentions(text):
            """URL, ì´ë©”ì¼, ë©˜ì…˜ ì œê±°"""
            # URL íŒ¨í„´ ì œê±°
            text = re.sub(
                r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+",
                "",
                text,
            )
            # ì´ë©”ì¼ íŒ¨í„´ ì œê±°
            text = re.sub(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b", "", text)
            # ë©˜ì…˜ íŒ¨í„´ ì œê±°
            text = re.sub(r"\B@\w+", "", text)

            return text

    ```



- **ì‹¤í—˜ 2.2: ë°ì´í„° ì¦ê°•(Data Augmentation) ë°©ì‹ ë° ë¹„ìœ¨ ë¹„êµ** - agentêµ¬ì„±ì´ í•„ìš”í•˜ì—¬ ëŒ€ê¸°

  - **ëª©í‘œ:** ë°ì´í„° ë¶ˆê· í˜• í•´ì†Œ ë° ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ìµœì ì˜ ì¦ê°• ì „ëµ íƒìƒ‰.
  - **ê¸°ì¤€ ëª¨ë¸:** `Best_Model` + `Best_Preprocess`
  - **ë³€ìˆ˜ (Methods):**
    1. ì ìš© ì•ˆ í•¨ (Baseline)
    2. Back-Translation (ì—­ë²ˆì—­)
    3. EDA (Easy Data Augmentation: Synonym Replacement, Random Deletion ë“±)
  - **ë³€ìˆ˜ (Ratios):** ê° Method ë³„ë¡œ ì›ë³¸ ëŒ€ë¹„ ì¦ê°• ë°ì´í„° ë¹„ìœ¨ (ì˜ˆ: 25%, 50%, 100%)ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
  - **ê²°ê³¼:** ìµœì ì˜ ì¦ê°• ë°©ì‹ ë° ë¹„ìœ¨(ì´í•˜ **`Best_Aug`**)ì„ ì„ ì •í•©ë‹ˆë‹¤.

---

#### Phase 3: í•™ìŠµ íŒŒì´í”„ë¼ì¸ ê³ ë„í™” (Advanced Training)

ë°ì´í„°ê°€ ì¤€ë¹„ëœ ìƒíƒœì—ì„œ, ëª¨ë¸ì˜ í•™ìŠµ ë°©ì‹ ìì²´ë¥¼ ê³ ë„í™”í•©ë‹ˆë‹¤.

- **ì‹¤í—˜ 3.1: DAPT (Domain-Adaptive Pre-Training) ì ìš©**

  - **ëª©í‘œ:** ë³´ìœ í•œ ë°ì´í„°(ë„ë©”ì¸)ì— ëª¨ë¸ì„ ì‚¬ì „ ì ì‘ì‹œì¼œ ì„±ëŠ¥ í–¥ìƒ í™•ì¸.
  - **ê¸°ì¤€:** `Best_Model` + `Best_Preprocess` + `Best_Aug`
  - **ë³€ìˆ˜:**
    1. DAPT ë¯¸ì ìš© (Baseline)
    2. ë³´ìœ í•œ Train (ë˜ëŠ” Train+Test) ë°ì´í„°ì˜ Unlabeled Corpusë¥¼ í™œìš©í•˜ì—¬ DAPT ì„ í–‰ í•™ìŠµ ì§„í–‰
  - **ê²°ê³¼:** DAPT ì ìš© ì—¬ë¶€(ì´í•˜ **`Best_DAPT`**)ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
- **ì‹¤í—˜ 3.2: ì†ì‹¤ í•¨ìˆ˜(Loss Function) ë¹„êµ**

  - **ëª©í‘œ:** í´ë˜ìŠ¤ ë¶ˆê· í˜• ë“± ë°ì´í„° íŠ¹ì„±ì— ë§ëŠ” ì†ì‹¤ í•¨ìˆ˜ íƒìƒ‰.
  - **ê¸°ì¤€:** `Best_Model` + `Best_Preprocess` + `Best_Aug` + `Best_DAPT`
  - **ë³€ìˆ˜:**
    1. Cross-Entropy Loss (Baseline)
    2. **Focal Loss** (ë¶ˆê· í˜• ë°ì´í„°ì— ìœ ë¦¬)
    3. Label Smoothing (ëª¨ë¸ì˜ ê³¼ì‹ (Over-confidence) ë°©ì§€)
  - **ê²°ê³¼:** ìµœì ì˜ ì†ì‹¤ í•¨ìˆ˜(ì´í•˜ **`Best_Loss`**)ë¥¼ ì„ ì •í•©ë‹ˆë‹¤.
- **ì‹¤í—˜ 3.3: íŒŒì¸íŠœë‹(Fine-tuning) ê¸°ë²• ë¹„êµ**

  - **ëª©í‘œ:** Full-tuning ëŒ€ë¹„ PEFT(LoRA)ì˜ ì„±ëŠ¥ ë° íš¨ìœ¨ì„± ë¹„êµ.
  - **ê¸°ì¤€:** `Best_Model` + `Best_Preprocess` + `Best_Aug` + `Best_DAPT` + `Best_Loss`
  - **ë³€ìˆ˜:**
    1. **Full Fine-tuning** (Baseline)
    2. **LoRA (Low-Rank Adaptation)** (Rank, Alpha ë“± í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”)
  - **ê²°ê³¼:** ì„±ëŠ¥ê³¼ í•™ìŠµ/ì¶”ë¡  íš¨ìœ¨ì„ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ íŠœë‹ ë°©ì‹(ì´í•˜ **`Best_Tuning`**)ì„ ê²°ì •í•©ë‹ˆë‹¤.

---

#### Phase 4: ì•™ìƒë¸”(Ensemble) ë° ìµœì¢… ëª¨ë¸ ê²°ì •

ê°œë³„ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•œ í›„, ì—¬ëŸ¬ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ì¶”ê°€ ì„±ëŠ¥ í–¥ìƒì„ ë„ëª¨í•©ë‹ˆë‹¤.

- **ì‹¤í—˜ 4.1: ì•™ìƒë¸” ì „ëµ ë¹„êµ**

  - **ëª©í‘œ:** ë‹¨ì¼ ëª¨ë¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³  ì˜ˆì¸¡ ì•ˆì •ì„± í™•ë³´.
  - **ê¸°ì¤€:** Phase 3ê¹Œì§€ì˜ ìµœì  íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ í•™ìŠµëœ **'Best Single Model'**
  - **ë³€ìˆ˜:**
    1. **(A) Heterogeneous Ensemble (ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸):**
       - Phase 1ì—ì„œ ì„ ì •ëœ Top 2~3 ëª¨ë¸ (ì˜ˆ: `klue/roberta`, `koelectra`)ì„ ê°ê° Phase 3ê¹Œì§€ì˜ ìµœì  íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ í•™ìŠµì‹œí‚¨ ë’¤, Soft/Hard Voting ì•™ìƒë¸”.
    2. **(B) Homogeneous Ensemble (ë™ì¼ ëª¨ë¸):**
       - 'Best Single Model'ì„ K-Fold êµì°¨ ê²€ì¦ ì‹œ ìƒì„±ëœ Kê°œì˜ ëª¨ë¸ ë˜ëŠ” ì„œë¡œ ë‹¤ë¥¸ Random Seedë¡œ Kë²ˆ í•™ìŠµí•œ ëª¨ë¸ë“¤ì„ Soft/Hard Voting ì•™ìƒë¸”.
  - **ê²°ê³¼:** (A), (B) ì „ëµê³¼ 'Best Single Model'ì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.
- **ì‹¤í—˜ 4.2: ìµœì¢… ëª¨ë¸(Final Model) ì œì‹œ**

  - ì‹¤í—˜ 4.1ì˜ ê²°ê³¼(A ë˜ëŠ” B)ê°€ 'Best Single Model' ëŒ€ë¹„ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒ(ë° ì¶”ë¡  ì‹œê°„ í—ˆìš© ë²”ìœ„ ë‚´)ì„ ë³´ì¸ë‹¤ë©´ ì•™ìƒë¸” ëª¨ë¸ì„ ì±„íƒí•©ë‹ˆë‹¤.
  - ê·¸ë ‡ì§€ ì•Šë‹¤ë©´, Phase 3ê¹Œì§€ì˜ **'Best Single Model'**ì„ ìµœì¢… ëª¨ë¸ë¡œ í™•ì •í•©ë‹ˆë‹¤.

---

### 4. í‰ê°€ ì§€í‘œ (Evaluation Metrics)

ì‹¤í—˜ì˜ ì„±íŒ¨ë¥¼ íŒë‹¨í•˜ê¸° ìœ„í•œ ì§€í‘œì…ë‹ˆë‹¤. (íƒœìŠ¤í¬ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ)

- **ì£¼ìš” ì§€í‘œ (Primary):** **Macro F1-Score** (í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ ê³ ë ¤í•œ ê°€ì¥ ì¼ë°˜ì ì¸ ì§€í‘œ)
- **ë³´ì¡° ì§€í‘œ (Secondary):**
  - Accuracy
  - Precision, Recall (per-class)
  - Confusion Matrix (ì˜¤ë¶„ë¥˜ íŒ¨í„´ ë¶„ì„)
- **ë¹„ìš© ì§€í‘œ (Cost):**
  - í•™ìŠµ ì‹œê°„ (Training Time)
  - ì¶”ë¡  ì†ë„ (Inference Speed, (ì˜ˆ: samples/sec))
  - (LoRA ì ìš© ì‹œ) í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜
