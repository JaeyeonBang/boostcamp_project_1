{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국어 텍스트 감정 분석 - WandB Sweep 하이퍼파라미터 튜닝\n",
    "\n",
    "이 노트북은 WandB Sweep을 사용하여 최적의 하이퍼파라미터를 찾습니다.\n",
    "\n",
    "## 주요 기능\n",
    "- WandB Sweep을 통한 자동 하이퍼파라미터 튜닝\n",
    "- Bayesian Optimization 방법 사용\n",
    "- 튜닝 대상: learning_rate, batch_size, label_smoothing_factor, weight_decay, num_epochs, augment_ratio, newly_gen_ratio\n",
    "- 데이터셋 비율 조정: original(1.0) : augment(0.5-2.0) : newly_generated(0.5-2.0)\n",
    "- 최적화 목표: 검증 정확도 최대화\n",
    "- 10-20회 실행으로 효율적인 탐색\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/py310/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/py310/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 라이브러리 임포트 완료\n"
     ]
    }
   ],
   "source": [
    "# Library Import\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "# PEFT for LoRA\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# 경고 메시지 필터링\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"✅ 라이브러리 임포트 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디바이스: cuda\n",
      "GPU 개수: 1\n",
      "   GPU 0: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "# 환경 설정\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 이 함수가 .env 파일을 읽어서 환경 변수로 로드합니다.\n",
    "load_dotenv()\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "set_seed(RANDOM_STATE)\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"디바이스: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 개수: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"⚠️  CUDA 사용 불가 - CPU로 훈련 진행\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "TIMESTAMP = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# 고정 설정\n",
    "PROJECT_NAME = \"[domain_project]_Sweep_Experiments\"\n",
    "MODEL_NAME = \"kykim_bert-base\"\n",
    "RUN_NAME = f\"{MODEL_NAME}_sweep_hyperparameter_tuning\"\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ WandB Sweep 설정 완료\n",
      "튜닝 대상: ['learning_rate', 'batch_size', 'label_smoothing_factor', 'weight_decay', 'num_epochs', 'augment_ratio', 'newly_gen_ratio']\n",
      "최적화 목표: val_accuracy (maximize)\n",
      "데이터셋 비율 범위:\n",
      "  - augment_ratio: [1.0, 2.0, 3.0]\n",
      "  - newly_gen_ratio: [0.0]\n",
      "  - original은 항상 1.0 (기준)\n"
     ]
    }
   ],
   "source": [
    "# WandB Sweep 설정\n",
    "sweep_config = {\n",
    "    'method': 'bayes',  # Bayesian optimization\n",
    "    'metric': {\n",
    "        'name': 'val_accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 1e-5,\n",
    "            'max': 5e-4\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [512]\n",
    "        },\n",
    "        'label_smoothing_factor': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.05,\n",
    "            'max': 0.15\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 0.005,\n",
    "            'max': 0.1\n",
    "        },\n",
    "        'num_epochs': {\n",
    "            'values': [5]\n",
    "        },\n",
    "        'augment_ratio': {\n",
    "            'values': [1.0, 2.0, 3.0]\n",
    "        },\n",
    "        'newly_gen_ratio': {\n",
    "            'values': [0.0]\n",
    "        }\n",
    "    },\n",
    "    'early_terminate': {\n",
    "        'type': 'hyperband',\n",
    "        'min_iter': 2,\n",
    "        'eta': 2\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ WandB Sweep 설정 완료\")\n",
    "print(f\"튜닝 대상: {list(sweep_config['parameters'].keys())}\")\n",
    "print(f\"최적화 목표: {sweep_config['metric']['name']} ({sweep_config['metric']['goal']})\")\n",
    "print(f\"데이터셋 비율 범위:\")\n",
    "print(f\"  - augment_ratio: {sweep_config['parameters']['augment_ratio']['values']}\")\n",
    "print(f\"  - newly_gen_ratio: {sweep_config['parameters']['newly_gen_ratio']['values']}\")\n",
    "print(f\"  - original은 항상 1.0 (기준)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 고정 설정 완료\n"
     ]
    }
   ],
   "source": [
    "# 고정 하이퍼파라미터 설정\n",
    "class FixedConfig:\n",
    "    # 모델 설정\n",
    "    model_name = MODEL_NAME\n",
    "    base_models_path = \"/data/ephemeral/home/code/basemodels/\"\n",
    "    local_model_path = base_models_path + MODEL_NAME\n",
    "\n",
    "    is_DAPT = True\n",
    "    DAPT_model_path = \"./best_unsupervised_model/TAPT_kykim_bert-kor-base_augX3_best_unsupervised_model_1028\"\n",
    "    \n",
    "    save_model_path = \"./DAPT_fine/\"+ RUN_NAME\n",
    "\n",
    "    num_classes = 4\n",
    "    max_length = 128\n",
    "\n",
    "    # 데이터 설정\n",
    "    train_data_path = \"/data/ephemeral/home/code/data/train_final_augX3_newly_gen_added.csv\"\n",
    "    val_data_path = \"/data/ephemeral/home/code/data/val_final.csv\"\n",
    "    \n",
    "    # 고정 훈련 설정\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    warmup_steps = 500\n",
    "    \n",
    "    # 기타 설정\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1.0\n",
    "    early_stopping_patience = 5\n",
    "    save_best_model = False  # Sweep에서는 개별 모델 저장 안함\n",
    "    \n",
    "    # WandB 설정\n",
    "    use_wandb = True\n",
    "    project_name = PROJECT_NAME\n",
    "\n",
    "    # Loss 설정\n",
    "    loss_function = \"CrossEntropy\"\n",
    "    use_label_smoothing = True  # Sweep에서 튜닝됨\n",
    "\n",
    "    use_lora = False\n",
    "\n",
    "fixed_config = FixedConfig()\n",
    "print(\"✅ 고정 설정 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 데이터셋 샘플링 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 비율에 따른 샘플링 함수\n",
    "def sample_dataset_by_ratio(df, augment_ratio=1.0, newly_gen_ratio=1.0, random_state=42):\n",
    "    \"\"\"\n",
    "    데이터셋을 비율에 따라 샘플링하는 함수\n",
    "    \n",
    "    Args:\n",
    "        df: 전체 데이터프레임\n",
    "        augment_ratio: augment 데이터 비율 (original 대비)\n",
    "        newly_gen_ratio: newly_generated 데이터 비율 (original 대비)\n",
    "        random_state: 랜덤 시드\n",
    "    \n",
    "    Returns:\n",
    "        sampled_df: 샘플링된 데이터프레임\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # 각 타입별 데이터 분리\n",
    "    original_data = df[df['type'] == 'original'].copy()\n",
    "    augment_data = df[df['type'] == 'augment'].copy()\n",
    "    newly_gen_data = df[df['type'] == 'newly_generated'].copy()\n",
    "    \n",
    "    print(f\"원본 데이터 분포:\")\n",
    "    print(f\"  Original: {len(original_data):,}개\")\n",
    "    print(f\"  Augment: {len(augment_data):,}개\")\n",
    "    print(f\"  Newly_generated: {len(newly_gen_data):,}개\")\n",
    "    \n",
    "    # Original 데이터는 항상 100% 사용\n",
    "    sampled_original = original_data.copy()\n",
    "    \n",
    "    # Augment 데이터 샘플링\n",
    "    if augment_ratio > 0 and len(augment_data) > 0:\n",
    "        # original 대비 비율로 샘플링\n",
    "        target_augment_size = int(len(original_data) * augment_ratio)\n",
    "        if target_augment_size > len(augment_data):\n",
    "            # 요청된 크기가 전체보다 크면 전체 사용\n",
    "            sampled_augment = augment_data.copy()\n",
    "        else:\n",
    "            # 비율에 맞게 샘플링\n",
    "            sampled_augment = augment_data.sample(n=target_augment_size, random_state=random_state)\n",
    "    else:\n",
    "        sampled_augment = pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    # Newly_generated 데이터 샘플링\n",
    "    if newly_gen_ratio > 0 and len(newly_gen_data) > 0:\n",
    "        # original 대비 비율로 샘플링\n",
    "        target_newly_gen_size = int(len(original_data) * newly_gen_ratio)\n",
    "        if target_newly_gen_size > len(newly_gen_data):\n",
    "            # 요청된 크기가 전체보다 크면 전체 사용\n",
    "            sampled_newly_gen = newly_gen_data.copy()\n",
    "        else:\n",
    "            # 비율에 맞게 샘플링\n",
    "            sampled_newly_gen = newly_gen_data.sample(n=target_newly_gen_size, random_state=random_state)\n",
    "    else:\n",
    "        sampled_newly_gen = pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    # 샘플링된 데이터 합치기\n",
    "    sampled_df = pd.concat([sampled_original, sampled_augment, sampled_newly_gen], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\\\n샘플링된 데이터 분포:\")\n",
    "    print(f\"  Original: {len(sampled_original):,}개 (비율: 1.0)\")\n",
    "    print(f\"  Augment: {len(sampled_augment):,}개 (비율: {augment_ratio:.1f})\")\n",
    "    print(f\"  Newly_generated: {len(sampled_newly_gen):,}개 (비율: {newly_gen_ratio:.1f})\")\n",
    "    print(f\"  총 데이터: {len(sampled_df):,}개\")\n",
    "    \n",
    "    return sampled_df\n",
    "\n",
    "print(\"✅ 데이터셋 샘플링 함수 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 로드 중...\n",
      "전체 데이터: 627,637개\n",
      "검증 데이터: 6,823개\n",
      "✅ 데이터 로드 완료\n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터 로드 (비율 샘플링을 위해)\n",
    "print(\"전체 데이터 로드 중...\")\n",
    "\n",
    "# 원본 데이터 로드 (preprocessing_final.ipynb에서 생성된 데이터)\n",
    "full_df = pd.read_csv(fixed_config.train_data_path)\n",
    "print(f\"전체 데이터: {len(full_df):,}개\")\n",
    "\n",
    "# 검증 데이터 로드 (original만 사용)\n",
    "val_df = pd.read_csv(fixed_config.val_data_path)\n",
    "print(f\"검증 데이터: {len(val_df):,}개\")\n",
    "\n",
    "print(\"✅ 데이터 로드 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 데이터셋 클래스 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 클래스 정의\n",
    "class ReviewDataset(Dataset):\n",
    "    \"\"\"\n",
    "    리뷰 텍스트 데이셋 클래스\n",
    "    - BERT 모델 훈련/추론을 위한 PyTorch Dataset 구현\n",
    "    - 텍스트 토크나이징 및 텐서 변환 처리\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        \"\"\"\n",
    "        데이터셋 초기화\n",
    "        \"\"\"\n",
    "        self.texts, self.labels, self.tokenizer, self.max_length = (\n",
    "            texts,\n",
    "            labels,\n",
    "            tokenizer,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"데이터셋 크기 반환\"\"\"\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        특정 인덱스의 데이터 아이템 반환\n",
    "        \"\"\"\n",
    "        # 텍스트 토크나이징 및 패딩\n",
    "        encoding = self.tokenizer(\n",
    "            str(self.texts.iloc[idx]),\n",
    "            truncation=True,  # 최대 길이 초과시 자르기\n",
    "            padding=\"max_length\",  # 최대 길이까지 패딩\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",  # PyTorch 텐서로 반환\n",
    "        )\n",
    "\n",
    "        # 기본 아이템 구성 (input_ids, attention_mask)\n",
    "        item = {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "        }\n",
    "\n",
    "        # labels가 None이 아닌 경우에만 추가 (train/valid용)\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "\n",
    "        return item\n",
    "\n",
    "print(\"✅ 데이터셋 클래스 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 평가 메트릭 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# 평가 메트릭 함수\n",
    "def compute_metrics(predictions, labels):\n",
    "    \"\"\"\n",
    "    모델 평가 메트릭 계산 함수\n",
    "    \"\"\"\n",
    "    # 예측값에서 가장 높은 확률의 클래스 선택\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "print(\"✅ 평가 메트릭 함수 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load_from_local(local_model_path : str = None, model_name : str = None):\n",
    "    # 1. 토크나이저 로드\n",
    "    # 절대 경로를 사용하여 로컬 모델 로드\n",
    "    print(\"로컬 경로에서 토크나이저 로딩 중...\")\n",
    "\n",
    "    # 경로가 존재하는지 확인\n",
    "    if not os.path.exists(local_model_path):\n",
    "        print(f\"❌ 경로가 존재하지 않습니다: {local_model_path}\")\n",
    "        return None, None\n",
    "    else:\n",
    "        print(f\"✅ 경로 확인됨: {local_model_path}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "\n",
    "    # 2. 모델 로드\n",
    "    # 마찬가지로 로컬 경로(local_model_path)를 사용\n",
    "    print(\"로컬 경로에서 모델 로딩 중...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        local_model_path,\n",
    "        num_labels=4,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    print(\"✅ 로컬 스냅샷에서 모델과 토크나이저 로딩 성공!\")\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Label Smoothing 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def label_smoothing_cross_entropy(predictions, targets, smoothing=0.1):\n",
    "    \"\"\"\n",
    "    predictions: 모델 출력 (logits) [batch_size, num_classes]\n",
    "    targets: 정답 라벨 (정수) [batch_size]\n",
    "    smoothing: smoothing factor\n",
    "    \"\"\"\n",
    "    num_classes = predictions.size(-1)\n",
    "    \n",
    "    # 하드 라벨을 원-핫으로 변환\n",
    "    true_dist = torch.zeros_like(predictions)\n",
    "    true_dist.fill_(smoothing / (num_classes - 1))\n",
    "    true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - smoothing)\n",
    "    \n",
    "    # KL divergence = Cross Entropy with smoothed labels\n",
    "    return F.kl_div(F.log_softmax(predictions, dim=1), true_dist, reduction='batchmean')\n",
    "\n",
    "print(\"✅ Label Smoothing 함수 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, scaler, device, config, criterion_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # 배치를 디바이스로 이동\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        batch_labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # 혼합 정밀도 훈련\n",
    "        if scaler is not None:\n",
    "            with autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "\n",
    "                logits = outputs.logits\n",
    "                loss = criterion_fn(logits, batch_labels) / config.gradient_accumulation_steps\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        else:\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            loss = criterion_fn(logits, batch_labels) / config.gradient_accumulation_steps\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * config.gradient_accumulation_steps\n",
    "        \n",
    "        # 예측값 저장\n",
    "        predictions.extend(logits.detach().cpu().numpy())\n",
    "        labels.extend(batch_labels.detach().cpu().numpy())\n",
    "        \n",
    "        # 진행률 업데이트\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item() * config.gradient_accumulation_steps:.4f}',\n",
    "            'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "        })\n",
    "    \n",
    "    # 메트릭 계산\n",
    "    metrics = compute_metrics(predictions, labels)\n",
    "    metrics['loss'] = total_loss / len(dataloader)\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 검증 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# 검증 함수\n",
    "def validate_epoch(model, dataloader, device, criterion_fn):\n",
    "    \"\"\"한 에포크 검증\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Validation\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            # 배치를 디바이스로 이동\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            batch_labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # 순전파\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            loss = criterion_fn(logits, batch_labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # 예측값 저장\n",
    "            predictions.extend(logits.detach().cpu().numpy())\n",
    "            labels.extend(batch_labels.detach().cpu().numpy())\n",
    "            \n",
    "            # 진행률 업데이트\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    # 메트릭 계산\n",
    "    metrics = compute_metrics(predictions, labels)\n",
    "    metrics['loss'] = total_loss / len(dataloader)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"✅ 검증 함수 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 훈련 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# 메인 훈련 함수 (Sweep용)\n",
    "def train():\n",
    "    # WandB 초기화\n",
    "    run = wandb.init()\n",
    "    \n",
    "    # Sweep에서 받은 하이퍼파라미터\n",
    "    config = wandb.config\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Sweep Run 시작\")\n",
    "    print(f\"learning_rate: {config.learning_rate:.2e}\")\n",
    "    print(f\"batch_size: {config.batch_size}\")\n",
    "    print(f\"label_smoothing_factor: {config.label_smoothing_factor:.3f}\")\n",
    "    print(f\"weight_decay: {config.weight_decay:.3f}\")\n",
    "    print(f\"num_epochs: {config.num_epochs}\")\n",
    "    print(f\"augment_ratio: {config.augment_ratio:.1f}\")\n",
    "    print(f\"newly_gen_ratio: {config.newly_gen_ratio:.1f}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        # 모델 및 토크나이저 로드\n",
    "        if fixed_config.is_DAPT:\n",
    "            model, tokenizer = model_load_from_local(fixed_config.DAPT_model_path, fixed_config.model_name)\n",
    "        else:\n",
    "            model, tokenizer = model_load_from_local(fixed_config.local_model_path, fixed_config.model_name)\n",
    "        \n",
    "        if model is None or tokenizer is None:\n",
    "            print(\"❌ 모델 로드 실패\")\n",
    "            return\n",
    "        \n",
    "        model = model.to(fixed_config.device)\n",
    "        \n",
    "        # 비율에 따라 훈련 데이터 샘플링\n",
    "        print(\"\\\\n데이터셋 비율에 따른 샘플링 중...\")\n",
    "        train_df_sampled = sample_dataset_by_ratio(\n",
    "            full_df, \n",
    "            augment_ratio=config.augment_ratio, \n",
    "            newly_gen_ratio=config.newly_gen_ratio,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        \n",
    "        # 데이터셋 및 데이터로더 생성\n",
    "        train_dataset = ReviewDataset(\n",
    "            train_df_sampled[\"review\"],\n",
    "            train_df_sampled[\"label\"],\n",
    "            tokenizer,\n",
    "            fixed_config.max_length,\n",
    "        )\n",
    "\n",
    "        val_dataset = ReviewDataset(\n",
    "            val_df[\"review\"],\n",
    "            val_df[\"label\"],\n",
    "            tokenizer,\n",
    "            fixed_config.max_length,\n",
    "        )\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "\n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        \n",
    "        # 손실 함수 설정\n",
    "        def criterion_fn(predictions, targets):\n",
    "            return label_smoothing_cross_entropy(predictions, targets, config.label_smoothing_factor)\n",
    "        \n",
    "        # 옵티마이저 설정\n",
    "        optimizer = AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "\n",
    "        # 전체 훈련 스텝 계산\n",
    "        total_steps = len(train_dataloader) * config.num_epochs // fixed_config.gradient_accumulation_steps\n",
    "\n",
    "        # 스케줄러 설정 (warmup + linear decay)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=fixed_config.warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        # 혼합 정밀도 스케일러\n",
    "        scaler = GradScaler() if torch.cuda.is_available() else None\n",
    "        \n",
    "        # 훈련 루프\n",
    "        best_val_accuracy = 0\n",
    "        \n",
    "        for epoch in range(config.num_epochs):\n",
    "            # 훈련\n",
    "            train_metrics = train_epoch(\n",
    "                model, train_dataloader, optimizer, scheduler, scaler, \n",
    "                fixed_config.device, fixed_config, criterion_fn\n",
    "            )\n",
    "            \n",
    "            # 검증\n",
    "            val_metrics = validate_epoch(model, val_dataloader, fixed_config.device, criterion_fn)\n",
    "            \n",
    "            # WandB 로깅\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_metrics['loss'],\n",
    "                \"train_accuracy\": train_metrics['accuracy'],\n",
    "                \"train_f1\": train_metrics['f1'],\n",
    "                \"val_loss\": val_metrics['loss'],\n",
    "                \"val_accuracy\": val_metrics['accuracy'],\n",
    "                \"val_f1\": val_metrics['f1'],\n",
    "                \"learning_rate\": scheduler.get_last_lr()[0]\n",
    "            })\n",
    "            \n",
    "            # 최고 성능 추적\n",
    "            if val_metrics['accuracy'] > best_val_accuracy:\n",
    "                best_val_accuracy = val_metrics['accuracy']\n",
    "        \n",
    "        print(f\"✅ 훈련 완료! 최고 검증 정확도: {best_val_accuracy:.4f}\")\n",
    "        \n",
    "        # 최종 메트릭 로깅\n",
    "        wandb.log({\"best_val_accuracy\": best_val_accuracy})\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 훈련 중 오류 발생: {str(e)}\")\n",
    "        wandb.log({\"error\": str(e)})\n",
    "    finally:\n",
    "        wandb.finish()\n",
    "\n",
    "print(\"✅ 훈련 함수 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB Sweep 시작...\n",
      "프로젝트: [domain_project]_Sweep_Experiments\n",
      "실행 횟수: 15회\n",
      "최적화 목표: 검증 정확도 최대화\n",
      "Create sweep with ID: awpthshs\n",
      "Sweep URL: https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments/sweeps/awpthshs\n",
      "\n",
      "✅ Sweep 생성 완료!\n",
      "Sweep ID: awpthshs\n",
      "Sweep URL: https://wandb.ai/qkdwodus777-/[domain_project]_Sweep_Experiments/sweeps/awpthshs\n",
      "\n",
      "Sweep 실행을 시작합니다...\n"
     ]
    }
   ],
   "source": [
    "# Sweep 실행\n",
    "print(\"WandB Sweep 시작...\")\n",
    "print(f\"프로젝트: {PROJECT_NAME}\")\n",
    "print(f\"실행 횟수: 10회\")\n",
    "print(f\"최적화 목표: 검증 정확도 최대화\")\n",
    "\n",
    "# Sweep ID 생성\n",
    "sweep_id = wandb.sweep(\n",
    "    sweep_config,\n",
    "    project=PROJECT_NAME\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Sweep 생성 완료!\")\n",
    "print(f\"Sweep ID: {sweep_id}\")\n",
    "print(f\"Sweep URL: https://wandb.ai/{wandb.api.default_entity}/{PROJECT_NAME}/sweeps/{sweep_id}\")\n",
    "print(\"\\nSweep 실행을 시작합니다...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: l1ynaqtz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment_ratio: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlabel_smoothing_factor: 0.11885811974590456\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5.255921600625229e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnewly_gen_ratio: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.07235098370604887\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/code/wandb/run-20251029_080650-l1ynaqtz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments/runs/l1ynaqtz' target=\"_blank\">deep-sweep-1</a></strong> to <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments/sweeps/awpthshs' target=\"_blank\">https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments/sweeps/awpthshs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments' target=\"_blank\">https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments/sweeps/awpthshs' target=\"_blank\">https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments/sweeps/awpthshs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments/runs/l1ynaqtz' target=\"_blank\">https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments/runs/l1ynaqtz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Sweep Run 시작\n",
      "learning_rate: 5.26e-05\n",
      "batch_size: 512\n",
      "label_smoothing_factor: 0.119\n",
      "weight_decay: 0.072\n",
      "num_epochs: 5\n",
      "augment_ratio: 3.0\n",
      "newly_gen_ratio: 0.0\n",
      "==================================================\n",
      "로컬 경로에서 토크나이저 로딩 중...\n",
      "✅ 경로 확인됨: ./best_unsupervised_model/TAPT_kykim_bert-kor-base_augX3_best_unsupervised_model_1028\n",
      "로컬 경로에서 모델 로딩 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./best_unsupervised_model/TAPT_kykim_bert-kor-base_augX3_best_unsupervised_model_1028 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 로컬 스냅샷에서 모델과 토크나이저 로딩 성공!\n",
      "\\n데이터셋 비율에 따른 샘플링 중...\n",
      "원본 데이터 분포:\n",
      "  Original: 129,630개\n",
      "  Augment: 407,336개\n",
      "  Newly_generated: 90,671개\n",
      "\\n샘플링된 데이터 분포:\n",
      "  Original: 129,630개 (비율: 1.0)\n",
      "  Augment: 388,890개 (비율: 3.0)\n",
      "  Newly_generated: 0개 (비율: 0.0)\n",
      "  총 데이터: 518,520개\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba466be2b0ec4e44a706a0ce7829ea55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014b552383de4009861fcd8beb07a8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffded969269a4e2a9a20992fd0374544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deddc816b84142b890bcfd1674211858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e5cc56575742f4918af932569507d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072b32689a964cffa5a3c68d12f11923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859c7fe64c5a4cafa9e557f6feb8ff7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ee63c2c2024a77a5e094e958091128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219b745b38ab462e875b20ab69a40fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219a7f0841c14bd0830e3b4746d8e7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 훈련 완료! 최고 검증 정확도: 0.8339\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_accuracy</td><td>▁</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>learning_rate</td><td>█▆▅▃▁</td></tr><tr><td>train_accuracy</td><td>▁▄▆▇█</td></tr><tr><td>train_f1</td><td>▁▄▆▇█</td></tr><tr><td>train_loss</td><td>█▅▄▂▁</td></tr><tr><td>val_accuracy</td><td>▁█▇▆▇</td></tr><tr><td>val_f1</td><td>▁▆▇██</td></tr><tr><td>val_loss</td><td>▂▁▃▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_accuracy</td><td>0.83394</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>learning_rate</td><td>0</td></tr><tr><td>train_accuracy</td><td>0.93393</td></tr><tr><td>train_f1</td><td>0.93339</td></tr><tr><td>train_loss</td><td>0.11313</td></tr><tr><td>val_accuracy</td><td>0.83233</td></tr><tr><td>val_f1</td><td>0.83173</td></tr><tr><td>val_loss</td><td>0.32228</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deep-sweep-1</strong> at: <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments/runs/l1ynaqtz' target=\"_blank\">https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments/runs/l1ynaqtz</a><br> View project at: <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments' target=\"_blank\">https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251029_080650-l1ynaqtz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q6gld2xb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugment_ratio: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlabel_smoothing_factor: 0.09911929760884385\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 9.955694167608006e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnewly_gen_ratio: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.015300418107446848\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/code/wandb/run-20251029_092359-q6gld2xb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments/runs/q6gld2xb' target=\"_blank\">trim-sweep-2</a></strong> to <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments/sweeps/awpthshs' target=\"_blank\">https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments/sweeps/awpthshs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments' target=\"_blank\">https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments/sweeps/awpthshs' target=\"_blank\">https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments/sweeps/awpthshs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments/runs/q6gld2xb' target=\"_blank\">https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Sweep_Experiments/runs/q6gld2xb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./best_unsupervised_model/TAPT_kykim_bert-kor-base_augX3_best_unsupervised_model_1028 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Sweep Run 시작\n",
      "learning_rate: 9.96e-05\n",
      "batch_size: 512\n",
      "label_smoothing_factor: 0.099\n",
      "weight_decay: 0.015\n",
      "num_epochs: 5\n",
      "augment_ratio: 2.0\n",
      "newly_gen_ratio: 0.0\n",
      "==================================================\n",
      "로컬 경로에서 토크나이저 로딩 중...\n",
      "✅ 경로 확인됨: ./best_unsupervised_model/TAPT_kykim_bert-kor-base_augX3_best_unsupervised_model_1028\n",
      "로컬 경로에서 모델 로딩 중...\n",
      "✅ 로컬 스냅샷에서 모델과 토크나이저 로딩 성공!\n",
      "\\n데이터셋 비율에 따른 샘플링 중...\n",
      "원본 데이터 분포:\n",
      "  Original: 129,630개\n",
      "  Augment: 407,336개\n",
      "  Newly_generated: 90,671개\n",
      "\\n샘플링된 데이터 분포:\n",
      "  Original: 129,630개 (비율: 1.0)\n",
      "  Augment: 259,260개 (비율: 2.0)\n",
      "  Newly_generated: 0개 (비율: 0.0)\n",
      "  총 데이터: 388,890개\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0fc0701e9f49c9a3cd23cf5703d28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99a9f8734144ccfa34d6af5ee2a51e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74aa056761404d519d0b69d64bccfdaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a210b9845f40a6961c58e49b870723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d13da6fd3447c3a0880f0545de1240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8800d1f49354be4a3be06ad83d8a214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445bb772e8dd417db2780a8b3d8b36ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sweep Agent 실행\n",
    "wandb.agent(\n",
    "    sweep_id,\n",
    "    function=train,\n",
    "    count=10,  # 10-20회 중 15회 실행\n",
    "    project=PROJECT_NAME\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 Sweep 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweep 결과 분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최고 성능 파라미터 조회\n",
    "api = wandb.Api()\n",
    "sweep = api.sweep(f\"{wandb.api.default_entity}/{PROJECT_NAME}/sweeps/{sweep_id}\")\n",
    "\n",
    "# 최고 성능 run 찾기\n",
    "best_run = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for run in sweep.runs:\n",
    "    if run.summary.get('best_val_accuracy', 0) > best_accuracy:\n",
    "        best_accuracy = run.summary.get('best_val_accuracy', 0)\n",
    "        best_run = run\n",
    "\n",
    "if best_run:\n",
    "    print(\"🏆 최고 성능 파라미터:\")\n",
    "    print(f\"  검증 정확도: {best_accuracy:.4f}\")\n",
    "    print(f\"  learning_rate: {best_run.config['learning_rate']:.2e}\")\n",
    "    print(f\"  batch_size: {best_run.config['batch_size']}\")\n",
    "    print(f\"  label_smoothing_factor: {best_run.config['label_smoothing_factor']:.3f}\")\n",
    "    print(f\"  weight_decay: {best_run.config['weight_decay']:.3f}\")\n",
    "    print(f\"  num_epochs: {best_run.config['num_epochs']}\")\n",
    "    print(f\"\\n  Run URL: {best_run.url}\")\n",
    "else:\n",
    "    print(\"❌ 최고 성능 run을 찾을 수 없습니다.\")\n",
    "\n",
    "print(f\"\\n📊 전체 Sweep 결과: {len(sweep.runs)}개 run 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 중요도 분석\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 모든 run의 결과 수집\n",
    "results = []\n",
    "for run in sweep.runs:\n",
    "    if run.summary.get('best_val_accuracy') is not None:\n",
    "        results.append({\n",
    "            'learning_rate': run.config['learning_rate'],\n",
    "            'batch_size': run.config['batch_size'],\n",
    "            'label_smoothing_factor': run.config['label_smoothing_factor'],\n",
    "            'weight_decay': run.config['weight_decay'],\n",
    "            'num_epochs': run.config['num_epochs'],\n",
    "            'val_accuracy': run.summary.get('best_val_accuracy', 0)\n",
    "        })\n",
    "\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # 상관관계 분석\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 1. learning_rate vs val_accuracy\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.scatter(results_df['learning_rate'], results_df['val_accuracy'], alpha=0.6)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title('Learning Rate vs Validation Accuracy')\n",
    "    \n",
    "    # 2. batch_size vs val_accuracy\n",
    "    plt.subplot(2, 3, 2)\n",
    "    sns.boxplot(data=results_df, x='batch_size', y='val_accuracy')\n",
    "    plt.xlabel('Batch Size')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title('Batch Size vs Validation Accuracy')\n",
    "    \n",
    "    # 3. label_smoothing_factor vs val_accuracy\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.scatter(results_df['label_smoothing_factor'], results_df['val_accuracy'], alpha=0.6)\n",
    "    plt.xlabel('Label Smoothing Factor')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title('Label Smoothing vs Validation Accuracy')\n",
    "    \n",
    "    # 4. weight_decay vs val_accuracy\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.scatter(results_df['weight_decay'], results_df['val_accuracy'], alpha=0.6)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Weight Decay')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title('Weight Decay vs Validation Accuracy')\n",
    "    \n",
    "    # 5. num_epochs vs val_accuracy\n",
    "    plt.subplot(2, 3, 5)\n",
    "    sns.boxplot(data=results_df, x='num_epochs', y='val_accuracy')\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title('Epochs vs Validation Accuracy')\n",
    "    \n",
    "    # 6. 빈 공간 (또는 추가 분석)\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.text(0.5, 0.5, 'Additional\\nAnalysis\\nSpace', \n",
    "             ha='center', va='center', fontsize=12, alpha=0.5)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 상관관계 매트릭스\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    correlation_matrix = results_df.corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Hyperparameter Correlation Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n📈 하이퍼파라미터 상관관계 분석 완료\")\n",
    "else:\n",
    "    print(\"❌ 분석할 결과가 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적 파라미터를 사용한 최종 훈련\n",
    "\n",
    "위에서 찾은 최적 파라미터를 사용하여 `finetuning_pytorch.ipynb`의 Config 클래스를 다음과 같이 수정하세요:\n",
    "\n",
    "```python\n",
    "class Config:\n",
    "    # ... 기존 설정들 ...\n",
    "    \n",
    "    # 최적화된 하이퍼파라미터 (Sweep 결과)\n",
    "    batch_size = [최적값]\n",
    "    learning_rate = [최적값]\n",
    "    weight_decay = [최적값]\n",
    "    label_smoothing_factor = [최적값]\n",
    "    num_epochs = [최적값]\n",
    "    \n",
    "    # ... 나머지 설정들 ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
