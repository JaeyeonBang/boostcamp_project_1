{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ - PyTorch ëª¨ë¸ í•™ìŠµ\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ PyTorch ê¸°ë°˜ì˜ BERT/RoBERTa ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
        "\n",
        "## ì£¼ìš” ê¸°ëŠ¥\n",
        "- PyTorch ë„¤ì´í‹°ë¸Œ í•™ìŠµ ë£¨í”„ (Transformers Trainer ëŒ€ì‹ )\n",
        "- LoRA íŒŒì¸íŠœë‹\n",
        "- WandB ì‹¤í—˜ ì¶”ì \n",
        "- í˜¼í•© ì •ë°€ë„ í•™ìŠµ (Mixed Precision)\n",
        "- ì¡°ê¸° ì¢…ë£Œ ë° ëª¨ë¸ ì²´í¬í¬ì¸íŒ…\n",
        "- í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/ephemeral/home/py310/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "/data/ephemeral/home/py310/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# Library Import\n",
        "import os\n",
        "import math\n",
        "import warnings\n",
        "from collections import Counter\n",
        "import platform\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import LinearLR\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm.auto import tqdm\n",
        "import wandb\n",
        "\n",
        "# Transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModelForSequenceClassification,\n",
        "    set_seed\n",
        ")\n",
        "\n",
        "# PEFT for LoRA\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë””ë°”ì´ìŠ¤: cuda\n",
            "GPU ê°œìˆ˜: 1\n",
            "   GPU 0: Tesla V100-SXM2-32GB\n"
          ]
        }
      ],
      "source": [
        "# í™˜ê²½ ì„¤ì •\n",
        "RANDOM_STATE = 42\n",
        "set_seed(RANDOM_STATE)\n",
        "\n",
        "# GPU ì„¤ì •\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ë””ë°”ì´ìŠ¤: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    print(\"âš ï¸  CUDA ì‚¬ìš© ë¶ˆê°€ - CPUë¡œ í›ˆë ¨ ì§„í–‰\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROJECT_NAME = \"korean-sentiment-analysis\"\n",
        "MODEL_NAME = \"klue/roberta-base\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "\n",
        "\n",
        "class Config:\n",
        "    # ëª¨ë¸ ì„¤ì •\n",
        "    model_name = MODEL_NAME\n",
        "    local_model_path = \"/data/ephemeral/home/code/klue-roberta-base-local\"\n",
        "    num_classes = 4\n",
        "    max_length = 256\n",
        "    \n",
        "    # í›ˆë ¨ ì„¤ì •\n",
        "    batch_size = 256\n",
        "    eval_batch_size = 256\n",
        "    num_epochs = 5\n",
        "    learning_rate = 2e-5\n",
        "    weight_decay = 0.01\n",
        "    warmup_steps = 500\n",
        "    \n",
        "    # ê¸°íƒ€ ì„¤ì •\n",
        "    gradient_accumulation_steps = 1\n",
        "    max_grad_norm = 1.0\n",
        "    early_stopping_patience = 5\n",
        "    save_best_model = True\n",
        "    \n",
        "    # WandB ì„¤ì •\n",
        "    use_wandb = True\n",
        "    project_name = PROJECT_NAME\n",
        "    run_name = f\"{}pytorch-training\"\n",
        "\n",
        "config = Config()\n",
        "print(\"âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„° ë¡œë“œ\n",
        "print(\"ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
        "\n",
        "# í›ˆë ¨ ë°ì´í„° ë¡œë“œ\n",
        "train_df = pd.read_csv(\"data/train_processed.csv\")\n",
        "print(f\"í›ˆë ¨ ë°ì´í„°: {len(train_df):,}ê°œ\")\n",
        "\n",
        "# ê²€ì¦ ë°ì´í„° ë¡œë“œ\n",
        "val_df = pd.read_csv(\"data/val_processed.csv\")\n",
        "print(f\"ê²€ì¦ ë°ì´í„°: {len(val_df):,}ê°œ\")\n",
        "\n",
        "# ë¼ë²¨ ë§¤í•‘\n",
        "LABEL_MAPPING = {0: \"ê°•í•œ ë¶€ì •\", 1: \"ì•½í•œ ë¶€ì •\", 2: \"ì•½í•œ ê¸ì •\", 3: \"ê°•í•œ ê¸ì •\"}\n",
        "\n",
        "print(\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜\n",
        "class ReviewDataset(Dataset):\n",
        "    \"\"\"\n",
        "    ë¦¬ë·° í…ìŠ¤íŠ¸ ë°ì´ì…‹ í´ë˜ìŠ¤\n",
        "    - BERT ëª¨ë¸ í›ˆë ¨/ì¶”ë¡ ì„ ìœ„í•œ PyTorch Dataset êµ¬í˜„\n",
        "    - í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§• ë° í…ì„œ ë³€í™˜ ì²˜ë¦¬\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        \"\"\"\n",
        "        ë°ì´í„°ì…‹ ì´ˆê¸°í™”\n",
        "        \"\"\"\n",
        "        self.texts, self.labels, self.tokenizer, self.max_length = (\n",
        "            texts,\n",
        "            labels,\n",
        "            tokenizer,\n",
        "            max_length,\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"ë°ì´í„°ì…‹ í¬ê¸° ë°˜í™˜\"\"\"\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        íŠ¹ì • ì¸ë±ìŠ¤ì˜ ë°ì´í„° ì•„ì´í…œ ë°˜í™˜\n",
        "        \"\"\"\n",
        "        # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§• ë° íŒ¨ë”©\n",
        "        encoding = self.tokenizer(\n",
        "            str(self.texts.iloc[idx]),\n",
        "            truncation=True,  # ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ì‹œ ìë¥´ê¸°\n",
        "            padding=\"max_length\",  # ìµœëŒ€ ê¸¸ì´ê¹Œì§€ íŒ¨ë”©\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",  # PyTorch í…ì„œë¡œ ë°˜í™˜\n",
        "        )\n",
        "\n",
        "        # ê¸°ë³¸ ì•„ì´í…œ êµ¬ì„± (input_ids, attention_mask)\n",
        "        item = {\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "        }\n",
        "\n",
        "        # labelsê°€ Noneì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì¶”ê°€ (train/validìš©)\n",
        "        if self.labels is not None:\n",
        "            item[\"labels\"] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
        "\n",
        "        return item\n",
        "\n",
        "print(\"âœ… ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í‰ê°€ ë©”íŠ¸ë¦­ í•¨ìˆ˜\n",
        "def compute_metrics(predictions, labels):\n",
        "    \"\"\"\n",
        "    ëª¨ë¸ í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜\n",
        "    \"\"\"\n",
        "    # ì˜ˆì¸¡ê°’ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì„ íƒ\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    \n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
        "    \n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "    }\n",
        "\n",
        "print(\"âœ… í‰ê°€ ë©”íŠ¸ë¦­ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "print(\"ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...\")\n",
        "\n",
        "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "try:\n",
        "    if os.path.exists(config.local_model_path):\n",
        "        print(f\"ë¡œì»¬ ëª¨ë¸ì—ì„œ í† í¬ë‚˜ì´ì € ë¡œë“œ: {config.local_model_path}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(config.local_model_path)\n",
        "    else:\n",
        "        print(f\"ì˜¨ë¼ì¸ì—ì„œ í† í¬ë‚˜ì´ì € ë¡œë“œ: {config.model_name}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "except Exception as e:\n",
        "    print(f\"í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ\n",
        "try:\n",
        "    if os.path.exists(config.local_model_path):\n",
        "        print(f\"ë¡œì»¬ ëª¨ë¸ì—ì„œ ëª¨ë¸ ë¡œë“œ: {config.local_model_path}\")\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            config.local_model_path,\n",
        "            num_labels=config.num_classes\n",
        "        )\n",
        "    else:\n",
        "        print(f\"ì˜¨ë¼ì¸ì—ì„œ ëª¨ë¸ ë¡œë“œ: {config.model_name}\")\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            config.model_name,\n",
        "            num_labels=config.num_classes\n",
        "        )\n",
        "except Exception as e:\n",
        "    print(f\"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        config.model_name,\n",
        "        num_labels=config.num_classes\n",
        "    )\n",
        "\n",
        "# LoRA ì„¤ì • ì ìš©\n",
        "print(\"LoRA ì„¤ì • ì ìš© ì¤‘...\")\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    r=config.lora_r,\n",
        "    lora_alpha=config.lora_alpha,\n",
        "    target_modules=[\"query\", \"key\", \"value\"],\n",
        "    lora_dropout=config.lora_dropout,\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±\n",
        "print(\"ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...\")\n",
        "\n",
        "# í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±\n",
        "train_dataset = ReviewDataset(\n",
        "    train_df[\"review\"],\n",
        "    train_df[\"label\"],\n",
        "    tokenizer,\n",
        "    config.max_length,\n",
        ")\n",
        "\n",
        "# ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„±\n",
        "val_dataset = ReviewDataset(\n",
        "    val_df[\"review\"],\n",
        "    val_df[\"label\"],\n",
        "    tokenizer,\n",
        "    config.max_length,\n",
        ")\n",
        "\n",
        "# ë°ì´í„°ë¡œë” ìƒì„±\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config.eval_batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(f\"í›ˆë ¨ ë°ì´í„°: {len(train_dataset):,}ê°œ\")\n",
        "print(f\"ê²€ì¦ ë°ì´í„°: {len(val_dataset):,}ê°œ\")\n",
        "print(\"âœ… ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •\n",
        "print(\"ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ì¤‘...\")\n",
        "\n",
        "# ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config.learning_rate,\n",
        "    weight_decay=config.weight_decay\n",
        ")\n",
        "\n",
        "# ì „ì²´ í›ˆë ¨ ìŠ¤í… ê³„ì‚°\n",
        "total_steps = len(train_dataloader) * config.num_epochs // config.gradient_accumulation_steps\n",
        "\n",
        "# ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (warmup + linear decay)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=config.warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# í˜¼í•© ì •ë°€ë„ ìŠ¤ì¼€ì¼ëŸ¬\n",
        "scaler = GradScaler() if torch.cuda.is_available() else None\n",
        "\n",
        "print(f\"ì´ í›ˆë ¨ ìŠ¤í…: {total_steps:,}\")\n",
        "print(f\"ì›Œë°ì—… ìŠ¤í…: {config.warmup_steps:,}\")\n",
        "print(\"âœ… ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WandB ì´ˆê¸°í™”\n",
        "if config.use_wandb:\n",
        "    wandb.init(\n",
        "        project=config.project_name,\n",
        "        name=config.run_name,\n",
        "        config={\n",
        "            \"model_name\": config.model_name,\n",
        "            \"num_epochs\": config.num_epochs,\n",
        "            \"batch_size\": config.batch_size,\n",
        "            \"learning_rate\": config.learning_rate,\n",
        "            \"max_length\": config.max_length,\n",
        "            \"num_classes\": config.num_classes,\n",
        "            \"warmup_steps\": config.warmup_steps,\n",
        "            \"weight_decay\": config.weight_decay,\n",
        "            \"lora_r\": config.lora_r,\n",
        "            \"lora_alpha\": config.lora_alpha,\n",
        "            \"lora_dropout\": config.lora_dropout,\n",
        "            \"random_seed\": RANDOM_STATE\n",
        "        }\n",
        "    )\n",
        "    print(\"âœ… WandB ì´ˆê¸°í™” ì™„ë£Œ\")\n",
        "else:\n",
        "    print(\"âš ï¸  WandB ì‚¬ìš© ì•ˆí•¨\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PyTorch í›ˆë ¨ ë£¨í”„\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, scaler, device, config):\n",
        "    \"\"\"í•œ ì—í¬í¬ í›ˆë ¨\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    \n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "    \n",
        "    for step, batch in enumerate(progress_bar):\n",
        "        # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        batch_labels = batch[\"labels\"].to(device)\n",
        "        \n",
        "        # í˜¼í•© ì •ë°€ë„ í›ˆë ¨\n",
        "        if scaler is not None:\n",
        "            with autocast():\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=batch_labels\n",
        "                )\n",
        "                loss = outputs.loss / config.gradient_accumulation_steps\n",
        "                logits = outputs.logits\n",
        "            \n",
        "            scaler.scale(loss).backward()\n",
        "            \n",
        "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "        else:\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=batch_labels\n",
        "            )\n",
        "            loss = outputs.loss / config.gradient_accumulation_steps\n",
        "            logits = outputs.logits\n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "        \n",
        "        total_loss += loss.item() * config.gradient_accumulation_steps\n",
        "        \n",
        "        # ì˜ˆì¸¡ê°’ ì €ì¥\n",
        "        predictions.extend(logits.detach().cpu().numpy())\n",
        "        labels.extend(batch_labels.detach().cpu().numpy())\n",
        "        \n",
        "        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f'{loss.item() * config.gradient_accumulation_steps:.4f}',\n",
        "            'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
        "        })\n",
        "    \n",
        "    # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
        "    metrics = compute_metrics(predictions, labels)\n",
        "    metrics['loss'] = total_loss / len(dataloader)\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "print(\"âœ… í›ˆë ¨ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê²€ì¦ í•¨ìˆ˜\n",
        "def validate_epoch(model, dataloader, device):\n",
        "    \"\"\"í•œ ì—í¬í¬ ê²€ì¦\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    \n",
        "    progress_bar = tqdm(dataloader, desc=\"Validation\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            batch_labels = batch[\"labels\"].to(device)\n",
        "            \n",
        "            # ìˆœì „íŒŒ\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=batch_labels\n",
        "            )\n",
        "            \n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # ì˜ˆì¸¡ê°’ ì €ì¥\n",
        "            predictions.extend(logits.detach().cpu().numpy())\n",
        "            labels.extend(batch_labels.detach().cpu().numpy())\n",
        "            \n",
        "            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸\n",
        "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "    \n",
        "    # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
        "    metrics = compute_metrics(predictions, labels)\n",
        "    metrics['loss'] = total_loss / len(dataloader)\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "print(\"âœ… ê²€ì¦ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë©”ì¸ í›ˆë ¨ ë£¨í”„\n",
        "print(\"=\" * 50)\n",
        "print(\"ëª¨ë¸ í›ˆë ¨ ì‹œì‘\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# í›ˆë ¨ ì •ë³´ ì¶œë ¥\n",
        "print(f\"í›ˆë ¨ ìƒ˜í”Œ: {len(train_dataset):,}ê°œ\")\n",
        "print(f\"ê²€ì¦ ìƒ˜í”Œ: {len(val_dataset):,}ê°œ\")\n",
        "print(f\"í›ˆë ¨ ì—í¬í¬: {config.num_epochs}íšŒ\")\n",
        "print(f\"ë°°ì¹˜ í¬ê¸°: {config.batch_size} (í›ˆë ¨) / {config.eval_batch_size} (ê²€ì¦)\")\n",
        "print(f\"í•™ìŠµë¥ : {config.learning_rate}\")\n",
        "print(f\"ì‹œë“œê°’: {RANDOM_STATE}\")\n",
        "print(f\"ë””ë°”ì´ìŠ¤: {device}\")\n",
        "print(f\"WandB ì‚¬ìš©: {config.use_wandb}\")\n",
        "\n",
        "# ì¡°ê¸° ì¢…ë£Œ ë° ì²´í¬í¬ì¸íŒ… ì„¤ì •\n",
        "best_val_accuracy = 0\n",
        "patience_counter = 0\n",
        "training_history = []\n",
        "\n",
        "# í›ˆë ¨ ì‹œì‘\n",
        "try:\n",
        "    for epoch in range(config.num_epochs):\n",
        "        print(f\"\\n{'='*20} Epoch {epoch+1}/{config.num_epochs} {'='*20}\")\n",
        "        \n",
        "        # í›ˆë ¨\n",
        "        train_metrics = train_epoch(\n",
        "            model, train_dataloader, optimizer, scheduler, scaler, device, config\n",
        "        )\n",
        "        \n",
        "        # ê²€ì¦\n",
        "        val_metrics = validate_epoch(model, val_dataloader, device)\n",
        "        \n",
        "        # ê²°ê³¼ ì¶œë ¥\n",
        "        print(f\"\\ní›ˆë ¨ ê²°ê³¼:\")\n",
        "        print(f\"  Loss: {train_metrics['loss']:.4f}\")\n",
        "        print(f\"  Accuracy: {train_metrics['accuracy']:.4f}\")\n",
        "        print(f\"  F1: {train_metrics['f1']:.4f}\")\n",
        "        \n",
        "        print(f\"\\nê²€ì¦ ê²°ê³¼:\")\n",
        "        print(f\"  Loss: {val_metrics['loss']:.4f}\")\n",
        "        print(f\"  Accuracy: {val_metrics['accuracy']:.4f}\")\n",
        "        print(f\"  F1: {val_metrics['f1']:.4f}\")\n",
        "        \n",
        "        # WandB ë¡œê¹…\n",
        "        if config.use_wandb:\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train_loss\": train_metrics['loss'],\n",
        "                \"train_accuracy\": train_metrics['accuracy'],\n",
        "                \"train_f1\": train_metrics['f1'],\n",
        "                \"val_loss\": val_metrics['loss'],\n",
        "                \"val_accuracy\": val_metrics['accuracy'],\n",
        "                \"val_f1\": val_metrics['f1'],\n",
        "                \"learning_rate\": scheduler.get_last_lr()[0]\n",
        "            })\n",
        "        \n",
        "        # íˆìŠ¤í† ë¦¬ ì €ì¥\n",
        "        training_history.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_metrics['loss'],\n",
        "            'train_accuracy': train_metrics['accuracy'],\n",
        "            'train_f1': train_metrics['f1'],\n",
        "            'val_loss': val_metrics['loss'],\n",
        "            'val_accuracy': val_metrics['accuracy'],\n",
        "            'val_f1': val_metrics['f1']\n",
        "        })\n",
        "        \n",
        "        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
        "        if config.save_best_model and val_metrics['accuracy'] > best_val_accuracy:\n",
        "            best_val_accuracy = val_metrics['accuracy']\n",
        "            patience_counter = 0\n",
        "            \n",
        "            # ëª¨ë¸ ì €ì¥\n",
        "            os.makedirs(\"./best_model\", exist_ok=True)\n",
        "            model.save_pretrained(\"./best_model\")\n",
        "            tokenizer.save_pretrained(\"./best_model\")\n",
        "            print(f\"âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (Accuracy: {best_val_accuracy:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        # ì¡°ê¸° ì¢…ë£Œ ì²´í¬\n",
        "        if patience_counter >= config.early_stopping_patience:\n",
        "            print(f\"\\nâš ï¸  ì¡°ê¸° ì¢…ë£Œ: {config.early_stopping_patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ\")\n",
        "            break\n",
        "    \n",
        "    print(f\"\\nâœ… í›ˆë ¨ ì™„ë£Œ!\")\n",
        "    print(f\"ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_accuracy:.4f}\")\n",
        "    \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ í›ˆë ¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡ \n",
        "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡  ì‹œì‘...\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
        "test_df = pd.read_csv(\"data/test.csv\")\n",
        "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df):,}ê°œ\")\n",
        "\n",
        "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ\n",
        "if os.path.exists(\"./best_model\"):\n",
        "    print(\"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
        "    inference_model = AutoModelForSequenceClassification.from_pretrained(\"./best_model\")\n",
        "    inference_tokenizer = AutoTokenizer.from_pretrained(\"./best_model\")\n",
        "    inference_model = inference_model.to(device)\n",
        "    inference_model.eval()\n",
        "    print(\"âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
        "else:\n",
        "    print(\"âš ï¸  ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ëª¨ë¸ ì‚¬ìš©\")\n",
        "    inference_model = model\n",
        "    inference_tokenizer = tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±\n",
        "test_dataset = ReviewDataset(\n",
        "    test_df[\"review\"],\n",
        "    None,  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ë¼ë²¨ ì—†ìŒ\n",
        "    inference_tokenizer,\n",
        "    config.max_length,\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config.eval_batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì¶”ë¡  ì‹¤í–‰\n",
        "print(\"ì¶”ë¡  ì‹¤í–‰ ì¤‘...\")\n",
        "inference_model.eval()\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dataloader, desc=\"Inference\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        \n",
        "        outputs = inference_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        \n",
        "        logits = outputs.logits\n",
        "        batch_predictions = torch.argmax(logits, dim=-1)\n",
        "        predictions.extend(batch_predictions.cpu().numpy())\n",
        "\n",
        "print(f\"âœ… ì¶”ë¡  ì™„ë£Œ: {len(predictions):,}ê°œ ì˜ˆì¸¡\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì œì¶œ íŒŒì¼ ìƒì„±\n",
        "print(\"ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\n",
        "\n",
        "# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€\n",
        "test_df[\"pred\"] = predictions\n",
        "\n",
        "# í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„í¬ í™•ì¸\n",
        "unique_predictions, counts = np.unique(predictions, return_counts=True)\n",
        "print(\"\\ní´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„í¬:\")\n",
        "for pred, count in zip(unique_predictions, counts):\n",
        "    percentage = (count / len(predictions)) * 100\n",
        "    class_name = LABEL_MAPPING.get(pred, f\"í´ë˜ìŠ¤ {pred}\")\n",
        "    print(f\"   {class_name} ({pred}): {count:,}ê°œ ({percentage:.1f}%)\")\n",
        "\n",
        "# ìƒ˜í”Œ ì œì¶œ íŒŒì¼ ë¡œë“œ\n",
        "sample_submission = pd.read_csv(\"data/sample_submission.csv\")\n",
        "\n",
        "# IDë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©í•˜ì—¬ ì œì¶œ íŒŒì¼ ìƒì„±\n",
        "submission_df = sample_submission[[\"ID\"]].merge(\n",
        "    test_df[[\"ID\", \"pred\"]], \n",
        "    left_on=\"ID\", \n",
        "    right_on=\"ID\", \n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# ì œì¶œ íŒŒì¼ ê²€ì¦\n",
        "assert len(submission_df) == len(sample_submission), f\"ê¸¸ì´ ë¶ˆì¼ì¹˜: {len(submission_df)} vs {len(sample_submission)}\"\n",
        "assert submission_df[\"pred\"].isin([0, 1, 2, 3]).all(), \"ëª¨ë“  ì˜ˆì¸¡ê°’ì€ [0, 1, 2, 3] ë²”ìœ„ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤\"\n",
        "assert not submission_df[\"pred\"].isnull().any(), \"ì˜ˆì¸¡ê°’ì— null ê°’ì´ ìˆìœ¼ë©´ ì•ˆë©ë‹ˆë‹¤\"\n",
        "assert not submission_df[\"ID\"].isnull().any(), \"ID ì»¬ëŸ¼ì— null ê°’ì´ ìˆìœ¼ë©´ ì•ˆë©ë‹ˆë‹¤\"\n",
        "\n",
        "print(\"âœ… ì œì¶œ íŒŒì¼ ê²€ì¦ í†µê³¼\")\n",
        "\n",
        "# ì œì¶œ íŒŒì¼ ì €ì¥\n",
        "submission_path = \"./output.csv\"\n",
        "submission_df.to_csv(submission_path, index=False)\n",
        "print(f\"âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {submission_path}\")\n",
        "\n",
        "# WandB ì¢…ë£Œ\n",
        "if config.use_wandb:\n",
        "    wandb.finish()\n",
        "    print(\"âœ… WandB ì„¸ì…˜ ì¢…ë£Œ\")\n",
        "\n",
        "print(\"\\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡ \n",
        "print(\"=\" * 50)\n",
        "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡ \")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
        "test_df = pd.read_csv(\"data/test.csv\")\n",
        "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df):,}ê°œ\")\n",
        "\n",
        "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ\n",
        "if os.path.exists(\"./best_model\"):\n",
        "    print(\"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
        "    inference_model = AutoModelForSequenceClassification.from_pretrained(\"./best_model\")\n",
        "    inference_tokenizer = AutoTokenizer.from_pretrained(\"./best_model\")\n",
        "    inference_model = inference_model.to(device)\n",
        "    inference_model.eval()\n",
        "    print(\"âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
        "else:\n",
        "    print(\"âš ï¸  ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ëª¨ë¸ ì‚¬ìš©\")\n",
        "    inference_model = model\n",
        "    inference_tokenizer = tokenizer\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±\n",
        "test_dataset = ReviewDataset(\n",
        "    test_df[\"review\"],\n",
        "    None,  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ë¼ë²¨ ì—†ìŒ\n",
        "    inference_tokenizer,\n",
        "    config.max_length,\n",
        ")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ìƒì„±\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config.eval_batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
