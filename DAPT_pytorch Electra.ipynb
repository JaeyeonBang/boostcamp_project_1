{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국어 텍스트 감정 분석 - PyTorch 모델 학습\n",
    "\n",
    "이 노트북은 전처리된 데이터를 사용하여 PyTorch 기반의 BERT/RoBERTa 모델을 학습합니다.\n",
    "\n",
    "## 주요 기능\n",
    "- PyTorch 네이티브 학습 루프 (Transformers Trainer 대신)\n",
    "- LoRA 파인튜닝\n",
    "- WandB 실험 추적\n",
    "- 혼합 정밀도 학습 (Mixed Precision)\n",
    "- 조기 종료 및 모델 체크포인팅\n",
    "- 테스트 데이터 추론 및 제출 파일 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/py310/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/py310/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 라이브러리 임포트 완료\n"
     ]
    }
   ],
   "source": [
    "# Library Import\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    set_seed,\n",
    "    AutoModelForMaskedLM\n",
    ")\n",
    "\n",
    "# PEFT for LoRA\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# 경고 메시지 필터링\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"✅ 라이브러리 임포트 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"DAPT_30_epochs_augX3\"\n",
    "PROJECT_NAME = f\"[domain_project]_Experiment_DAPT\"\n",
    "MODEL_NAME = \"kykim_bert-kor-base\"\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디바이스: cuda\n",
      "GPU 개수: 1\n",
      "   GPU 0: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "# 환경 설정\n",
    "RANDOM_STATE = 42\n",
    "set_seed(RANDOM_STATE)\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"디바이스: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 개수: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"⚠️  CUDA 사용 불가 - CPU로 훈련 진행\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 이 함수가 .env 파일을 읽어서 환경 변수로 로드합니다.\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "molels = [\"beomi_kcbert-base\", \"klue_roberta-base\", \"klue_bert-base\",\"kykim_bert-kor-base\", \"monologg_koelectra-base-v3-discriminator\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 하이퍼파라미터 설정 완료\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "\n",
    "class Config:\n",
    "    # 모델 설정\n",
    "    model_name = MODEL_NAME\n",
    "    base_models_path = \"/data/ephemeral/home/code/basemodels/\"\n",
    "    local_model_path = base_models_path + MODEL_NAME\n",
    "    num_classes = 4\n",
    "    max_length = 128\n",
    "\n",
    "    save_best_model_path = \"./best_unsupervised_model\"\n",
    "    \n",
    "    # 훈련 설정\n",
    "    unsupervised_batch_size = 196\n",
    "    #batch_size = 256\n",
    "\n",
    "    DAPT_num_epochs = 20\n",
    "    TAPT_num_epochs = 30\n",
    "\n",
    "\n",
    "    learning_rate = 2e-5\n",
    "    weight_decay = 0.01\n",
    "    warmup_steps = 500\n",
    "    \n",
    "    # 기타 설정\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1.0\n",
    "    early_stopping_patience = 3\n",
    "    save_best_model = True\n",
    "    \n",
    "    # WandB 설정\n",
    "    use_wandb = True\n",
    "    project_name = PROJECT_NAME\n",
    "    run_name = f\"{EXPERIMENT_NAME}-training\"\n",
    "\n",
    "config = Config()\n",
    "print(\"✅ 하이퍼파라미터 설정 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 데이터 로드 중...\n",
      "훈련 데이터: 571,014개\n",
      "검증 데이터: 63,446개\n",
      "테스트 데이터: 59,928개\n",
      "✅ 데이터 로드 완료\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "print(\"전처리된 데이터 로드 중...\")\n",
    "\n",
    "# 훈련 데이터 로드\n",
    "train_df = pd.read_csv(\"/data/ephemeral/home/code/data/train_processed_augX3_newly_gen_added.csv\")\n",
    "print(f\"훈련 데이터: {len(train_df):,}개\")\n",
    "\n",
    "# 검증 데이터 로드\n",
    "val_df = pd.read_csv(\"/data/ephemeral/home/code/data/val_processed_augX3_newly_gen_added.csv\")\n",
    "print(f\"검증 데이터: {len(val_df):,}개\")\n",
    "\n",
    "test_df = pd.read_csv(\"data/test_processed.csv\")\n",
    "print(f\"테스트 데이터: {len(test_df):,}개\")\n",
    "\n",
    "# 라벨 매핑\n",
    "LABEL_MAPPING = {0: \"강한 부정\", 1: \"약한 부정\", 2: \"약한 긍정\", 3: \"강한 긍정\"}\n",
    "\n",
    "print(\"✅ 데이터 로드 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'review', 'label', 'type'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlabeledDataset(Dataset):\n",
    "    \"\"\"\n",
    "    라벨이 없는 데이터셋 클래스 (ELECTRA RTD 작업 지원)\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, tokenizer, max_length, mask_probability=0.15):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mask_probability = mask_probability\n",
    "        self.mask_token_id = tokenizer.mask_token_id\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def _apply_electra_masking(self, input_ids, attention_mask):\n",
    "        \"\"\"ELECTRA 스타일 마스킹 적용 (Generator용)\"\"\"\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        # 패딩 토큰이 아닌 위치만 선택\n",
    "        non_padding_mask = attention_mask.bool()\n",
    "        \n",
    "        # 마스킹할 토큰 선택 (패딩 토큰 제외)\n",
    "        candidate_indices = torch.where(non_padding_mask)[0]\n",
    "        \n",
    "        if len(candidate_indices) == 0:\n",
    "            return input_ids, labels\n",
    "        \n",
    "        # 마스킹할 토큰 수 계산\n",
    "        num_tokens_to_mask = max(1, int(len(candidate_indices) * self.mask_probability))\n",
    "        \n",
    "        # 랜덤하게 토큰 선택\n",
    "        selected_indices = torch.randperm(len(candidate_indices))[:num_tokens_to_mask]\n",
    "        selected_positions = candidate_indices[selected_indices]\n",
    "        \n",
    "        # 선택된 토큰에 대해 마스킹 적용\n",
    "        for pos in selected_positions:\n",
    "            if torch.rand(1) < 1:\n",
    "                input_ids[pos] = self.mask_token_id\n",
    "        \n",
    "        return input_ids, labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"인덱스에 해당하는 샘플 반환 (ELECTRA RTD 작업용)\"\"\"\n",
    "        # 텍스트 추출 - 문자열로 변환 보장\n",
    "        if hasattr(self.texts, 'iloc'):\n",
    "            text = str(self.texts.iloc[idx])  # pandas Series인 경우\n",
    "        else:\n",
    "            text = str(self.texts[idx])  # list나 다른 타입인 경우\n",
    "        \n",
    "        # None이나 NaN 값 처리\n",
    "        if text == 'None' or text == 'nan' or text == '':\n",
    "            text = \"빈 텍스트입니다.\"  # 기본값 설정\n",
    "        \n",
    "        # 텍스트 토크나이징\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "        \n",
    "        # ELECTRA 마스킹 적용 (Generator용)\n",
    "        masked_input_ids, mlm_labels = self._apply_electra_masking(input_ids, attention_mask)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,  # 원본 토큰 (Discriminator용)\n",
    "            'masked_input_ids': masked_input_ids,  # 마스킹된 토큰 (Generator용)\n",
    "            'attention_mask': attention_mask,\n",
    "            'mlm_labels': mlm_labels  # Generator 학습을 위한 라벨\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 평가 메트릭 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# 평가 메트릭 함수\n",
    "def compute_metrics(predictions, labels):\n",
    "    \"\"\"\n",
    "    모델 평가 메트릭 계산 함수\n",
    "    \"\"\"\n",
    "    # 예측값에서 가장 높은 확률의 클래스 선택\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "print(\"✅ 평가 메트릭 함수 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load_from_local(local_model_path : str = None, model_name : str = None):\n",
    "    # 1. 토크나이저 로드\n",
    "    # 절대 경로를 사용하여 로컬 모델 로드\n",
    "    print(\"로컬 경로에서 토크나이저 로딩 중...\")\n",
    "\n",
    "    # 경로가 존재하는지 확인\n",
    "    if not os.path.exists(local_model_path):\n",
    "        print(f\"❌ 경로가 존재하지 않습니다: {local_model_path}\")\n",
    "        return None, None, None\n",
    "    else:\n",
    "        print(f\"✅ 경로 확인됨: {local_model_path}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "\n",
    "    # 2. ELECTRA 모델 로드 (Generator + Discriminator)\n",
    "    print(\"로컬 경로에서 ELECTRA 모델 로딩 중...\")\n",
    "    \n",
    "    # Generator (MLM용)\n",
    "    generator = AutoModelForMaskedLM.from_pretrained(local_model_path)\n",
    "    \n",
    "    # Discriminator (RTD용) - Generator와 동일한 구조이지만 RTD 헤드 사용\n",
    "    discriminator = AutoModelForMaskedLM.from_pretrained(local_model_path)\n",
    "    \n",
    "    # Discriminator의 MLM 헤드를 RTD 헤드로 교체\n",
    "    from transformers import ElectraForPreTraining\n",
    "    discriminator = ElectraForPreTraining.from_pretrained(local_model_path)\n",
    "\n",
    "    print(\"✅ 로컬 스냅샷에서 ELECTRA 모델과 토크나이저 로딩 성공!\")\n",
    "\n",
    "    # --- 이제 평소처럼 모델을 사용할 수 있습니다 ---\n",
    "    inputs = tokenizer(\"이 영화 정말 재미있네요!\", return_tensors=\"pt\")\n",
    "    outputs = generator(**inputs)\n",
    "    print(\"Generator outputs shape:\", outputs.logits.shape)\n",
    "\n",
    "    return generator, discriminator, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로컬 경로에서 토크나이저 로딩 중...\n",
      "✅ 경로 확인됨: /data/ephemeral/home/code/basemodels/kykim_bert-kor-base\n",
      "로컬 경로에서 모델 로딩 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/ephemeral/home/code/basemodels/kykim_bert-kor-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 로컬 스냅샷에서 모델과 토크나이저 로딩 성공!\n",
      "tensor([[[ -8.7732,  -5.3598,  -4.4707,  ...,  -5.5349,  -4.1685,  -6.2984],\n",
      "         [-13.5281,  -7.3256,  -5.6978,  ...,  -4.6070,  -7.2017, -13.3684],\n",
      "         [-13.5850, -10.6456,  -6.2533,  ...,  -5.0207,  -6.9485, -10.5358],\n",
      "         ...,\n",
      "         [-13.1823,  -8.8666,  -4.9712,  ..., -10.3408,  -7.1187, -11.3603],\n",
      "         [-11.3309,  -6.7285,  -2.5937,  ...,  -6.8013,  -4.9377, -10.5218],\n",
      "         [ -9.3873,  -4.1437,  -6.7594,  ...,  -3.4999,  -1.9037,  -9.5742]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "generator, discriminator, tokenizer = model_load_from_local(config.local_model_path, config.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 및 토크나이저 로드 완료\n"
     ]
    }
   ],
   "source": [
    "# 모델을 디바이스로 이동\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "print(\"✅ ELECTRA 모델 및 토크나이저 로드 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(42000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=42000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   review\n",
      "0                          이런 애매한 끝맺음은 좀 아니지 않나 싶네. ㅡ,. ㅡ\n",
      "1       와, 오랜만에 이 영화 다시 봤는데 정말이지. 편집은 무슨 정신으로 한 건지 도저히...\n",
      "2       캐릭터들도 하나같이 너무 매력적이어서 보는 내내 미소가 끊이질 않았어. 특히 조카가...\n",
      "3           나도 소름 끼치면서 잘봣는데! 나이트샤말론감독 영화는 다른영화들과 좀 다른듯!멋짐\n",
      "4       두 사람의 케미가 정말 최고였고, 보는 내내 입가에 미소가 떠나지 않았어요. 특히 ...\n",
      "...                                                   ...\n",
      "694383  여자 주인공은 예쁜데 잠깐 나오고 전체적으로는 좋은 내용이지만 세부적인 내용들이 많...\n",
      "694384                기대 안하고 봤는데 오랜만에 보는 정말 좋은 코미디 영화였습니다\n",
      "694385  서론이 너무길고 별거아닌거에 나레이션을 너무많이깔아서 지루함.. 스토리는 별거아닌데...\n",
      "694386                                   고급스럽게 포장된 고급 영화.\n",
      "694387                             난이런밋밋한영화가별로라서..살짝지루했음.\n",
      "\n",
      "[694388 rows x 1 columns]\n",
      "196381\n"
     ]
    }
   ],
   "source": [
    "unlabeled_df = pd.DataFrame(pd.concat([\n",
    "    train_df[\"review\"], \n",
    "    val_df[\"review\"], \n",
    "    test_df[\"review\"]\n",
    "], ignore_index=True))\n",
    "\n",
    "original_df = pd.DataFrame(pd.concat([\n",
    "    train_df[train_df[\"type\"] == \"original\"][\"review\"], \n",
    "    val_df[val_df[\"type\"] == \"original\"][\"review\"], \n",
    "    test_df[\"review\"]\n",
    "], ignore_index=True))\n",
    "\n",
    "print(unlabeled_df)\n",
    "print(len(original_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 및 데이터로더 생성 중...\n",
      "Unlabeled 데이터: 694,388개\n",
      "Original 데이터: 196,381개\n",
      "✅ 데이터셋 및 데이터로더 생성 완료\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 및 데이터로더 생성\n",
    "print(\"데이터셋 및 데이터로더 생성 중...\")\n",
    "\n",
    "original_dataset = UnlabeledDataset(\n",
    "    original_df[\"review\"],\n",
    "    tokenizer,\n",
    "    config.max_length,\n",
    ")\n",
    "\n",
    "unlabeled_dataset = UnlabeledDataset(\n",
    "    unlabeled_df[\"review\"],\n",
    "    tokenizer,\n",
    "    config.max_length,\n",
    ")\n",
    "\n",
    "unlabeled_dataloader = DataLoader(\n",
    "    unlabeled_dataset,\n",
    "    batch_size=config.unsupervised_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "original_dataloader = DataLoader(\n",
    "    original_dataset,\n",
    "    batch_size=config.unsupervised_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"Unlabeled 데이터: {len(unlabeled_dataset):,}개\")\n",
    "print(f\"Original 데이터: {len(original_dataset):,}개\")\n",
    "print(\"✅ 데이터셋 및 데이터로더 생성 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "옵티마이저 및 스케줄러 설정 중...\n",
      "DAPT 총 훈련 스텝: 70,860\n",
      "TAPT 총 훈련 스텝: 30,060\n",
      "DAPT 워밍업 스텝: 500\n",
      "TAPT 워밍업 스텝: 500\n",
      "✅ 옵티마이저 및 스케줄러 설정 완료\n"
     ]
    }
   ],
   "source": [
    "# 옵티마이저 및 스케줄러 설정\n",
    "print(\"옵티마이저 및 스케줄러 설정 중...\")\n",
    "\n",
    "# 옵티마이저 설정 (Generator + Discriminator)\n",
    "optimizer = AdamW(\n",
    "    list(generator.parameters()) + list(discriminator.parameters()),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "# 전체 훈련 스텝 계산\n",
    "DAPT_total_steps = len(unlabeled_dataloader) * config.DAPT_num_epochs // config.gradient_accumulation_steps\n",
    "TAPT_total_steps = len(original_dataloader) * config.TAPT_num_epochs // config.gradient_accumulation_steps\n",
    "\n",
    "# 스케줄러 설정 (warmup + linear decay)\n",
    "DAPT_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config.warmup_steps,\n",
    "    num_training_steps=DAPT_total_steps\n",
    ")\n",
    "TAPT_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config.warmup_steps,\n",
    "    num_training_steps=TAPT_total_steps\n",
    ")\n",
    "\n",
    "# 혼합 정밀도 스케일러\n",
    "scaler = GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "print(f\"DAPT 총 훈련 스텝: {DAPT_total_steps:,}\")\n",
    "print(f\"TAPT 총 훈련 스텝: {TAPT_total_steps:,}\")\n",
    "print(f\"DAPT 워밍업 스텝: {config.warmup_steps:,}\")\n",
    "print(f\"TAPT 워밍업 스텝: {config.warmup_steps:,}\")\n",
    "print(\"✅ 옵티마이저 및 스케줄러 설정 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqkdwodus777\u001b[0m (\u001b[33mqkdwodus777-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/code/wandb/run-20251028_040242-r2xhxh4t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Experiment_DAPT/runs/r2xhxh4t' target=\"_blank\">DAPT_30_epochs_augX3-training</a></strong> to <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Experiment_DAPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Experiment_DAPT' target=\"_blank\">https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Experiment_DAPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Experiment_DAPT/runs/r2xhxh4t' target=\"_blank\">https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Experiment_DAPT/runs/r2xhxh4t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ WandB 초기화 완료\n"
     ]
    }
   ],
   "source": [
    "# WandB 초기화\n",
    "if config.use_wandb:\n",
    "    wandb.init(\n",
    "        project=config.project_name,\n",
    "        name=config.run_name,\n",
    "        config={\n",
    "            \"model_name\": config.model_name,\n",
    "            \"DAPT_num_epochs\": config.DAPT_num_epochs,\n",
    "            \"TAPT_num_epochs\": config.TAPT_num_epochs,\n",
    "            \"batch_size\": config.unsupervised_batch_size,\n",
    "            \"learning_rate\": config.learning_rate,\n",
    "            \"max_length\": config.max_length,\n",
    "            \"num_classes\": config.num_classes,\n",
    "            \"warmup_steps\": config.warmup_steps,\n",
    "            \"weight_decay\": config.weight_decay,\n",
    "            # \"lora_r\": config.lora_r,\n",
    "            # \"lora_alpha\": config.lora_alpha,\n",
    "            # \"lora_dropout\": config.lora_dropout,\n",
    "            \"random_seed\": RANDOM_STATE\n",
    "        }\n",
    "    )\n",
    "    print(\"✅ WandB 초기화 완료\")\n",
    "else:\n",
    "    print(\"⚠️  WandB 사용 안함\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsupervie_train_epoch(generator, discriminator, dataloader, optimizer, scheduler, scaler, device, config):\n",
    "    \"\"\"한 에포크 훈련 (ELECTRA 비지도 학습)\"\"\"\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    total_loss = 0\n",
    "    total_mlm_loss = 0\n",
    "    total_rtd_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"ELECTRA Unsupervised Training\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        input_ids = batch[\"input_ids\"].to(device)  # 원본 토큰\n",
    "        masked_input_ids = batch[\"masked_input_ids\"].to(device)  # 마스킹된 토큰\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        mlm_labels = batch[\"mlm_labels\"].to(device)  # Generator용 라벨\n",
    "        \n",
    "        # 그래디언트 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 혼합 정밀도 학습\n",
    "        if scaler is not None:\n",
    "            with autocast():\n",
    "                # 1. Generator 학습 (MLM)\n",
    "                gen_outputs = generator(\n",
    "                    input_ids=masked_input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=mlm_labels\n",
    "                )\n",
    "                mlm_loss = gen_outputs.loss\n",
    "                \n",
    "                # 2. Generator로 대체 토큰 생성\n",
    "                with torch.no_grad():\n",
    "                    gen_logits = generator(\n",
    "                        input_ids=masked_input_ids,\n",
    "                        attention_mask=attention_mask\n",
    "                    ).logits\n",
    "                    generated_tokens = torch.argmax(gen_logits, dim=-1)\n",
    "                \n",
    "                # 3. RTD 라벨 생성 (원본과 다른 토큰 위치)\n",
    "                rtd_labels = (input_ids != generated_tokens).float()\n",
    "                \n",
    "                # 4. Discriminator 학습 (RTD)\n",
    "                disc_outputs = discriminator(\n",
    "                    input_ids=generated_tokens,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=rtd_labels\n",
    "                )\n",
    "                rtd_loss = disc_outputs.loss\n",
    "                \n",
    "                # 5. 전체 손실 (Generator + Discriminator)\n",
    "                total_batch_loss = mlm_loss + rtd_loss\n",
    "                \n",
    "            # 스케일된 그래디언트 계산\n",
    "            scaler.scale(total_batch_loss).backward()\n",
    "            \n",
    "            # 그래디언트 클리핑\n",
    "            if config.max_grad_norm > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(list(generator.parameters()) + list(discriminator.parameters()), config.max_grad_norm)\n",
    "            \n",
    "            # 옵티마이저 스텝\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "        else:\n",
    "            # 일반 정밀도 학습\n",
    "            # 1. Generator 학습 (MLM)\n",
    "            gen_outputs = generator(\n",
    "                input_ids=masked_input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=mlm_labels\n",
    "            )\n",
    "            mlm_loss = gen_outputs.loss\n",
    "            \n",
    "            # 2. Generator로 대체 토큰 생성\n",
    "            with torch.no_grad():\n",
    "                gen_logits = generator(\n",
    "                    input_ids=masked_input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                ).logits\n",
    "                generated_tokens = torch.argmax(gen_logits, dim=-1)\n",
    "            \n",
    "            # 3. RTD 라벨 생성 (원본과 다른 토큰 위치)\n",
    "            rtd_labels = (input_ids != generated_tokens).float()\n",
    "            \n",
    "            # 4. Discriminator 학습 (RTD)\n",
    "            disc_outputs = discriminator(\n",
    "                input_ids=generated_tokens,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=rtd_labels\n",
    "            )\n",
    "            rtd_loss = disc_outputs.loss\n",
    "            \n",
    "            # 5. 전체 손실 (Generator + Discriminator)\n",
    "            total_batch_loss = mlm_loss + rtd_loss\n",
    "            \n",
    "            # 역전파\n",
    "            total_batch_loss.backward()\n",
    "            \n",
    "            # 그래디언트 클리핑\n",
    "            if config.max_grad_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(list(generator.parameters()) + list(discriminator.parameters()), config.max_grad_norm)\n",
    "            \n",
    "            # 옵티마이저 스텝\n",
    "            optimizer.step()\n",
    "        \n",
    "        # 스케줄러 스텝\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # 손실 누적\n",
    "        total_loss += total_batch_loss.item()\n",
    "        total_mlm_loss += mlm_loss.item()\n",
    "        total_rtd_loss += rtd_loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # 진행률 표시 업데이트\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_mlm_loss = total_mlm_loss / num_batches\n",
    "        avg_rtd_loss = total_rtd_loss / num_batches\n",
    "        progress_bar.set_postfix({\n",
    "            'total_loss': f'{avg_loss:.4f}',\n",
    "            'mlm_loss': f'{avg_mlm_loss:.4f}',\n",
    "            'rtd_loss': f'{avg_rtd_loss:.4f}',\n",
    "            'lr': f'{optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "        })\n",
    "        \n",
    "        # 그래디언트 누적 (선택사항)\n",
    "        if config.gradient_accumulation_steps > 1:\n",
    "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "    \n",
    "    # 평균 손실 계산\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    avg_mlm_loss = total_mlm_loss / num_batches if num_batches > 0 else 0\n",
    "    avg_rtd_loss = total_rtd_loss / num_batches if num_batches > 0 else 0\n",
    "    \n",
    "    return avg_loss, avg_mlm_loss, avg_rtd_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DAPT_training_loop(generator, discriminator, unlabeled_dataloader, optimizer, scheduler, scaler, device, config):\n",
    "    \"\"\"ELECTRA 비지도 학습 메인 루프\"\"\"\n",
    "    \n",
    "    # 초기화\n",
    "    best_unsup_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    print(\"🚀 ELECTRA DAPT 학습 시작...\")\n",
    "    print(f\"총 에포크: {config.DAPT_num_epochs}\")\n",
    "    print(f\"조기 종료 인내심: {config.early_stopping_patience}\")\n",
    "    \n",
    "        \n",
    "    for epoch in range(config.DAPT_num_epochs):\n",
    "        print(f\"\\n=== Epoch {epoch+1}/{config.DAPT_num_epochs} ===\")\n",
    "        \n",
    "        # ELECTRA 비지도 학습 에포크\n",
    "        total_loss, mlm_loss, rtd_loss = unsupervie_train_epoch(\n",
    "            generator=generator,\n",
    "            discriminator=discriminator,\n",
    "            dataloader=unlabeled_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            scaler=scaler,\n",
    "            device=device,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        print(f\"Total Loss: {total_loss:.4f}\")\n",
    "        print(f\"MLM Loss: {mlm_loss:.4f}\")\n",
    "        print(f\"RTD Loss: {rtd_loss:.4f}\")\n",
    "        \n",
    "        # 모델 저장 조건 (손실이 개선되었을 때)\n",
    "        if config.save_best_model and total_loss < best_unsup_loss:\n",
    "            best_unsup_loss = total_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # 모델 저장\n",
    "            os.makedirs(config.save_best_model_path, exist_ok=True)\n",
    "            generator.save_pretrained(f\"./{config.save_best_model_path}/DAPT_{MODEL_NAME}_augX3_best_generator_1028\")\n",
    "            discriminator.save_pretrained(f\"./{config.save_best_model_path}/DAPT_{MODEL_NAME}_augX3_best_discriminator_1028\")\n",
    "            tokenizer.save_pretrained(f\"./{config.save_best_model_path}/DAPT_{MODEL_NAME}_augX3_best_generator_1028\")\n",
    "            print(f\"✅ 최고 성능 ELECTRA 모델 저장 (Total Loss: {best_unsup_loss:.4f})\")\n",
    "            \n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"⏳ 조기 종료 카운터: {patience_counter}/{config.early_stopping_patience}\")\n",
    "        \n",
    "        # WandB 로깅 (선택사항)\n",
    "        if config.use_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"total_loss\": total_loss,\n",
    "                \"mlm_loss\": mlm_loss,\n",
    "                \"rtd_loss\": rtd_loss,\n",
    "                \"best_unsupervised_loss\": best_unsup_loss,\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "        \n",
    "        # 조기 종료 체크\n",
    "        if patience_counter >= config.early_stopping_patience:\n",
    "            print(f\"🛑 조기 종료: {config.early_stopping_patience} 에포크 동안 개선 없음\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n🎯 ELECTRA 비지도 학습 완료!\")\n",
    "    print(f\"최고 Total Loss: {best_unsup_loss:.4f}\")\n",
    "    \n",
    "    return best_unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TAPT_training_loop(generator, discriminator, unlabeled_dataloader, optimizer, scheduler, scaler, device, config):\n",
    "    \"\"\"ELECTRA 비지도 학습 메인 루프\"\"\"\n",
    "    \n",
    "    # 초기화\n",
    "    best_unsup_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    print(\"🚀 ELECTRA TAPT 학습 시작...\")\n",
    "    print(f\"총 에포크: {config.TAPT_num_epochs}\")\n",
    "    print(f\"조기 종료 인내심: {config.early_stopping_patience}\")\n",
    "    \n",
    "        \n",
    "    for epoch in range(config.TAPT_num_epochs):\n",
    "        print(f\"\\n=== Epoch {epoch+1}/{config.TAPT_num_epochs} ===\")\n",
    "        \n",
    "        # ELECTRA 비지도 학습 에포크\n",
    "        total_loss, mlm_loss, rtd_loss = unsupervie_train_epoch(\n",
    "            generator=generator,\n",
    "            discriminator=discriminator,\n",
    "            dataloader=unlabeled_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            scaler=scaler,\n",
    "            device=device,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        print(f\"Total Loss: {total_loss:.4f}\")\n",
    "        print(f\"MLM Loss: {mlm_loss:.4f}\")\n",
    "        print(f\"RTD Loss: {rtd_loss:.4f}\")\n",
    "        \n",
    "        # 모델 저장 조건 (손실이 개선되었을 때)\n",
    "        if config.save_best_model and total_loss < best_unsup_loss:\n",
    "            best_unsup_loss = total_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # 모델 저장\n",
    "            os.makedirs(config.save_best_model_path, exist_ok=True)\n",
    "            generator.save_pretrained(f\"./{config.save_best_model_path}/TAPT_{MODEL_NAME}_augX3_best_generator_1026\")\n",
    "            discriminator.save_pretrained(f\"./{config.save_best_model_path}/TAPT_{MODEL_NAME}_augX3_best_discriminator_1026\")\n",
    "            tokenizer.save_pretrained(f\"./{config.save_best_model_path}/TAPT_{MODEL_NAME}_augX3_best_generator_1026\")\n",
    "            print(f\"✅ 최고 성능 ELECTRA 모델 저장 (Total Loss: {best_unsup_loss:.4f})\")\n",
    "            \n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"⏳ 조기 종료 카운터: {patience_counter}/{config.early_stopping_patience}\")\n",
    "        \n",
    "        # WandB 로깅 (선택사항)\n",
    "        if config.use_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"total_loss\": total_loss,\n",
    "                \"mlm_loss\": mlm_loss,\n",
    "                \"rtd_loss\": rtd_loss,\n",
    "                \"best_unsupervised_loss\": best_unsup_loss,\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "        \n",
    "        # 조기 종료 체크\n",
    "        if patience_counter >= config.early_stopping_patience:\n",
    "            print(f\"🛑 조기 종료: {config.early_stopping_patience} 에포크 동안 개선 없음\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n🎯 ELECTRA 비지도 학습 완료!\")\n",
    "    print(f\"최고 Total Loss: {best_unsup_loss:.4f}\")\n",
    "    \n",
    "    return best_unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 DAPT 학습 시작...\n",
      "총 에포크: 20\n",
      "조기 종료 인내심: 3\n",
      "\n",
      "=== Epoch 1/20 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e233c61dab5845c3932dfe08231ad009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsupervised Training:   0%|          | 0/3543 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_loss_stage_DAPT = DAPT_training_loop(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    unlabeled_dataloader=unlabeled_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=DAPT_scheduler,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss_stage_2 = TAPT_training_loop(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    unlabeled_dataloader=original_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=TAPT_scheduler,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 추론 실행\n",
    "# print(\"추론 실행 중...\")\n",
    "# inference_model.eval()\n",
    "# predictions = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in tqdm(test_dataloader, desc=\"Inference\"):\n",
    "#         input_ids = batch[\"input_ids\"].to(device)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "#         outputs = inference_model(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "        \n",
    "#         logits = outputs.logits\n",
    "#         batch_predictions = torch.argmax(logits, dim=-1)\n",
    "#         predictions.extend(batch_predictions.cpu().numpy())\n",
    "\n",
    "# print(f\"✅ 추론 완료: {len(predictions):,}개 예측\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 제출 파일 생성\n",
    "# print(\"제출 파일 생성 중...\")\n",
    "\n",
    "# # 예측 결과를 데이터프레임에 추가\n",
    "# test_df[\"pred\"] = predictions\n",
    "\n",
    "# # 클래스별 예측 분포 확인\n",
    "# unique_predictions, counts = np.unique(predictions, return_counts=True)\n",
    "# print(\"\\n클래스별 예측 분포:\")\n",
    "# for pred, count in zip(unique_predictions, counts):\n",
    "#     percentage = (count / len(predictions)) * 100\n",
    "#     class_name = LABEL_MAPPING.get(pred, f\"클래스 {pred}\")\n",
    "#     print(f\"   {class_name} ({pred}): {count:,}개 ({percentage:.1f}%)\")\n",
    "\n",
    "# # 샘플 제출 파일 로드\n",
    "# sample_submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "\n",
    "# # ID를 기준으로 병합하여 제출 파일 생성\n",
    "# submission_df = sample_submission[[\"ID\"]].merge(\n",
    "#     test_df[[\"ID\", \"pred\"]], \n",
    "#     left_on=\"ID\", \n",
    "#     right_on=\"ID\", \n",
    "#     how=\"left\"\n",
    "# )\n",
    "\n",
    "# # 제출 파일 검증\n",
    "# assert len(submission_df) == len(sample_submission), f\"길이 불일치: {len(submission_df)} vs {len(sample_submission)}\"\n",
    "# assert submission_df[\"pred\"].isin([0, 1, 2, 3]).all(), \"모든 예측값은 [0, 1, 2, 3] 범위에 있어야 합니다\"\n",
    "# assert not submission_df[\"pred\"].isnull().any(), \"예측값에 null 값이 있으면 안됩니다\"\n",
    "# assert not submission_df[\"ID\"].isnull().any(), \"ID 컬럼에 null 값이 있으면 안됩니다\"\n",
    "\n",
    "# print(\"✅ 제출 파일 검증 통과\")\n",
    "\n",
    "# # 제출 파일 저장\n",
    "# submission_path = \"./output.csv\"\n",
    "# submission_df.to_csv(submission_path, index=False)\n",
    "# print(f\"✅ 제출 파일 저장 완료: {submission_path}\")\n",
    "\n",
    "# # WandB 종료\n",
    "# if config.use_wandb:\n",
    "#     wandb.finish()\n",
    "#     print(\"✅ WandB 세션 종료\")\n",
    "\n",
    "# print(\"\\n🎉 모든 작업 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 테스트 데이터 추론\n",
    "# print(\"=\" * 50)\n",
    "# print(\"테스트 데이터 추론\")\n",
    "# print(\"=\" * 50)\n",
    "\n",
    "# # 테스트 데이터 로드\n",
    "# test_df = pd.read_csv(\"data/test.csv\")\n",
    "# print(f\"테스트 데이터: {len(test_df):,}개\")\n",
    "\n",
    "# # 최고 성능 모델 로드\n",
    "# if os.path.exists(\"./best_model\"):\n",
    "#     print(\"최고 성능 모델 로드 중...\")\n",
    "#     inference_model = AutoModelForSequenceClassification.from_pretrained(\"./best_model\")\n",
    "#     inference_tokenizer = AutoTokenizer.from_pretrained(\"./best_model\")\n",
    "#     inference_model = inference_model.to(device)\n",
    "#     inference_model.eval()\n",
    "#     print(\"✅ 최고 성능 모델 로드 완료\")\n",
    "# else:\n",
    "#     print(\"⚠️  저장된 모델이 없습니다. 현재 모델 사용\")\n",
    "#     inference_model = model\n",
    "#     inference_tokenizer = tokenizer\n",
    "\n",
    "# # 테스트 데이터셋 생성\n",
    "# test_dataset = ReviewDataset(\n",
    "#     test_df[\"review\"],\n",
    "#     None,  # 테스트 데이터는 라벨 없음\n",
    "#     inference_tokenizer,\n",
    "#     config.max_length,\n",
    "# )\n",
    "\n",
    "# # 테스트 데이터로더 생성\n",
    "# test_dataloader = DataLoader(\n",
    "#     test_dataset,\n",
    "#     batch_size=config.eval_batch_size,\n",
    "#     shuffle=False,\n",
    "#     num_workers=2,\n",
    "#     pin_memory=True if torch.cuda.is_available() else False\n",
    "# )\n",
    "\n",
    "# print(\"✅ 테스트 데이터셋 준비 완료\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
