{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ - PyTorch ëª¨ë¸ í•™ìŠµ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ PyTorch ê¸°ë°˜ì˜ BERT/RoBERTa ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì£¼ìš” ê¸°ëŠ¥\n",
    "- PyTorch ë„¤ì´í‹°ë¸Œ í•™ìŠµ ë£¨í”„ (Transformers Trainer ëŒ€ì‹ )\n",
    "- LoRA íŒŒì¸íŠœë‹\n",
    "- WandB ì‹¤í—˜ ì¶”ì \n",
    "- í˜¼í•© ì •ë°€ë„ í•™ìŠµ (Mixed Precision)\n",
    "- ì¡°ê¸° ì¢…ë£Œ ë° ëª¨ë¸ ì²´í¬í¬ì¸íŒ…\n",
    "- í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/py310/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/py310/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# Library Import\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    set_seed,\n",
    "    AutoModelForMaskedLM\n",
    ")\n",
    "\n",
    "# PEFT for LoRA\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"DAPT_30_epochs_augX3\"\n",
    "PROJECT_NAME = f\"[domain_project]_Experiment_DAPT\"\n",
    "MODEL_NAME = \"kykim_bert-kor-base\"\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë””ë°”ì´ìŠ¤: cuda\n",
      "GPU ê°œìˆ˜: 1\n",
      "   GPU 0: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "# í™˜ê²½ ì„¤ì •\n",
    "RANDOM_STATE = 42\n",
    "set_seed(RANDOM_STATE)\n",
    "\n",
    "# GPU ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"âš ï¸  CUDA ì‚¬ìš© ë¶ˆê°€ - CPUë¡œ í›ˆë ¨ ì§„í–‰\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# ì´ í•¨ìˆ˜ê°€ .env íŒŒì¼ì„ ì½ì–´ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "molels = [\"beomi_kcbert-base\", \"klue_roberta-base\", \"klue_bert-base\",\"kykim_bert-kor-base\", \"monologg_koelectra-base-v3-discriminator\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "\n",
    "class Config:\n",
    "    # ëª¨ë¸ ì„¤ì •\n",
    "    model_name = MODEL_NAME\n",
    "    base_models_path = \"/data/ephemeral/home/code/basemodels/\"\n",
    "    local_model_path = base_models_path + MODEL_NAME\n",
    "    num_classes = 4\n",
    "    max_length = 128\n",
    "\n",
    "    save_best_model_path = \"./best_unsupervised_model\"\n",
    "    \n",
    "    # í›ˆë ¨ ì„¤ì •\n",
    "    unsupervised_batch_size = 196\n",
    "    #batch_size = 256\n",
    "\n",
    "    DAPT_num_epochs = 20\n",
    "    TAPT_num_epochs = 30\n",
    "\n",
    "\n",
    "    learning_rate = 2e-5\n",
    "    weight_decay = 0.01\n",
    "    warmup_steps = 500\n",
    "    \n",
    "    # ê¸°íƒ€ ì„¤ì •\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1.0\n",
    "    early_stopping_patience = 3\n",
    "    save_best_model = True\n",
    "    \n",
    "    # WandB ì„¤ì •\n",
    "    use_wandb = True\n",
    "    project_name = PROJECT_NAME\n",
    "    run_name = f\"{EXPERIMENT_NAME}-training\"\n",
    "\n",
    "config = Config()\n",
    "print(\"âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì¤‘...\n",
      "í›ˆë ¨ ë°ì´í„°: 571,014ê°œ\n",
      "ê²€ì¦ ë°ì´í„°: 63,446ê°œ\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„°: 59,928ê°œ\n",
      "âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "print(\"ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "\n",
    "# í›ˆë ¨ ë°ì´í„° ë¡œë“œ\n",
    "train_df = pd.read_csv(\"/data/ephemeral/home/code/data/train_processed_augX3_newly_gen_added.csv\")\n",
    "print(f\"í›ˆë ¨ ë°ì´í„°: {len(train_df):,}ê°œ\")\n",
    "\n",
    "# ê²€ì¦ ë°ì´í„° ë¡œë“œ\n",
    "val_df = pd.read_csv(\"/data/ephemeral/home/code/data/val_processed_augX3_newly_gen_added.csv\")\n",
    "print(f\"ê²€ì¦ ë°ì´í„°: {len(val_df):,}ê°œ\")\n",
    "\n",
    "test_df = pd.read_csv(\"data/test_processed.csv\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df):,}ê°œ\")\n",
    "\n",
    "# ë¼ë²¨ ë§¤í•‘\n",
    "LABEL_MAPPING = {0: \"ê°•í•œ ë¶€ì •\", 1: \"ì•½í•œ ë¶€ì •\", 2: \"ì•½í•œ ê¸ì •\", 3: \"ê°•í•œ ê¸ì •\"}\n",
    "\n",
    "print(\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'review', 'label', 'type'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlabeledDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ë¼ë²¨ì´ ì—†ëŠ” ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ELECTRA RTD ì‘ì—… ì§€ì›)\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, tokenizer, max_length, mask_probability=0.15):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mask_probability = mask_probability\n",
    "        self.mask_token_id = tokenizer.mask_token_id\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def _apply_electra_masking(self, input_ids, attention_mask):\n",
    "        \"\"\"ELECTRA ìŠ¤íƒ€ì¼ ë§ˆìŠ¤í‚¹ ì ìš© (Generatorìš©)\"\"\"\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        # íŒ¨ë”© í† í°ì´ ì•„ë‹Œ ìœ„ì¹˜ë§Œ ì„ íƒ\n",
    "        non_padding_mask = attention_mask.bool()\n",
    "        \n",
    "        # ë§ˆìŠ¤í‚¹í•  í† í° ì„ íƒ (íŒ¨ë”© í† í° ì œì™¸)\n",
    "        candidate_indices = torch.where(non_padding_mask)[0]\n",
    "        \n",
    "        if len(candidate_indices) == 0:\n",
    "            return input_ids, labels\n",
    "        \n",
    "        # ë§ˆìŠ¤í‚¹í•  í† í° ìˆ˜ ê³„ì‚°\n",
    "        num_tokens_to_mask = max(1, int(len(candidate_indices) * self.mask_probability))\n",
    "        \n",
    "        # ëœë¤í•˜ê²Œ í† í° ì„ íƒ\n",
    "        selected_indices = torch.randperm(len(candidate_indices))[:num_tokens_to_mask]\n",
    "        selected_positions = candidate_indices[selected_indices]\n",
    "        \n",
    "        # ì„ íƒëœ í† í°ì— ëŒ€í•´ ë§ˆìŠ¤í‚¹ ì ìš©\n",
    "        for pos in selected_positions:\n",
    "            if torch.rand(1) < 1:\n",
    "                input_ids[pos] = self.mask_token_id\n",
    "        \n",
    "        return input_ids, labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ìƒ˜í”Œ ë°˜í™˜ (ELECTRA RTD ì‘ì—…ìš©)\"\"\"\n",
    "        # í…ìŠ¤íŠ¸ ì¶”ì¶œ - ë¬¸ìì—´ë¡œ ë³€í™˜ ë³´ì¥\n",
    "        if hasattr(self.texts, 'iloc'):\n",
    "            text = str(self.texts.iloc[idx])  # pandas Seriesì¸ ê²½ìš°\n",
    "        else:\n",
    "            text = str(self.texts[idx])  # listë‚˜ ë‹¤ë¥¸ íƒ€ì…ì¸ ê²½ìš°\n",
    "        \n",
    "        # Noneì´ë‚˜ NaN ê°’ ì²˜ë¦¬\n",
    "        if text == 'None' or text == 'nan' or text == '':\n",
    "            text = \"ë¹ˆ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\"  # ê¸°ë³¸ê°’ ì„¤ì •\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].flatten()\n",
    "        attention_mask = encoding['attention_mask'].flatten()\n",
    "        \n",
    "        # ELECTRA ë§ˆìŠ¤í‚¹ ì ìš© (Generatorìš©)\n",
    "        masked_input_ids, mlm_labels = self._apply_electra_masking(input_ids, attention_mask)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,  # ì›ë³¸ í† í° (Discriminatorìš©)\n",
    "            'masked_input_ids': masked_input_ids,  # ë§ˆìŠ¤í‚¹ëœ í† í° (Generatorìš©)\n",
    "            'attention_mask': attention_mask,\n",
    "            'mlm_labels': mlm_labels  # Generator í•™ìŠµì„ ìœ„í•œ ë¼ë²¨\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í‰ê°€ ë©”íŠ¸ë¦­ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# í‰ê°€ ë©”íŠ¸ë¦­ í•¨ìˆ˜\n",
    "def compute_metrics(predictions, labels):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    # ì˜ˆì¸¡ê°’ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì„ íƒ\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "print(\"âœ… í‰ê°€ ë©”íŠ¸ë¦­ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load_from_local(local_model_path : str = None, model_name : str = None):\n",
    "    # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "    # ì ˆëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ ëª¨ë¸ ë¡œë“œ\n",
    "    print(\"ë¡œì»¬ ê²½ë¡œì—ì„œ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...\")\n",
    "\n",
    "    # ê²½ë¡œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "    if not os.path.exists(local_model_path):\n",
    "        print(f\"âŒ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {local_model_path}\")\n",
    "        return None, None, None\n",
    "    else:\n",
    "        print(f\"âœ… ê²½ë¡œ í™•ì¸ë¨: {local_model_path}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "\n",
    "    # 2. ELECTRA ëª¨ë¸ ë¡œë“œ (Generator + Discriminator)\n",
    "    print(\"ë¡œì»¬ ê²½ë¡œì—ì„œ ELECTRA ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "    \n",
    "    # Generator (MLMìš©)\n",
    "    generator = AutoModelForMaskedLM.from_pretrained(local_model_path)\n",
    "    \n",
    "    # Discriminator (RTDìš©) - Generatorì™€ ë™ì¼í•œ êµ¬ì¡°ì´ì§€ë§Œ RTD í—¤ë“œ ì‚¬ìš©\n",
    "    discriminator = AutoModelForMaskedLM.from_pretrained(local_model_path)\n",
    "    \n",
    "    # Discriminatorì˜ MLM í—¤ë“œë¥¼ RTD í—¤ë“œë¡œ êµì²´\n",
    "    from transformers import ElectraForPreTraining\n",
    "    discriminator = ElectraForPreTraining.from_pretrained(local_model_path)\n",
    "\n",
    "    print(\"âœ… ë¡œì»¬ ìŠ¤ëƒ…ìƒ·ì—ì„œ ELECTRA ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”© ì„±ê³µ!\")\n",
    "\n",
    "    # --- ì´ì œ í‰ì†Œì²˜ëŸ¼ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ---\n",
    "    inputs = tokenizer(\"ì´ ì˜í™” ì •ë§ ì¬ë¯¸ìˆë„¤ìš”!\", return_tensors=\"pt\")\n",
    "    outputs = generator(**inputs)\n",
    "    print(\"Generator outputs shape:\", outputs.logits.shape)\n",
    "\n",
    "    return generator, discriminator, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¡œì»¬ ê²½ë¡œì—ì„œ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...\n",
      "âœ… ê²½ë¡œ í™•ì¸ë¨: /data/ephemeral/home/code/basemodels/kykim_bert-kor-base\n",
      "ë¡œì»¬ ê²½ë¡œì—ì„œ ëª¨ë¸ ë¡œë”© ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/ephemeral/home/code/basemodels/kykim_bert-kor-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¡œì»¬ ìŠ¤ëƒ…ìƒ·ì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”© ì„±ê³µ!\n",
      "tensor([[[ -8.7732,  -5.3598,  -4.4707,  ...,  -5.5349,  -4.1685,  -6.2984],\n",
      "         [-13.5281,  -7.3256,  -5.6978,  ...,  -4.6070,  -7.2017, -13.3684],\n",
      "         [-13.5850, -10.6456,  -6.2533,  ...,  -5.0207,  -6.9485, -10.5358],\n",
      "         ...,\n",
      "         [-13.1823,  -8.8666,  -4.9712,  ..., -10.3408,  -7.1187, -11.3603],\n",
      "         [-11.3309,  -6.7285,  -2.5937,  ...,  -6.8013,  -4.9377, -10.5218],\n",
      "         [ -9.3873,  -4.1437,  -6.7594,  ...,  -3.4999,  -1.9037,  -9.5742]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "generator, discriminator, tokenizer = model_load_from_local(config.local_model_path, config.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "print(\"âœ… ELECTRA ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(42000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=42000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   review\n",
      "0                          ì´ëŸ° ì• ë§¤í•œ ëë§ºìŒì€ ì¢€ ì•„ë‹ˆì§€ ì•Šë‚˜ ì‹¶ë„¤. ã…¡,. ã…¡\n",
      "1       ì™€, ì˜¤ëœë§Œì— ì´ ì˜í™” ë‹¤ì‹œ ë´¤ëŠ”ë° ì •ë§ì´ì§€. í¸ì§‘ì€ ë¬´ìŠ¨ ì •ì‹ ìœ¼ë¡œ í•œ ê±´ì§€ ë„ì €íˆ...\n",
      "2       ìºë¦­í„°ë“¤ë„ í•˜ë‚˜ê°™ì´ ë„ˆë¬´ ë§¤ë ¥ì ì´ì–´ì„œ ë³´ëŠ” ë‚´ë‚´ ë¯¸ì†Œê°€ ëŠì´ì§ˆ ì•Šì•˜ì–´. íŠ¹íˆ ì¡°ì¹´ê°€...\n",
      "3           ë‚˜ë„ ì†Œë¦„ ë¼ì¹˜ë©´ì„œ ì˜ë´£ëŠ”ë°! ë‚˜ì´íŠ¸ìƒ¤ë§ë¡ ê°ë… ì˜í™”ëŠ” ë‹¤ë¥¸ì˜í™”ë“¤ê³¼ ì¢€ ë‹¤ë¥¸ë“¯!ë©‹ì§\n",
      "4       ë‘ ì‚¬ëŒì˜ ì¼€ë¯¸ê°€ ì •ë§ ìµœê³ ì˜€ê³ , ë³´ëŠ” ë‚´ë‚´ ì…ê°€ì— ë¯¸ì†Œê°€ ë– ë‚˜ì§€ ì•Šì•˜ì–´ìš”. íŠ¹íˆ ...\n",
      "...                                                   ...\n",
      "694383  ì—¬ì ì£¼ì¸ê³µì€ ì˜ˆìœë° ì ê¹ ë‚˜ì˜¤ê³  ì „ì²´ì ìœ¼ë¡œëŠ” ì¢‹ì€ ë‚´ìš©ì´ì§€ë§Œ ì„¸ë¶€ì ì¸ ë‚´ìš©ë“¤ì´ ë§...\n",
      "694384                ê¸°ëŒ€ ì•ˆí•˜ê³  ë´¤ëŠ”ë° ì˜¤ëœë§Œì— ë³´ëŠ” ì •ë§ ì¢‹ì€ ì½”ë¯¸ë”” ì˜í™”ì˜€ìŠµë‹ˆë‹¤\n",
      "694385  ì„œë¡ ì´ ë„ˆë¬´ê¸¸ê³  ë³„ê±°ì•„ë‹Œê±°ì— ë‚˜ë ˆì´ì…˜ì„ ë„ˆë¬´ë§ì´ê¹”ì•„ì„œ ì§€ë£¨í•¨.. ìŠ¤í† ë¦¬ëŠ” ë³„ê±°ì•„ë‹Œë°...\n",
      "694386                                   ê³ ê¸‰ìŠ¤ëŸ½ê²Œ í¬ì¥ëœ ê³ ê¸‰ ì˜í™”.\n",
      "694387                             ë‚œì´ëŸ°ë°‹ë°‹í•œì˜í™”ê°€ë³„ë¡œë¼ì„œ..ì‚´ì§ì§€ë£¨í–ˆìŒ.\n",
      "\n",
      "[694388 rows x 1 columns]\n",
      "196381\n"
     ]
    }
   ],
   "source": [
    "unlabeled_df = pd.DataFrame(pd.concat([\n",
    "    train_df[\"review\"], \n",
    "    val_df[\"review\"], \n",
    "    test_df[\"review\"]\n",
    "], ignore_index=True))\n",
    "\n",
    "original_df = pd.DataFrame(pd.concat([\n",
    "    train_df[train_df[\"type\"] == \"original\"][\"review\"], \n",
    "    val_df[val_df[\"type\"] == \"original\"][\"review\"], \n",
    "    test_df[\"review\"]\n",
    "], ignore_index=True))\n",
    "\n",
    "print(unlabeled_df)\n",
    "print(len(original_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...\n",
      "Unlabeled ë°ì´í„°: 694,388ê°œ\n",
      "Original ë°ì´í„°: 196,381ê°œ\n",
      "âœ… ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±\n",
    "print(\"ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...\")\n",
    "\n",
    "original_dataset = UnlabeledDataset(\n",
    "    original_df[\"review\"],\n",
    "    tokenizer,\n",
    "    config.max_length,\n",
    ")\n",
    "\n",
    "unlabeled_dataset = UnlabeledDataset(\n",
    "    unlabeled_df[\"review\"],\n",
    "    tokenizer,\n",
    "    config.max_length,\n",
    ")\n",
    "\n",
    "unlabeled_dataloader = DataLoader(\n",
    "    unlabeled_dataset,\n",
    "    batch_size=config.unsupervised_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "original_dataloader = DataLoader(\n",
    "    original_dataset,\n",
    "    batch_size=config.unsupervised_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"Unlabeled ë°ì´í„°: {len(unlabeled_dataset):,}ê°œ\")\n",
    "print(f\"Original ë°ì´í„°: {len(original_dataset):,}ê°œ\")\n",
    "print(\"âœ… ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ì¤‘...\n",
      "DAPT ì´ í›ˆë ¨ ìŠ¤í…: 70,860\n",
      "TAPT ì´ í›ˆë ¨ ìŠ¤í…: 30,060\n",
      "DAPT ì›Œë°ì—… ìŠ¤í…: 500\n",
      "TAPT ì›Œë°ì—… ìŠ¤í…: 500\n",
      "âœ… ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •\n",
    "print(\"ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ì¤‘...\")\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì € ì„¤ì • (Generator + Discriminator)\n",
    "optimizer = AdamW(\n",
    "    list(generator.parameters()) + list(discriminator.parameters()),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "# ì „ì²´ í›ˆë ¨ ìŠ¤í… ê³„ì‚°\n",
    "DAPT_total_steps = len(unlabeled_dataloader) * config.DAPT_num_epochs // config.gradient_accumulation_steps\n",
    "TAPT_total_steps = len(original_dataloader) * config.TAPT_num_epochs // config.gradient_accumulation_steps\n",
    "\n",
    "# ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (warmup + linear decay)\n",
    "DAPT_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config.warmup_steps,\n",
    "    num_training_steps=DAPT_total_steps\n",
    ")\n",
    "TAPT_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config.warmup_steps,\n",
    "    num_training_steps=TAPT_total_steps\n",
    ")\n",
    "\n",
    "# í˜¼í•© ì •ë°€ë„ ìŠ¤ì¼€ì¼ëŸ¬\n",
    "scaler = GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "print(f\"DAPT ì´ í›ˆë ¨ ìŠ¤í…: {DAPT_total_steps:,}\")\n",
    "print(f\"TAPT ì´ í›ˆë ¨ ìŠ¤í…: {TAPT_total_steps:,}\")\n",
    "print(f\"DAPT ì›Œë°ì—… ìŠ¤í…: {config.warmup_steps:,}\")\n",
    "print(f\"TAPT ì›Œë°ì—… ìŠ¤í…: {config.warmup_steps:,}\")\n",
    "print(\"âœ… ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqkdwodus777\u001b[0m (\u001b[33mqkdwodus777-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/code/wandb/run-20251028_040242-r2xhxh4t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Experiment_DAPT/runs/r2xhxh4t' target=\"_blank\">DAPT_30_epochs_augX3-training</a></strong> to <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Experiment_DAPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Experiment_DAPT' target=\"_blank\">https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Experiment_DAPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Experiment_DAPT/runs/r2xhxh4t' target=\"_blank\">https://wandb.ai/qkdwodus777-/%5Bdomain_project%5D_Experiment_DAPT/runs/r2xhxh4t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… WandB ì´ˆê¸°í™” ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# WandB ì´ˆê¸°í™”\n",
    "if config.use_wandb:\n",
    "    wandb.init(\n",
    "        project=config.project_name,\n",
    "        name=config.run_name,\n",
    "        config={\n",
    "            \"model_name\": config.model_name,\n",
    "            \"DAPT_num_epochs\": config.DAPT_num_epochs,\n",
    "            \"TAPT_num_epochs\": config.TAPT_num_epochs,\n",
    "            \"batch_size\": config.unsupervised_batch_size,\n",
    "            \"learning_rate\": config.learning_rate,\n",
    "            \"max_length\": config.max_length,\n",
    "            \"num_classes\": config.num_classes,\n",
    "            \"warmup_steps\": config.warmup_steps,\n",
    "            \"weight_decay\": config.weight_decay,\n",
    "            # \"lora_r\": config.lora_r,\n",
    "            # \"lora_alpha\": config.lora_alpha,\n",
    "            # \"lora_dropout\": config.lora_dropout,\n",
    "            \"random_seed\": RANDOM_STATE\n",
    "        }\n",
    "    )\n",
    "    print(\"âœ… WandB ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "else:\n",
    "    print(\"âš ï¸  WandB ì‚¬ìš© ì•ˆí•¨\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsupervie_train_epoch(generator, discriminator, dataloader, optimizer, scheduler, scaler, device, config):\n",
    "    \"\"\"í•œ ì—í¬í¬ í›ˆë ¨ (ELECTRA ë¹„ì§€ë„ í•™ìŠµ)\"\"\"\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    total_loss = 0\n",
    "    total_mlm_loss = 0\n",
    "    total_rtd_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"ELECTRA Unsupervised Training\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        input_ids = batch[\"input_ids\"].to(device)  # ì›ë³¸ í† í°\n",
    "        masked_input_ids = batch[\"masked_input_ids\"].to(device)  # ë§ˆìŠ¤í‚¹ëœ í† í°\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        mlm_labels = batch[\"mlm_labels\"].to(device)  # Generatorìš© ë¼ë²¨\n",
    "        \n",
    "        # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # í˜¼í•© ì •ë°€ë„ í•™ìŠµ\n",
    "        if scaler is not None:\n",
    "            with autocast():\n",
    "                # 1. Generator í•™ìŠµ (MLM)\n",
    "                gen_outputs = generator(\n",
    "                    input_ids=masked_input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=mlm_labels\n",
    "                )\n",
    "                mlm_loss = gen_outputs.loss\n",
    "                \n",
    "                # 2. Generatorë¡œ ëŒ€ì²´ í† í° ìƒì„±\n",
    "                with torch.no_grad():\n",
    "                    gen_logits = generator(\n",
    "                        input_ids=masked_input_ids,\n",
    "                        attention_mask=attention_mask\n",
    "                    ).logits\n",
    "                    generated_tokens = torch.argmax(gen_logits, dim=-1)\n",
    "                \n",
    "                # 3. RTD ë¼ë²¨ ìƒì„± (ì›ë³¸ê³¼ ë‹¤ë¥¸ í† í° ìœ„ì¹˜)\n",
    "                rtd_labels = (input_ids != generated_tokens).float()\n",
    "                \n",
    "                # 4. Discriminator í•™ìŠµ (RTD)\n",
    "                disc_outputs = discriminator(\n",
    "                    input_ids=generated_tokens,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=rtd_labels\n",
    "                )\n",
    "                rtd_loss = disc_outputs.loss\n",
    "                \n",
    "                # 5. ì „ì²´ ì†ì‹¤ (Generator + Discriminator)\n",
    "                total_batch_loss = mlm_loss + rtd_loss\n",
    "                \n",
    "            # ìŠ¤ì¼€ì¼ëœ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°\n",
    "            scaler.scale(total_batch_loss).backward()\n",
    "            \n",
    "            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘\n",
    "            if config.max_grad_norm > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(list(generator.parameters()) + list(discriminator.parameters()), config.max_grad_norm)\n",
    "            \n",
    "            # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "        else:\n",
    "            # ì¼ë°˜ ì •ë°€ë„ í•™ìŠµ\n",
    "            # 1. Generator í•™ìŠµ (MLM)\n",
    "            gen_outputs = generator(\n",
    "                input_ids=masked_input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=mlm_labels\n",
    "            )\n",
    "            mlm_loss = gen_outputs.loss\n",
    "            \n",
    "            # 2. Generatorë¡œ ëŒ€ì²´ í† í° ìƒì„±\n",
    "            with torch.no_grad():\n",
    "                gen_logits = generator(\n",
    "                    input_ids=masked_input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                ).logits\n",
    "                generated_tokens = torch.argmax(gen_logits, dim=-1)\n",
    "            \n",
    "            # 3. RTD ë¼ë²¨ ìƒì„± (ì›ë³¸ê³¼ ë‹¤ë¥¸ í† í° ìœ„ì¹˜)\n",
    "            rtd_labels = (input_ids != generated_tokens).float()\n",
    "            \n",
    "            # 4. Discriminator í•™ìŠµ (RTD)\n",
    "            disc_outputs = discriminator(\n",
    "                input_ids=generated_tokens,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=rtd_labels\n",
    "            )\n",
    "            rtd_loss = disc_outputs.loss\n",
    "            \n",
    "            # 5. ì „ì²´ ì†ì‹¤ (Generator + Discriminator)\n",
    "            total_batch_loss = mlm_loss + rtd_loss\n",
    "            \n",
    "            # ì—­ì „íŒŒ\n",
    "            total_batch_loss.backward()\n",
    "            \n",
    "            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘\n",
    "            if config.max_grad_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(list(generator.parameters()) + list(discriminator.parameters()), config.max_grad_norm)\n",
    "            \n",
    "            # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…\n",
    "            optimizer.step()\n",
    "        \n",
    "        # ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í…\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # ì†ì‹¤ ëˆ„ì \n",
    "        total_loss += total_batch_loss.item()\n",
    "        total_mlm_loss += mlm_loss.item()\n",
    "        total_rtd_loss += rtd_loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # ì§„í–‰ë¥  í‘œì‹œ ì—…ë°ì´íŠ¸\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_mlm_loss = total_mlm_loss / num_batches\n",
    "        avg_rtd_loss = total_rtd_loss / num_batches\n",
    "        progress_bar.set_postfix({\n",
    "            'total_loss': f'{avg_loss:.4f}',\n",
    "            'mlm_loss': f'{avg_mlm_loss:.4f}',\n",
    "            'rtd_loss': f'{avg_rtd_loss:.4f}',\n",
    "            'lr': f'{optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "        })\n",
    "        \n",
    "        # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  (ì„ íƒì‚¬í•­)\n",
    "        if config.gradient_accumulation_steps > 1:\n",
    "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "    \n",
    "    # í‰ê·  ì†ì‹¤ ê³„ì‚°\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    avg_mlm_loss = total_mlm_loss / num_batches if num_batches > 0 else 0\n",
    "    avg_rtd_loss = total_rtd_loss / num_batches if num_batches > 0 else 0\n",
    "    \n",
    "    return avg_loss, avg_mlm_loss, avg_rtd_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DAPT_training_loop(generator, discriminator, unlabeled_dataloader, optimizer, scheduler, scaler, device, config):\n",
    "    \"\"\"ELECTRA ë¹„ì§€ë„ í•™ìŠµ ë©”ì¸ ë£¨í”„\"\"\"\n",
    "    \n",
    "    # ì´ˆê¸°í™”\n",
    "    best_unsup_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    print(\"ğŸš€ ELECTRA DAPT í•™ìŠµ ì‹œì‘...\")\n",
    "    print(f\"ì´ ì—í¬í¬: {config.DAPT_num_epochs}\")\n",
    "    print(f\"ì¡°ê¸° ì¢…ë£Œ ì¸ë‚´ì‹¬: {config.early_stopping_patience}\")\n",
    "    \n",
    "        \n",
    "    for epoch in range(config.DAPT_num_epochs):\n",
    "        print(f\"\\n=== Epoch {epoch+1}/{config.DAPT_num_epochs} ===\")\n",
    "        \n",
    "        # ELECTRA ë¹„ì§€ë„ í•™ìŠµ ì—í¬í¬\n",
    "        total_loss, mlm_loss, rtd_loss = unsupervie_train_epoch(\n",
    "            generator=generator,\n",
    "            discriminator=discriminator,\n",
    "            dataloader=unlabeled_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            scaler=scaler,\n",
    "            device=device,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        print(f\"Total Loss: {total_loss:.4f}\")\n",
    "        print(f\"MLM Loss: {mlm_loss:.4f}\")\n",
    "        print(f\"RTD Loss: {rtd_loss:.4f}\")\n",
    "        \n",
    "        # ëª¨ë¸ ì €ì¥ ì¡°ê±´ (ì†ì‹¤ì´ ê°œì„ ë˜ì—ˆì„ ë•Œ)\n",
    "        if config.save_best_model and total_loss < best_unsup_loss:\n",
    "            best_unsup_loss = total_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # ëª¨ë¸ ì €ì¥\n",
    "            os.makedirs(config.save_best_model_path, exist_ok=True)\n",
    "            generator.save_pretrained(f\"./{config.save_best_model_path}/DAPT_{MODEL_NAME}_augX3_best_generator_1028\")\n",
    "            discriminator.save_pretrained(f\"./{config.save_best_model_path}/DAPT_{MODEL_NAME}_augX3_best_discriminator_1028\")\n",
    "            tokenizer.save_pretrained(f\"./{config.save_best_model_path}/DAPT_{MODEL_NAME}_augX3_best_generator_1028\")\n",
    "            print(f\"âœ… ìµœê³  ì„±ëŠ¥ ELECTRA ëª¨ë¸ ì €ì¥ (Total Loss: {best_unsup_loss:.4f})\")\n",
    "            \n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"â³ ì¡°ê¸° ì¢…ë£Œ ì¹´ìš´í„°: {patience_counter}/{config.early_stopping_patience}\")\n",
    "        \n",
    "        # WandB ë¡œê¹… (ì„ íƒì‚¬í•­)\n",
    "        if config.use_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"total_loss\": total_loss,\n",
    "                \"mlm_loss\": mlm_loss,\n",
    "                \"rtd_loss\": rtd_loss,\n",
    "                \"best_unsupervised_loss\": best_unsup_loss,\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "        \n",
    "        # ì¡°ê¸° ì¢…ë£Œ ì²´í¬\n",
    "        if patience_counter >= config.early_stopping_patience:\n",
    "            print(f\"ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ: {config.early_stopping_patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nğŸ¯ ELECTRA ë¹„ì§€ë„ í•™ìŠµ ì™„ë£Œ!\")\n",
    "    print(f\"ìµœê³  Total Loss: {best_unsup_loss:.4f}\")\n",
    "    \n",
    "    return best_unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TAPT_training_loop(generator, discriminator, unlabeled_dataloader, optimizer, scheduler, scaler, device, config):\n",
    "    \"\"\"ELECTRA ë¹„ì§€ë„ í•™ìŠµ ë©”ì¸ ë£¨í”„\"\"\"\n",
    "    \n",
    "    # ì´ˆê¸°í™”\n",
    "    best_unsup_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    print(\"ğŸš€ ELECTRA TAPT í•™ìŠµ ì‹œì‘...\")\n",
    "    print(f\"ì´ ì—í¬í¬: {config.TAPT_num_epochs}\")\n",
    "    print(f\"ì¡°ê¸° ì¢…ë£Œ ì¸ë‚´ì‹¬: {config.early_stopping_patience}\")\n",
    "    \n",
    "        \n",
    "    for epoch in range(config.TAPT_num_epochs):\n",
    "        print(f\"\\n=== Epoch {epoch+1}/{config.TAPT_num_epochs} ===\")\n",
    "        \n",
    "        # ELECTRA ë¹„ì§€ë„ í•™ìŠµ ì—í¬í¬\n",
    "        total_loss, mlm_loss, rtd_loss = unsupervie_train_epoch(\n",
    "            generator=generator,\n",
    "            discriminator=discriminator,\n",
    "            dataloader=unlabeled_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            scaler=scaler,\n",
    "            device=device,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        print(f\"Total Loss: {total_loss:.4f}\")\n",
    "        print(f\"MLM Loss: {mlm_loss:.4f}\")\n",
    "        print(f\"RTD Loss: {rtd_loss:.4f}\")\n",
    "        \n",
    "        # ëª¨ë¸ ì €ì¥ ì¡°ê±´ (ì†ì‹¤ì´ ê°œì„ ë˜ì—ˆì„ ë•Œ)\n",
    "        if config.save_best_model and total_loss < best_unsup_loss:\n",
    "            best_unsup_loss = total_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # ëª¨ë¸ ì €ì¥\n",
    "            os.makedirs(config.save_best_model_path, exist_ok=True)\n",
    "            generator.save_pretrained(f\"./{config.save_best_model_path}/TAPT_{MODEL_NAME}_augX3_best_generator_1026\")\n",
    "            discriminator.save_pretrained(f\"./{config.save_best_model_path}/TAPT_{MODEL_NAME}_augX3_best_discriminator_1026\")\n",
    "            tokenizer.save_pretrained(f\"./{config.save_best_model_path}/TAPT_{MODEL_NAME}_augX3_best_generator_1026\")\n",
    "            print(f\"âœ… ìµœê³  ì„±ëŠ¥ ELECTRA ëª¨ë¸ ì €ì¥ (Total Loss: {best_unsup_loss:.4f})\")\n",
    "            \n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"â³ ì¡°ê¸° ì¢…ë£Œ ì¹´ìš´í„°: {patience_counter}/{config.early_stopping_patience}\")\n",
    "        \n",
    "        # WandB ë¡œê¹… (ì„ íƒì‚¬í•­)\n",
    "        if config.use_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"total_loss\": total_loss,\n",
    "                \"mlm_loss\": mlm_loss,\n",
    "                \"rtd_loss\": rtd_loss,\n",
    "                \"best_unsupervised_loss\": best_unsup_loss,\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "        \n",
    "        # ì¡°ê¸° ì¢…ë£Œ ì²´í¬\n",
    "        if patience_counter >= config.early_stopping_patience:\n",
    "            print(f\"ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ: {config.early_stopping_patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nğŸ¯ ELECTRA ë¹„ì§€ë„ í•™ìŠµ ì™„ë£Œ!\")\n",
    "    print(f\"ìµœê³  Total Loss: {best_unsup_loss:.4f}\")\n",
    "    \n",
    "    return best_unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ DAPT í•™ìŠµ ì‹œì‘...\n",
      "ì´ ì—í¬í¬: 20\n",
      "ì¡°ê¸° ì¢…ë£Œ ì¸ë‚´ì‹¬: 3\n",
      "\n",
      "=== Epoch 1/20 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e233c61dab5845c3932dfe08231ad009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsupervised Training:   0%|          | 0/3543 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_loss_stage_DAPT = DAPT_training_loop(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    unlabeled_dataloader=unlabeled_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=DAPT_scheduler,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss_stage_2 = TAPT_training_loop(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    unlabeled_dataloader=original_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=TAPT_scheduler,\n",
    "    scaler=scaler,\n",
    "    device=device,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ì¶”ë¡  ì‹¤í–‰\n",
    "# print(\"ì¶”ë¡  ì‹¤í–‰ ì¤‘...\")\n",
    "# inference_model.eval()\n",
    "# predictions = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in tqdm(test_dataloader, desc=\"Inference\"):\n",
    "#         input_ids = batch[\"input_ids\"].to(device)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "#         outputs = inference_model(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask\n",
    "#         )\n",
    "        \n",
    "#         logits = outputs.logits\n",
    "#         batch_predictions = torch.argmax(logits, dim=-1)\n",
    "#         predictions.extend(batch_predictions.cpu().numpy())\n",
    "\n",
    "# print(f\"âœ… ì¶”ë¡  ì™„ë£Œ: {len(predictions):,}ê°œ ì˜ˆì¸¡\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "# print(\"ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...\")\n",
    "\n",
    "# # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€\n",
    "# test_df[\"pred\"] = predictions\n",
    "\n",
    "# # í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„í¬ í™•ì¸\n",
    "# unique_predictions, counts = np.unique(predictions, return_counts=True)\n",
    "# print(\"\\ní´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„í¬:\")\n",
    "# for pred, count in zip(unique_predictions, counts):\n",
    "#     percentage = (count / len(predictions)) * 100\n",
    "#     class_name = LABEL_MAPPING.get(pred, f\"í´ë˜ìŠ¤ {pred}\")\n",
    "#     print(f\"   {class_name} ({pred}): {count:,}ê°œ ({percentage:.1f}%)\")\n",
    "\n",
    "# # ìƒ˜í”Œ ì œì¶œ íŒŒì¼ ë¡œë“œ\n",
    "# sample_submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "\n",
    "# # IDë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©í•˜ì—¬ ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "# submission_df = sample_submission[[\"ID\"]].merge(\n",
    "#     test_df[[\"ID\", \"pred\"]], \n",
    "#     left_on=\"ID\", \n",
    "#     right_on=\"ID\", \n",
    "#     how=\"left\"\n",
    "# )\n",
    "\n",
    "# # ì œì¶œ íŒŒì¼ ê²€ì¦\n",
    "# assert len(submission_df) == len(sample_submission), f\"ê¸¸ì´ ë¶ˆì¼ì¹˜: {len(submission_df)} vs {len(sample_submission)}\"\n",
    "# assert submission_df[\"pred\"].isin([0, 1, 2, 3]).all(), \"ëª¨ë“  ì˜ˆì¸¡ê°’ì€ [0, 1, 2, 3] ë²”ìœ„ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤\"\n",
    "# assert not submission_df[\"pred\"].isnull().any(), \"ì˜ˆì¸¡ê°’ì— null ê°’ì´ ìˆìœ¼ë©´ ì•ˆë©ë‹ˆë‹¤\"\n",
    "# assert not submission_df[\"ID\"].isnull().any(), \"ID ì»¬ëŸ¼ì— null ê°’ì´ ìˆìœ¼ë©´ ì•ˆë©ë‹ˆë‹¤\"\n",
    "\n",
    "# print(\"âœ… ì œì¶œ íŒŒì¼ ê²€ì¦ í†µê³¼\")\n",
    "\n",
    "# # ì œì¶œ íŒŒì¼ ì €ì¥\n",
    "# submission_path = \"./output.csv\"\n",
    "# submission_df.to_csv(submission_path, index=False)\n",
    "# print(f\"âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {submission_path}\")\n",
    "\n",
    "# # WandB ì¢…ë£Œ\n",
    "# if config.use_wandb:\n",
    "#     wandb.finish()\n",
    "#     print(\"âœ… WandB ì„¸ì…˜ ì¢…ë£Œ\")\n",
    "\n",
    "# print(\"\\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡ \n",
    "# print(\"=\" * 50)\n",
    "# print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡ \")\n",
    "# print(\"=\" * 50)\n",
    "\n",
    "# # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "# test_df = pd.read_csv(\"data/test.csv\")\n",
    "# print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df):,}ê°œ\")\n",
    "\n",
    "# # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ\n",
    "# if os.path.exists(\"./best_model\"):\n",
    "#     print(\"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "#     inference_model = AutoModelForSequenceClassification.from_pretrained(\"./best_model\")\n",
    "#     inference_tokenizer = AutoTokenizer.from_pretrained(\"./best_model\")\n",
    "#     inference_model = inference_model.to(device)\n",
    "#     inference_model.eval()\n",
    "#     print(\"âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "# else:\n",
    "#     print(\"âš ï¸  ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ëª¨ë¸ ì‚¬ìš©\")\n",
    "#     inference_model = model\n",
    "#     inference_tokenizer = tokenizer\n",
    "\n",
    "# # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±\n",
    "# test_dataset = ReviewDataset(\n",
    "#     test_df[\"review\"],\n",
    "#     None,  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ë¼ë²¨ ì—†ìŒ\n",
    "#     inference_tokenizer,\n",
    "#     config.max_length,\n",
    "# )\n",
    "\n",
    "# # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ìƒì„±\n",
    "# test_dataloader = DataLoader(\n",
    "#     test_dataset,\n",
    "#     batch_size=config.eval_batch_size,\n",
    "#     shuffle=False,\n",
    "#     num_workers=2,\n",
    "#     pin_memory=True if torch.cuda.is_available() else False\n",
    "# )\n",
    "\n",
    "# print(\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
